{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from source.torch_utils import getContrastiveFeatures\n",
    "import torch\n",
    "from source.read_HAR_dataset import read_har_dataset, har_dimensions, har_activities, har_activities_map, har_ind_IDS\n",
    "from source.utils import  filter_dimensions\n",
    "from source.tserie import TSerie\n",
    "from source.utils import classify_dataset\n",
    "from itertools import chain, combinations\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "from source.utils import idsStd\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import umap\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from source.augmentation import  * \n",
    "# from cuml.datasets import make_blobs\n",
    "# from cuml.neighbors import NearestNeighbors\n",
    "# from cuml.manifold import UMAP\n",
    "# from cuml.cluster import DBSCAN\n",
    "\n",
    "from scipy.spatial import distance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "sys.path.insert(0, '/home/texs/Documentos/Repositories/mts_viz')\n",
    "from server.source.storage import MTSStorage\n",
    "\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)  # allows duplicate elements\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "# Z_SCORE_NORM = True\n",
    "EXPERIMENTS_DIR = 'distance_experiments_cont'\n",
    "NORM = 0 # 0: No normalization, 1: centering 2: z_score_norm\n",
    "DATASET = 'HAR-UML20'\n",
    "KFOLDS = 9\n",
    "N_TESTS = 2\n",
    "EPOCHS = 15\n",
    "FEATURE_SIZE = 1024\n",
    "ENCODING_SIZE = 16\n",
    "# METRIC  = 'braycurtis'\n",
    "METRIC  = 'cosine'\n",
    "RESULTS_PATH = 'outputs/augmentation/'\n",
    "AUGMENTATIONS = ['rotation', 'permutation', 'time_warp', 'magnitude_warp', 'scaling', 'jitter']\n",
    "# AUGMENTATIONS = ['none']\n",
    "# AUGMENTATIONS = ['scaling']\n",
    "# ALL_AUGMENTATIONS = ['none', 'rotation', 'permutation', 'time_warp', 'magnitude_warp', 'scaling', 'jitter']\n",
    "# ALL_AUGMENTATIONS = ['none', 'rotation', 'rotation', 'rotation', 'rotation', 'rotation', 'rotation']\n",
    "# ALL_AUGMENTATIONS = ['none'] * 7\n",
    "# AUGMENTATIONS = ['magnitude_warp']\n",
    "REPEATS_PER_AUGMENTATION = 1\n",
    "INCLUDE_ORIGINAL = True\n",
    "# N_DIMS_NAMES = ['Acc', 'Gyro', 'Mag']\n",
    "N_DIMS_NAMES = ['Acc', 'Gyro']\n",
    "# N_DIMS_NAMES = ['Acc']\n",
    "N_DIMENSIONS = [\n",
    "    [\n",
    "        'Accelerometer-X',\t\n",
    "        'Accelerometer-Y',\t\n",
    "        'Accelerometer-Z',\n",
    "    ],\n",
    "    [\n",
    "        'Gyrometer-X',\n",
    "        'Gyrometer-Y',\n",
    "        'Gyrometer-Z',\n",
    "    ],\n",
    "    # [\n",
    "    #     'Magnetometer-X',\n",
    "    #     'Magnetometer-Y',\n",
    "    #     'Magnetometer-Z'\n",
    "    # ]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    def __init__(self, epochs = 100, batch_size = 32, loss_metric = 'SimCLR', encoding_size = 8, mode = 'subsequences'):\n",
    "        self.model = None\n",
    "        self.device = None\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_metric = loss_metric\n",
    "        self.encoding_size = encoding_size\n",
    "        self.mode = mode\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        embeddings, self.model, self.device =  getContrastiveFeatures(X.transpose([0, 2, 1]), y,\n",
    "                epochs = self.epochs, \n",
    "                loss_metric=self.loss_metric, \n",
    "                feat_size=FEATURE_SIZE, \n",
    "                encoding_size=ENCODING_SIZE,\n",
    "                mode=self.mode,\n",
    "        )\n",
    "        print(X.shape)\n",
    "        return embeddings\n",
    "    \n",
    "    def transform(self, X):\n",
    "        print(X.shape)\n",
    "        return self.model.encode(X.transpose([0, 2, 1]), self.device)\n",
    "\n",
    "\n",
    "def augmentData(X, y, I, augmentation, repeat = 3):\n",
    "    X_out = []\n",
    "    y_out = []\n",
    "    I_out = []\n",
    "    for i in range(repeat):\n",
    "        if augmentation == 'rotation':\n",
    "            augmented = rotation(X, angle_range=[-np.pi/4, np.pi/4])\n",
    "            # augmented = rotation(X, angle_range=[-np.pi/64, np.pi/64])\n",
    "        elif augmentation == 'permutation':\n",
    "            augmented = permutation(X)\n",
    "        elif augmentation == 'time_warp':\n",
    "            augmented = time_warp(X, sigma=0.03)\n",
    "        elif augmentation == 'magnitude_warp':\n",
    "            augmented = magnitude_warp(X, sigma=0.04, knot=4)\n",
    "        elif augmentation == 'scaling':\n",
    "            augmented = scaling(X, sigma=0.05)\n",
    "        elif augmentation == 'jitter':\n",
    "            augmented = jitter(X, sigma=0.01)\n",
    "        # elif augmentation == 'magnitude_pert':\n",
    "        #     augmented = magnitude_pert(X, prange=[0, 1])\n",
    "        else:\n",
    "            augmented = X\n",
    "        if len(X_out) == 0:\n",
    "            X_out = augmented\n",
    "            y_out = y\n",
    "            I_out = I\n",
    "        else:\n",
    "            X_out = np.concatenate((X_out, augmented), axis=0)\n",
    "            y_out = np.concatenate((y_out, y), axis=0)\n",
    "            I_out = np.concatenate((I_out, I), axis=0)\n",
    "    return X_out, y_out, I_out\n",
    "\n",
    "def augment(X, y, I, augmentations, repeats_per_augmentation=1, include_original=False):\n",
    "    X_aug = []\n",
    "    y_aug = []\n",
    "    I_aug = []\n",
    "    if include_original:\n",
    "        X_aug = X\n",
    "        y_aug = y\n",
    "        I_aug = I\n",
    "    for augmentation in augmentations:\n",
    "        curr_X_aug, curr_y_aug, curr_I_aug = augmentData(X, y, I, augmentation, repeat=repeats_per_augmentation)\n",
    "        if len(X_aug) == 0:\n",
    "            X_aug = curr_X_aug\n",
    "            y_aug = curr_y_aug\n",
    "            I_aug = curr_I_aug\n",
    "        else:\n",
    "            X_aug = np.concatenate((X_aug, curr_X_aug), axis=0)\n",
    "            y_aug = np.concatenate((y_aug, curr_y_aug), axis=0)\n",
    "            I_aug = np.concatenate((I_aug, curr_I_aug), axis=0)\n",
    "    return X_aug, y_aug, I_aug\n",
    "\n",
    "def minoritySampling(X, y):\n",
    "    rus = RandomUnderSampler(sampling_strategy='not minority', random_state=1)\n",
    "    N, T, D = X.shape\n",
    "    X_temp = X.reshape([N, T * D])\n",
    "    X_temp, y = rus.fit_resample(X_temp, y)\n",
    "    X = X_temp.reshape([X_temp.shape[0], T, D])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "activities_map = {\n",
    "    0: \"Sedentary\",\n",
    "    1: \"Walking\",\n",
    "    2: \"Running\",\n",
    "    3: \"Downstairs\",\n",
    "    4: \"Upstairs\"\n",
    "}\n",
    "def load_data(k):\n",
    "    all_ids = har_ind_IDS\n",
    "    test_ids = all_ids[k: k + N_TESTS]\n",
    "    train_ids = all_ids[:k] + all_ids[k + N_TESTS:]        \n",
    "    \n",
    "    data = read_har_dataset('./datasets/HAR-UML20/', train_ids=train_ids, test_ids=test_ids, val_ids=[], cache=True)\n",
    "    ids_train, X_train, y_train, I_train, train_kcal_MET = data['train']\n",
    "    # ids_val, X_val, y_val, I_val, val_kcal_MET = data['val']\n",
    "    ids_test, X_test, y_test, I_test, test_kcal_MET = data['test']\n",
    "    \n",
    "    \n",
    "\n",
    "    all_dimensions = har_dimensions\n",
    "    activities_map = har_activities_map\n",
    "    \n",
    "    y_train[y_train==0] = 0\n",
    "    y_train[y_train==1] = 0\n",
    "    y_train[y_train==2] = 0\n",
    "    y_test[y_test==0] = 0\n",
    "    y_test[y_test==1] = 0\n",
    "    y_test[y_test==2] = 0\n",
    "\n",
    "    for i in range(3, len(har_activities)):\n",
    "        y_train[y_train==i] = i - 2\n",
    "        y_test[y_test==i] = i - 2\n",
    "    \n",
    "    ind_std_train = idsStd(train_ids , X_train, I_train)\n",
    "    ind_std_test = idsStd(test_ids, X_test, I_test)\n",
    "    \n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    unique, counts = np.unique(y_test, return_counts=True)\n",
    "    \n",
    "    I_train = np.expand_dims(I_train, axis=1)\n",
    "    I_test = np.expand_dims(I_test, axis=1)\n",
    "    ltrain = np.arange(len(y_train))\n",
    "    ltest = np.arange(len(y_test))\n",
    "    \n",
    "    X_train, zlabels_train = minoritySampling(X_train, ltrain)\n",
    "    X_test, zlabels_test = minoritySampling(X_test, ltest)\n",
    "    \n",
    "    y_train = y_train[ltrain]\n",
    "    I_train = I_train[ltrain]\n",
    "    y_test = y_test[ltest]\n",
    "    I_test = I_test[ltest]\n",
    "    \n",
    "    return X_train, y_train, I_train, X_test, y_test, I_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train IDS: [2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Test IDS: [0, 1]\n",
      "Val IDS: []\n",
      "Loading dataset from cache...\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, I_train, X_test, y_test, I_test = load_data(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = [\n",
    "    'Accelerometer-X',\t\n",
    "    'Accelerometer-Y',\t\n",
    "    'Accelerometer-Z',\n",
    "    'Gyrometer-X',\n",
    "    'Gyrometer-Y',\n",
    "    'Gyrometer-Z'\n",
    "]\n",
    "\n",
    "X_train = filter_dimensions(X_train, har_dimensions, dimensions)\n",
    "X_test = filter_dimensions(X_test, har_dimensions, dimensions)\n",
    "\n",
    "mts_train = TSerie(X = X_train, y = y_train, I = I_train, dimensions = dimensions, classLabels=har_activities_map)\n",
    "mts_test = TSerie(X = X_test, y = y_test, I = I_test, dimensions = dimensions, classLabels=har_activities_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsequence length: 180\n",
      "Epoch[1] Train loss    avg: 3.8767960446740166\n",
      "Epoch[10] Train loss    avg: 3.452676445094462\n",
      "(6300, 200, 6)\n",
      "(6300, 200, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mts shape: N: 6300 -  T: 200 - D: 6\n",
      "mts shape: N: 6300 -  T: 200 - D: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/texs/Documentos/Repositories/mts_feature_learning/source/augmentation.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  warp = np.concatenate(np.random.permutation(splits)).ravel()\n",
      "<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsequence length: 180\n",
      "Epoch[1] Train loss    avg: 3.829630052982853\n",
      "Epoch[10] Train loss    avg: 3.4238772428580346\n",
      "(6300, 200, 6)\n",
      "(6300, 200, 6)\n",
      "mts shape: N: 6300 -  T: 200 - D: 6\n",
      "mts shape: N: 6300 -  T: 200 - D: 6\n",
      "Subsequence length: 180\n",
      "Epoch[1] Train loss    avg: 3.8433365797633448\n",
      "Epoch[10] Train loss    avg: 3.4457414343877493\n",
      "(6300, 200, 6)\n",
      "(6300, 200, 6)\n",
      "mts shape: N: 6300 -  T: 200 - D: 6\n",
      "mts shape: N: 6300 -  T: 200 - D: 6\n",
      "Subsequence length: 180\n",
      "Epoch[1] Train loss    avg: 3.794914879774684\n",
      "Epoch[10] Train loss    avg: 3.4406840510779833\n",
      "(6300, 200, 6)\n",
      "(6300, 200, 6)\n",
      "mts shape: N: 6300 -  T: 200 - D: 6\n",
      "mts shape: N: 6300 -  T: 200 - D: 6\n",
      "Subsequence length: 180\n",
      "Epoch[1] Train loss    avg: 3.872161975366815\n",
      "Epoch[10] Train loss    avg: 3.4491710275562886\n",
      "(6300, 200, 6)\n",
      "(6300, 200, 6)\n",
      "mts shape: N: 6300 -  T: 200 - D: 6\n",
      "mts shape: N: 6300 -  T: 200 - D: 6\n",
      "Subsequence length: 180\n",
      "Epoch[1] Train loss    avg: 3.8581866128795643\n",
      "Epoch[10] Train loss    avg: 3.4431997664688807\n",
      "(6300, 200, 6)\n",
      "(6300, 200, 6)\n",
      "mts shape: N: 6300 -  T: 200 - D: 6\n",
      "mts shape: N: 6300 -  T: 200 - D: 6\n",
      "[0.37478739077464585, 0.4052394827544197, 0.3472302262878284, 0.5420763514577712, 0.29579512875247504, 0.5350180457677883]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "storage = MTSStorage('har_augmentations_cont')\n",
    "\n",
    "# storage.delete()\n",
    "storage.load()\n",
    "\n",
    "mode = 3 # 0: original - 1: minMax - 2: Centered - 3: Zscore\n",
    "\n",
    "cum_list =[]\n",
    "for augmentation in AUGMENTATIONS:\n",
    "        mts_train.X = mts_train.X_o\n",
    "        \n",
    "        if mode == 1 or mode == 2 or mode == 3:\n",
    "                minl, maxl = mts_train.minMaxNormalization()\n",
    "        \n",
    "        train_aug, y_aug, I_aug = augment(\n",
    "                mts_train.X, mts_train.y, mts_train.I,\n",
    "                repeats_per_augmentation = 1,\n",
    "                # augmentations = ['rotation', 'permutation', 'time_warp', 'magnitude_warp', 'scaling', 'jitter'],\n",
    "                augmentations = [augmentation],\n",
    "                include_original = False\n",
    "        )\n",
    "        mts_aug = TSerie(X = train_aug, y = y_aug, I = I_aug, dimensions = dimensions, classLabels=har_activities_map)\n",
    "        \n",
    "        if mode == 2:\n",
    "                mts_train.center()\n",
    "                mts_aug.center()\n",
    "        elif mode == 3:\n",
    "                mts_train.znorm()\n",
    "                mts_aug.znorm()\n",
    "\n",
    "        # mts_train.folding_features_v1()\n",
    "        # mts_aug.folding_features_v1()\n",
    "\n",
    "        reducer = FeatureExtractor(epochs = EPOCHS, loss_metric='SupConLoss')\n",
    "        mts_train.features = reducer.fit_transform(mts_train.X, mts_train.y)\n",
    "        mts_aug.features = reducer.transform(mts_aug.X)\n",
    "        \n",
    "        umapRed = umap.UMAP(n_components=2)\n",
    "        train_coords = umapRed.fit_transform(mts_train.features)\n",
    "        aug_coords = umapRed.transform(mts_aug.features)\n",
    "        \n",
    "\n",
    "       \n",
    "\n",
    "        storage.add_mts(\n",
    "                'original_{}_{}'.format(augmentation, mode),\n",
    "                mts_train.X, \n",
    "                dimensions,\n",
    "                coords={\n",
    "                        'umap': train_coords\n",
    "                }, \n",
    "                labels={\n",
    "                        'activities': mts_train.y, \n",
    "                }, \n",
    "                labelsNames={'activities': activities_map },\n",
    "                sampling = True,\n",
    "                n_samples = 400\n",
    "        )\n",
    "\n",
    "        storage.add_mts(\n",
    "                'augmented_{}_{}'.format(augmentation, mode),\n",
    "                mts_aug.X, \n",
    "                dimensions,\n",
    "                coords={\n",
    "                        'umap': aug_coords\n",
    "                }, \n",
    "                labels={\n",
    "                        'activities': mts_aug.y, \n",
    "                }, \n",
    "                labelsNames={'activities': activities_map },\n",
    "                sampling = True,\n",
    "                n_samples = 400\n",
    "        )\n",
    "\n",
    "        storage.save()\n",
    "\n",
    "\n",
    "        n1 = 0\n",
    "        n2 = 10\n",
    "        acum_dist = 0\n",
    "        for i in range(mts_train.N):\n",
    "                acum_dist = acum_dist + distance.braycurtis(mts_train.features[i], mts_aug.features[i])\n",
    "        acum_dist = acum_dist / mts_train.N\n",
    "\n",
    "        cum_list.append(acum_dist)\n",
    "\n",
    "        # print('Mean distance {}'.format(acum_dist))\n",
    "        \n",
    "        serie1_o = mts_train.features[n1]\n",
    "        serie1_a = mts_aug.features[n1]\n",
    "        serie2_o = mts_train.features[n2]\n",
    "        serie2_a = mts_aug.features[n2]\n",
    "\n",
    "        plt.plot(serie1_a)\n",
    "        plt.plot(serie1_o)\n",
    "        \n",
    "        spath = os.path.join(EXPERIMENTS_DIR, '{}_serie_{}.png'.format(mode, augmentation))\n",
    "        plt.savefig(spath)\n",
    "        plt.clf()\n",
    "\n",
    "\n",
    "print(cum_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- rotation: 0.3748\n",
      "- permutation: 0.4052\n",
      "- time_warp: 0.3472\n",
      "- magnitude_warp: 0.5421\n",
      "- scaling: 0.2958\n",
      "- jitter: 0.535\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(AUGMENTATIONS)):\n",
    "    print('- {}: {}'.format(AUGMENTATIONS[i], round(cum_list[i], 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('contrastive')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af38bbb6e377e6d688e9d49e963edf808fac73a4ecd279c84061bb0d9783d83c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
