{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from source.read_HAR_dataset import read_har_dataset, har_dimensions, har_activities, har_activities_map, har_ind_IDS\n",
    "from source.utils import  filter_dimensions\n",
    "from source.tserie import TSerie\n",
    "from source.utils import classify_dataset\n",
    "from itertools import chain, combinations\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "from source.utils import idsStd\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import umap\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from source.augmentation import  * \n",
    "# from cuml.datasets import make_blobs\n",
    "# from cuml.neighbors import NearestNeighbors\n",
    "# from cuml.manifold import UMAP\n",
    "# from cuml.cluster import DBSCAN\n",
    "\n",
    "\n",
    "sys.path.insert(0, '/home/texs/Documentos/Repositories/mts_viz')\n",
    "from server.source.storage import MTSStorage\n",
    "\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)  # allows duplicate elements\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "N_COMPONENTS=32\n",
    "Z_SCORE_NORM = False\n",
    "DATASET = 'HAR-UML20'\n",
    "KFOLDS = 1\n",
    "N_TESTS = 2\n",
    "METRIC  = 'braycurtis'\n",
    "RESULTS_PATH = 'outputs/augmentation/'\n",
    "# AUGMENTATIONS = ['rotation', 'permutation', 'time_warp', 'magnitude_warp', 'scaling', 'jitter']\n",
    "# AUGMENTATIONS = ['none']\n",
    "AUGMENTATIONS = ['scaling']\n",
    "ALL_AUGMENTATIONS = ['none', 'rotation', 'permutation', 'time_warp', 'magnitude_warp', 'scaling', 'jitter']\n",
    "# AUGMENTATIONS = ['magnitude_warp']\n",
    "REPEATS_PER_AUGMENTATION = 1\n",
    "INCLUDE_ORIGINAL = True\n",
    "# N_DIMS_NAMES = ['Acc', 'Gyro', 'Mag']\n",
    "# N_DIMS_NAMES = ['Acc', 'Gyro']\n",
    "N_DIMS_NAMES = ['Acc']\n",
    "N_DIMENSIONS = [\n",
    "    [\n",
    "        'Accelerometer-X',\t\n",
    "        'Accelerometer-Y',\t\n",
    "        'Accelerometer-Z',\n",
    "    ],\n",
    "    # [\n",
    "    #     'Gyrometer-X',\n",
    "    #     'Gyrometer-Y',\n",
    "    #     'Gyrometer-Z',\n",
    "    # ],\n",
    "    # [\n",
    "    #     'Magnetometer-X',\n",
    "    #     'Magnetometer-Y',\n",
    "    #     'Magnetometer-Z'\n",
    "    # ]\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center(X_train, X_test, train_ind, test_ind, dimensions ):\n",
    "    N_tr, T, D = X_train.shape\n",
    "    N_te, T, D = X_test.shape\n",
    "    X_train_sh = np.zeros(X_train.shape)\n",
    "    X_test_sh = np.zeros(X_test.shape)\n",
    "    for i in range(N_tr):\n",
    "        \n",
    "        for k in range(D):\n",
    "            if k in dimensions:\n",
    "                mag = np.mean(X_train[i, :, k], axis = 0)\n",
    "                indice = np.where(train_ind ==  I_train[i])[0][0]\n",
    "                std = ind_std_train[indice][k] * 6\n",
    "                X_train_sh[i, :, k] = (X_train[i, :, k] - mag)\n",
    "                # X_train_sh[i, :, k] = np.concatenate([[0], fft(X_train[i, :, k])[1:]])\n",
    "            else:\n",
    "                X_train_sh[i, :, k] = X_train[i, :, k]\n",
    "\n",
    "    for i in range(N_te):\n",
    "        for k in range(D):\n",
    "            if k in dimensions:\n",
    "                mag = np.mean(X_test[i, :, k], axis = 0)\n",
    "                indice = np.where(test_ind ==  I_test[i])[0][0]\n",
    "                std = ind_std_test[indice][k] * 6\n",
    "                X_test_sh[i, :, k] = (X_test[i, :, k] - mag)\n",
    "                # X_test_sh[i, :, k] = fft(X_test[i, :, k])[0:]\n",
    "            else:\n",
    "                X_test_sh[i, :, k] = X_test[i, :, k]\n",
    "    return X_train_sh, X_test_sh\n",
    "\n",
    "def znorm(X_train, X_test, train_ind, test_ind, ind_std_train, ind_std_test, dimensions):\n",
    "    N_tr, T, D = X_train.shape\n",
    "    N_te, T, D = X_test.shape\n",
    "    X_train_sh = np.zeros(X_train.shape)\n",
    "    X_test_sh = np.zeros(X_test.shape)\n",
    "    for i in range(N_tr):\n",
    "        \n",
    "        for k in range(D):\n",
    "            if k in dimensions:\n",
    "                mag = np.mean(X_train[i, :, k], axis = 0)\n",
    "                indice = np.where(train_ind ==  I_train[i])[0][0]\n",
    "                std = ind_std_train[indice][k] * 6\n",
    "                X_train_sh[i, :, k] = (X_train[i, :, k] - mag) / std\n",
    "            else:\n",
    "                X_train_sh[i, :, k] = X_train[i, :, k]\n",
    "\n",
    "    for i in range(N_te):\n",
    "        for k in range(D):\n",
    "            if k in dimensions:\n",
    "                mag = np.mean(X_test[i, :, k], axis = 0)\n",
    "                indice = np.where(test_ind ==  I_test[i])[0][0]\n",
    "                std = ind_std_test[indice][k] * 6\n",
    "                X_test_sh[i, :, k] = (X_test[i, :, k] - mag) / std\n",
    "            else:\n",
    "                X_test_sh[i, :, k] = X_test[i, :, k]\n",
    "    return X_train_sh, X_test_sh\n",
    "\n",
    "def augmentData(X, y, augmentation, repeat = 3):\n",
    "    X_out = []\n",
    "    y_out = []\n",
    "    for i in range(repeat):\n",
    "        if augmentation == 'rotation':\n",
    "            augmented = rotation(X, angle_range=[-np.pi/8, np.pi/8])\n",
    "        elif augmentation == 'permutation':\n",
    "            augmented = permutation(X)\n",
    "        elif augmentation == 'time_warp':\n",
    "            augmented = time_warp(X, sigma=0.08)\n",
    "        elif augmentation == 'magnitude_warp':\n",
    "            augmented = magnitude_warp(X, sigma=0.8, knot=4)\n",
    "        elif augmentation == 'scaling':\n",
    "            augmented = scaling(X, sigma=0.1)\n",
    "        elif augmentation == 'jitter':\n",
    "            augmented = jitter(X, sigma=0.02)\n",
    "        else:\n",
    "            augmented = X\n",
    "        if len(X_out) == 0:\n",
    "            X_out = augmented\n",
    "            y_out = y\n",
    "        else:\n",
    "            X_out = np.concatenate((X_out, augmented), axis=0)\n",
    "            y_out = np.concatenate((y_out, y), axis=0)\n",
    "    return X_out, y_out\n",
    "\n",
    "def augment(X, y, augmentations, repeats_per_augmentation=1, include_original=False):\n",
    "    X_aug = []\n",
    "    y_aug = []\n",
    "    if include_original:\n",
    "        X_aug = X\n",
    "        y_aug = y\n",
    "    for augmentation in augmentations:\n",
    "        curr_X_aug, curr_y_aug = augmentData(X, y, augmentation, repeat=repeats_per_augmentation)\n",
    "        if len(X_aug) == 0:\n",
    "            X_aug = curr_X_aug\n",
    "            y_aug = curr_y_aug\n",
    "        else:\n",
    "            X_aug = np.concatenate((X_aug, curr_X_aug), axis=0)\n",
    "            y_aug = np.concatenate((y_aug, curr_y_aug), axis=0)\n",
    "    return X_aug, y_aug\n",
    "\n",
    "def minoritySampling(X, y):\n",
    "    rus = RandomUnderSampler(sampling_strategy='not minority', random_state=1)\n",
    "    N, T, D = X.shape\n",
    "    X_temp = X.reshape([N, T * D])\n",
    "    X_temp, y = rus.fit_resample(X_temp, y)\n",
    "    X = X_temp.reshape([X_temp.shape[0], T, D])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from cache...\n",
      "Loaded mts - N: 9000, T: 200, D: 3 \n",
      "Loaded mts - N: 1500, T: 200, D: 3 \n",
      "Features shape: (9000, 600)\n",
      "Features shape: (1500, 600)\n",
      "Subsequence length: 180\n",
      "Epoch[1] Train loss    avg: 4.212961258617699\n",
      "Epoch[10] Train loss    avg: 3.424758141767894\n",
      "Epoch[20] Train loss    avg: 3.365975374871112\n",
      "mts shape: N: 9000 -  T: 200 - D: 3\n",
      "mts shape: N: 1500 -  T: 200 - D: 3\n"
     ]
    }
   ],
   "source": [
    "from source.torch_utils import getContrastiveFeatures\n",
    "\n",
    "storage = MTSStorage('har_augmentations')\n",
    "storage.delete()\n",
    "storage.load()\n",
    "\n",
    "\n",
    "components_map = {}\n",
    "\n",
    "for k in range(KFOLDS):\n",
    "    # ------------------------ Reading the dataset ------------------------\n",
    "    if DATASET == 'HAR-UML20':\n",
    "        all_ids = har_ind_IDS\n",
    "        test_ids = all_ids[k: k + N_TESTS]\n",
    "        train_ids = all_ids[:k] + all_ids[k + N_TESTS:]        \n",
    "        \n",
    "        data = read_har_dataset('./datasets/HAR-UML20/', train_ids=train_ids, test_ids=test_ids, val_ids=[], cache=True)\n",
    "        ids_train, X_train, y_train, I_train, train_kcal_MET = data['train']\n",
    "        # ids_val, X_val, y_val, I_val, val_kcal_MET = data['val']\n",
    "        ids_test, X_test, y_test, I_test, test_kcal_MET = data['test']\n",
    "        \n",
    "        har_activities_map = {\n",
    "            0: \"Sedentary\",\n",
    "            1: \"Walking\",\n",
    "            2: \"Running\",\n",
    "            3: \"Downstairs\",\n",
    "            4: \"Upstairs\"\n",
    "        }\n",
    "\n",
    "        \n",
    "        all_dimensions = har_dimensions\n",
    "        activities_map = har_activities_map\n",
    "        \n",
    "        y_train[y_train==0] = 0\n",
    "        y_train[y_train==1] = 0\n",
    "        y_train[y_train==2] = 0\n",
    "        y_test[y_test==0] = 0\n",
    "        y_test[y_test==1] = 0\n",
    "        y_test[y_test==2] = 0\n",
    "\n",
    "        for i in range(3, len(har_activities)):\n",
    "            y_train[y_train==i] = i - 2\n",
    "            y_test[y_test==i] = i - 2\n",
    "        \n",
    "        ind_std_train = idsStd(train_ids , X_train, I_train)\n",
    "        ind_std_test = idsStd(test_ids, X_test, I_test)\n",
    "        \n",
    "        # unique, counts = np.unique(y_train, return_counts=True)\n",
    "        # unique, counts = np.unique(y_test, return_counts=True)\n",
    "        X_train, y_train = minoritySampling(X_train, y_train)\n",
    "        X_test, y_test = minoritySampling(X_test, y_test)\n",
    "        X_test_o = X_test.copy()\n",
    "        y_test_o = y_test.copy()\n",
    "        \n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    # --------------------------------AugmentData ---------------------------------------------------\n",
    "    if Z_SCORE_NORM:\n",
    "        # X_train, X_test = center(X_train, X_test, train_ids, test_ids, dimensions = [0, 1, 2])\n",
    "        X_train, X_test = znorm(X_train, X_test, train_ids, test_ids, ind_std_train, ind_std_test, dimensions = [0, 1, 2, 3, 4, 5])\n",
    "        X_test_o = X_test.copy()\n",
    "    \n",
    "    X_train, y_train = augment(X_train, y_train, repeats_per_augmentation = REPEATS_PER_AUGMENTATION, augmentations = AUGMENTATIONS, include_original = INCLUDE_ORIGINAL)\n",
    "    \n",
    "    additional = 1 if INCLUDE_ORIGINAL else 0\n",
    "    # X_train = np.repeat(X_train, REPEATS_PER_AUGMENTATION * len(AUGMENTATIONS) + additional, axis=0)\n",
    "    # y_train = np.repeat(y_train, REPEATS_PER_AUGMENTATION * len(AUGMENTATIONS) + additional, axis=0)\n",
    "    I_train = np.repeat(I_train, REPEATS_PER_AUGMENTATION * len(AUGMENTATIONS) + additional, axis=0)\n",
    "    # X_train = np.concatenate([X_train, X_train], axis=0)\n",
    "    # y_train = np.concatenate([y_train, y_train], axis=0)\n",
    "    # I_train = np.concatenate([I_train, I_train], axis=0)\n",
    "    \n",
    "    \n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    train_embeddings = []\n",
    "    test_embeddings = []\n",
    "    for dimensions in N_DIMENSIONS:    \n",
    "        X_train_f = filter_dimensions(X_train, all_dimensions, dimensions)\n",
    "        X_test_f = filter_dimensions(X_test, all_dimensions, dimensions)\n",
    "        \n",
    "        mts_train = TSerie(X = X_train_f, y = y_train, I = I_train, dimensions = dimensions, classLabels=activities_map)\n",
    "        mts_test = TSerie(X = X_test_f, y = y_test, I = I_test, dimensions = dimensions, classLabels=activities_map)\n",
    "        \n",
    "        mts_train.folding_features_v2()\n",
    "        mts_test.folding_features_v2()\n",
    "        \n",
    "        n_neighbors = 15\n",
    "        \n",
    "        # reducer = umap.UMAP(n_components=N_COMPONENTS, metric=METRIC, n_neighbors=n_neighbors)\n",
    "        # embeddings_train = reducer.fit_transform(mts_train.features, y=mts_train.y)\n",
    "        # embeddings_test = reducer.transform(mts_test.features)\n",
    "        \n",
    "        embeddings_train, embeddings_test = getContrastiveFeatures(mts_train.X.transpose([0, 2, 1]), mts_train.y, epochs =20, loss_metric='SupConLoss', X_test=mts_test.X.transpose([0, 2, 1]))\n",
    "        \n",
    "        train_embeddings.append(embeddings_train)\n",
    "        test_embeddings.append(embeddings_test)\n",
    "        \n",
    "        mts_train.features = embeddings_train\n",
    "        mts_test.features = embeddings_test\n",
    "        \n",
    "        # reducer = UMAP(n_components=N_COMPONENTS, n_neighbors=n_neighbors)\n",
    "        # nn = NearestNeighbors(n_neighbors=n_neighbors, metric=METRIC)\n",
    "        # nn.fit(mts_train.features)\n",
    "        # knn_graph = nn.kneighbors_graph(mts_train.features, mode=\"distance\")\n",
    "        # embeddings_train = reducer.fit_transform(mts_train.features, y=mts_train.y, knn_graph=knn_graph.tocsr(), convert_dtype=True)\n",
    "        \n",
    "        # knn_graph2 = nn.kneighbors_graph(mts_test.features, mode=\"distance\")\n",
    "        # embeddings_test = reducer.transform(mts_test.features, knn_graph=knn_graph2.tocsr())\n",
    "        \n",
    "        # train_embeddings.append(embeddings_train)\n",
    "        # test_embeddings.append(embeddings_test)\n",
    "        reducer = umap.UMAP(n_components=2, metric=METRIC, n_neighbors=n_neighbors)\n",
    "        coords_train = reducer.fit_transform(mts_train.features, y=mts_train.y)\n",
    "        coords_test = reducer.transform(mts_test.features)\n",
    "        \n",
    "        \n",
    "\n",
    "        # reducer = UMAP(n_components=2, n_neighbors=n_neighbors)\n",
    "        # nn = NearestNeighbors(n_neighbors=n_neighbors, metric=METRIC)\n",
    "        # nn.fit(mts_train.features)\n",
    "        # knn_graph = nn.kneighbors_graph(mts_train.features, mode=\"distance\")\n",
    "        # coords_train = reducer.fit_transform(mts_train.features, y=mts_train.y, knn_graph=knn_graph.tocsr(), convert_dtype=False)\n",
    "        \n",
    "        # knn_graph2 = nn.kneighbors_graph(mts_test.features, mode=\"distance\")\n",
    "        # coords_test = reducer.transform(mts_test.features,  knn_graph=knn_graph2.tocsr())\n",
    "        \n",
    "\n",
    "        indMapTrain = {int(ind): 'sub_' + str(ind) for ind in np.unique(mts_train.I)}\n",
    "        indMapTest = {int(ind): 'sub_' + str(ind) for ind in np.unique(mts_test.I)}\n",
    "        \n",
    "        storage.add_mts(\n",
    "            'train_{}_{}'.format(' '.join(AUGMENTATIONS), ' '.join(dimensions)), \n",
    "            mts_train.X,\n",
    "            dimensions,\n",
    "            coords={'umap': coords_train}, \n",
    "            labels={\n",
    "                'activities': mts_train.y, \n",
    "                # 'participants': mts_train.I\n",
    "            }, \n",
    "            labelsNames={'activities': activities_map, 'participants': indMapTrain},\n",
    "            sampling = True,\n",
    "            n_samples = 400\n",
    "        )\n",
    "                \n",
    "        storage.add_mts(\n",
    "            'test_{}_{}'.format(' '.join(AUGMENTATIONS), ' '.join(dimensions)), \n",
    "            mts_test.X, \n",
    "            dimensions,\n",
    "            coords={'umap': coords_test}, \n",
    "            labels={\n",
    "                'activities': mts_test.y, \n",
    "                # 'participants': mts_test.I\n",
    "            }, \n",
    "            labelsNames={'activities': activities_map, 'participants': indMapTest},\n",
    "            sampling = True,\n",
    "            n_samples = 400\n",
    "        )\n",
    "        \n",
    "        storage.save()\n",
    "    \n",
    "    names_comb = []\n",
    "    embeddings_comb = []\n",
    "    for i, combo in enumerate(powerset(list(range(len(N_DIMS_NAMES)))), 1):\n",
    "        indexes = list(combo)\n",
    "        name = ''\n",
    "        train_embedding = []\n",
    "        test_embedding = []\n",
    "        if len(indexes) == 0:\n",
    "            continue\n",
    "        for ind in indexes:\n",
    "            name = name + ' ' + N_DIMS_NAMES[ind]\n",
    "            if len(train_embedding) == 0:\n",
    "                train_embedding = train_embeddings[ind]\n",
    "                test_embedding = test_embeddings[ind]\n",
    "            else:\n",
    "                train_embedding = np.concatenate([train_embedding, train_embeddings[ind]], axis=1) \n",
    "                test_embedding = np.concatenate([test_embedding, test_embeddings[ind]], axis=1) \n",
    "        \n",
    "        names_comb.append(name)\n",
    "        embeddings_comb.append((train_embedding, test_embedding))\n",
    "    \n",
    "    for j in range(len(names_comb)):\n",
    "        name = names_comb[j]\n",
    "        # clf = AdaBoostClassifier()\n",
    "        # clf = XGBClassifier()\n",
    "        clf = svm.SVC()\n",
    "        train_feat, test_feat = embeddings_comb[j]\n",
    "        clf.fit(train_feat, mts_train.y)\n",
    "        pred_train, pred_test = clf.predict(train_feat), clf.predict(test_feat)\n",
    "        \n",
    "        f1_tr = metrics.f1_score(mts_train.y, pred_train, average='weighted')\n",
    "        f1_te = metrics.f1_score(mts_test.y, pred_test, average='weighted')\n",
    "        \n",
    "        if name not in components_map:\n",
    "            components_map[name] = [(f1_tr, f1_te)]\n",
    "        else:\n",
    "            components_map[name] = components_map[name] + [(f1_tr, f1_te)]\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Acc', '0.996 (0.000)', '0.835 (0.000)']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "path = os.path.join(RESULTS_PATH, 'har_kfold_{}.csv'.format('_'.join(AUGMENTATIONS)))\n",
    "with open(path, 'w', newline='') as csvfile:\n",
    "    row = ['Sensors', 'f1 train', 'f1 test']\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quoting=csv.QUOTE_MINIMAL)\n",
    "    spamwriter.writerow(row)\n",
    "    for name in names_comb:\n",
    "        row = [name]\n",
    "        f1_mean_tr = np.array([ f1[0] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_tr = np.array([ f1[0] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te = np.array([ f1[1] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te = np.array([ f1[1] for f1 in components_map[name]]).std()\n",
    "        \n",
    "        row = [\n",
    "            name, \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_tr, f1_stds_tr), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te, f1_stds_te), \n",
    "        ]\n",
    "        print(row)\n",
    "        spamwriter.writerow(row)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Centered  0.966 (0.017)'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Centered  0.966 (0.017)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Rotation 0.999 (0.001)', '0.955 (0.027)')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Rotation 0.999 (0.001)', '0.955 (0.027)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Original Acc', '0.998 (0.001)', '0.925 (0.045)')"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Original Acc', '0.998 (0.001)', '0.925 (0.045)'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('contrastive')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af38bbb6e377e6d688e9d49e963edf808fac73a4ecd279c84061bb0d9783d83c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
