{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from source.read_HAR_dataset import read_har_dataset, har_dimensions, har_activities, har_activities_map, har_ind_IDS\n",
    "from source.utils import  filter_dimensions\n",
    "from source.tserie import TSerie\n",
    "from source.utils import classify_dataset\n",
    "from itertools import chain, combinations\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "from sklearn import svm\n",
    "from source.utils import idsStd\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import umap\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from source.augmentation import  * \n",
    "# from cuml.datasets import make_blobs\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "from cuml.manifold import UMAP\n",
    "from cuml.cluster import DBSCAN\n",
    "\n",
    "\n",
    "sys.path.insert(0, '/home/texs/Documentos/Repositories/mts_viz')\n",
    "from server.source.storage import MTSStorage\n",
    "\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)  # allows duplicate elements\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "# Z_SCORE_NORM = True\n",
    "NORM = 1 # 0: No normalization, 1: centering 2: z_score_norm\n",
    "DATASET = 'HAR-UML20'\n",
    "KFOLDS = 10\n",
    "N_TESTS = 1\n",
    "METRIC  = 'braycurtis'\n",
    "RESULTS_PATH = 'outputs/augmentation/'\n",
    "# AUGMENTATIONS = ['rotation', 'permutation', 'time_warp', 'magnitude_warp', 'scaling', 'jitter']\n",
    "# AUGMENTATIONS = ['none']\n",
    "# AUGMENTATIONS = ['scaling']\n",
    "ALL_AUGMENTATIONS = ['none', 'rotation', 'permutation', 'time_warp', 'magnitude_warp', 'scaling', 'jitter']\n",
    "# ALL_AUGMENTATIONS = ['none', 'rotation', 'rotation', 'rotation', 'rotation', 'rotation', 'rotation']\n",
    "# ALL_AUGMENTATIONS = ['none'] * 7\n",
    "# AUGMENTATIONS = ['magnitude_warp']\n",
    "N_COMPONENTS = 32\n",
    "REPEATS_PER_AUGMENTATION = 1\n",
    "INCLUDE_ORIGINAL = True\n",
    "# N_DIMS_NAMES = ['Acc', 'Gyro', 'Mag']\n",
    "N_DIMS_NAMES = ['Acc', 'Gyro']\n",
    "# N_DIMS_NAMES = ['Acc']\n",
    "N_DIMENSIONS = [\n",
    "    [\n",
    "        'Accelerometer-X',\t\n",
    "        'Accelerometer-Y',\t\n",
    "        'Accelerometer-Z',\n",
    "    ],\n",
    "    [\n",
    "        'Gyrometer-X',\n",
    "        'Gyrometer-Y',\n",
    "        'Gyrometer-Z',\n",
    "    ],\n",
    "    # [\n",
    "    #     'Magnetometer-X',\n",
    "    #     'Magnetometer-Y',\n",
    "    #     'Magnetometer-Z'\n",
    "    # ]\n",
    "]\n",
    "\n",
    "AUG_RATIO = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    def __init__(self, n_components, n_neighbors):\n",
    "        self.reducer = UMAP(n_components=n_components, n_neighbors=n_neighbors, n_epochs=2000)\n",
    "        self.nearNeigh = NearestNeighbors(n_neighbors=n_neighbors, metric=METRIC)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.nearNeigh.fit(X)\n",
    "        knn_graph = self.nearNeigh.kneighbors_graph(X, mode=\"distance\")\n",
    "        embeddings =  self.reducer.fit_transform(X, y=y, knn_graph=knn_graph.tocsr(), convert_dtype=True)\n",
    "        return embeddings\n",
    "    \n",
    "    def transform(self, X):\n",
    "        knn_graph = self.nearNeigh.kneighbors_graph(X, mode=\"distance\")\n",
    "        embeddings =  self.reducer.transform(X, knn_graph=knn_graph.tocsr(), convert_dtype=True)\n",
    "        return embeddings\n",
    "def augmentData(X, y, I, augmentation, repeat = 3):\n",
    "    X_out = []\n",
    "    y_out = []\n",
    "    I_out = []\n",
    "    for i in range(repeat):\n",
    "        if augmentation == 'rotation':\n",
    "            augmented = rotation(X, angle_range=[-np.pi/4, np.pi/4])\n",
    "            # augmented = rotation(X, angle_range=[-np.pi/64, np.pi/64])\n",
    "        elif augmentation == 'permutation':\n",
    "            augmented = permutation(X)\n",
    "        elif augmentation == 'time_warp':\n",
    "            augmented = time_warp(X, sigma=0.03)\n",
    "        elif augmentation == 'magnitude_warp':\n",
    "            augmented = magnitude_warp(X, sigma=0.015, knot=4)\n",
    "        elif augmentation == 'scaling':\n",
    "            augmented = scaling(X, sigma=0.05)\n",
    "        elif augmentation == 'jitter':\n",
    "            augmented = jitter(X, sigma=0.005)\n",
    "        # elif augmentation == 'magnitude_pert':\n",
    "        #     augmented = magnitude_pert(X, prange=[0, 1])\n",
    "        else:\n",
    "            augmented = X\n",
    "        if len(X_out) == 0:\n",
    "            X_out = augmented\n",
    "            y_out = y\n",
    "            I_out = I\n",
    "        else:\n",
    "            X_out = np.concatenate((X_out, augmented), axis=0)\n",
    "            y_out = np.concatenate((y_out, y), axis=0)\n",
    "            I_out = np.concatenate((I_out, I), axis=0)\n",
    "    return X_out, y_out, I_out\n",
    "\n",
    "def augment(X, y, I, augmentations, repeats_per_augmentation=1, include_original=False):\n",
    "    X_aug = []\n",
    "    y_aug = []\n",
    "    I_aug = []\n",
    "    if include_original:\n",
    "        X_aug = X\n",
    "        y_aug = y\n",
    "        I_aug = I\n",
    "    for augmentation in augmentations:\n",
    "        curr_X_aug, curr_y_aug, curr_I_aug = augmentData(X, y, I, augmentation, repeat=repeats_per_augmentation)\n",
    "        if len(X_aug) == 0:\n",
    "            X_aug = curr_X_aug\n",
    "            y_aug = curr_y_aug\n",
    "            I_aug = curr_I_aug\n",
    "        else:\n",
    "            X_aug = np.concatenate((X_aug, curr_X_aug), axis=0)\n",
    "            y_aug = np.concatenate((y_aug, curr_y_aug), axis=0)\n",
    "            I_aug = np.concatenate((I_aug, curr_I_aug), axis=0)\n",
    "    return X_aug, y_aug, I_aug\n",
    "def random_augmentation(X, y, I, n_times = 1):\n",
    "    n = len(X)\n",
    "    new_n = n * n_times\n",
    "    ind = np.random.randint(0, n, new_n)\n",
    "    augmentations = ['rotation', 'permutation', 'time_warp', 'magnitude_warp', 'scaling', 'jitter']\n",
    "    per_trans_n = new_n // len(augmentations )\n",
    "    \n",
    "    all_X = []\n",
    "    all_y = []\n",
    "    all_I = []\n",
    "    for i in range(len(augmentations)):\n",
    "        n_ind = np.random.randint(0, new_n, per_trans_n)\n",
    "        sub_X = X[ind[n_ind]]\n",
    "        sub_y = y[ind[n_ind]]\n",
    "        sub_I = I[ind[n_ind]]\n",
    "        \n",
    "        \n",
    "        sub_aug, sub_y, sub_I = augment(sub_X, sub_y, sub_I, [augmentations[i]], repeats_per_augmentation=1, include_original=False)\n",
    "        \n",
    "        if len(all_X) == 0:\n",
    "            all_X = sub_aug\n",
    "            all_y = sub_y\n",
    "            all_I = sub_I\n",
    "        else:\n",
    "            all_X = np.concatenate([all_X, sub_aug], axis = 0)\n",
    "            all_y = np.concatenate([all_y, sub_y], axis = 0)\n",
    "            all_I = np.concatenate([all_I, sub_I], axis = 0)\n",
    "    return all_X, all_y, all_I\n",
    "    \n",
    "\n",
    "def minoritySampling(X, y):\n",
    "    rus = RandomUnderSampler(sampling_strategy='not minority', random_state=1)\n",
    "    N, T, D = X.shape\n",
    "    X_temp = X.reshape([N, T * D])\n",
    "    X_temp, y = rus.fit_resample(X_temp, y)\n",
    "    X = X_temp.reshape([X_temp.shape[0], T, D])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "activities_map = har_activities_map\n",
    "# activities_map = {\n",
    "#     0: \"Sedentary\",\n",
    "#     1: \"Walking\",\n",
    "#     2: \"Running\",\n",
    "#     3: \"Downstairs\",\n",
    "#     4: \"Upstairs\"\n",
    "# }\n",
    "def load_data(k):\n",
    "    all_ids = har_ind_IDS\n",
    "    test_ids = all_ids[k: k + N_TESTS]\n",
    "    train_ids = all_ids[:k] + all_ids[k + N_TESTS:]        \n",
    "    \n",
    "    data = read_har_dataset('./datasets/HAR-UML20/', train_ids=train_ids, test_ids=test_ids, val_ids=[], cache=True)\n",
    "    ids_train, X_train, y_train, I_train, train_kcal_MET = data['train']\n",
    "    # ids_val, X_val, y_val, I_val, val_kcal_MET = data['val']\n",
    "    ids_test, X_test, y_test, I_test, test_kcal_MET = data['test']\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    all_dimensions = har_dimensions\n",
    "    activities_map = har_activities_map\n",
    "    \n",
    "    # y_train[y_train==0] = 0\n",
    "    # y_train[y_train==1] = 0\n",
    "    # y_train[y_train==2] = 0\n",
    "    # y_test[y_test==0] = 0\n",
    "    # y_test[y_test==1] = 0\n",
    "    # y_test[y_test==2] = 0\n",
    "\n",
    "    # for i in range(3, len(har_activities)):\n",
    "    #     y_train[y_train==i] = i - 2\n",
    "    #     y_test[y_test==i] = i - 2\n",
    "    \n",
    "    ind_std_train = idsStd(train_ids , X_train, I_train)\n",
    "    ind_std_test = idsStd(test_ids, X_test, I_test)\n",
    "    \n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    unique, counts = np.unique(y_test, return_counts=True)\n",
    "    \n",
    "    \n",
    "    I_train = np.expand_dims(I_train, axis=1)\n",
    "    I_test = np.expand_dims(I_test, axis=1)\n",
    "    ltrain = np.arange(len(y_train))\n",
    "    ltest = np.arange(len(y_test))\n",
    "    \n",
    "    X_train, zlabels_train = minoritySampling(X_train, ltrain)\n",
    "    X_test, zlabels_test = minoritySampling(X_test, ltest)\n",
    "    \n",
    "    y_train = y_train[ltrain]\n",
    "    I_train = I_train[ltrain]\n",
    "    y_test = y_test[ltest]\n",
    "    I_test = I_test[ltest]\n",
    "    \n",
    "    y_train = train_kcal_MET[ltrain, 1]\n",
    "    y_test = test_kcal_MET[ltest, 1]\n",
    "    \n",
    "    return X_train, y_train, I_train, X_test, y_test, I_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 0\n",
      "Train IDS: [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Test IDS: [0]\n",
      "Val IDS: []\n",
      "Loading dataset from cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/texs/Documentos/Repositories/mts_feature_learning/source/augmentation.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  warp = np.concatenate(np.random.permutation(splits)).ravel()\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22050, 32)\n",
      "(22050, 32)\n",
      "(22050, 32)\n",
      "(22050, 32)\n",
      "Classifying\n",
      "Classifying done\n",
      "FOLD: 1\n",
      "Train IDS: [0, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Test IDS: [1]\n",
      "Val IDS: []\n",
      "Loading dataset from cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/texs/Documentos/Repositories/mts_feature_learning/source/augmentation.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  warp = np.concatenate(np.random.permutation(splits)).ravel()\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22050, 32)\n",
      "(22050, 32)\n",
      "(22050, 32)\n",
      "(22050, 32)\n",
      "Classifying\n",
      "Classifying done\n",
      "FOLD: 2\n",
      "Train IDS: [0, 1, 3, 4, 5, 6, 7, 8, 9]\n",
      "Test IDS: [2]\n",
      "Val IDS: []\n",
      "Loading dataset from cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/texs/Documentos/Repositories/mts_feature_learning/source/augmentation.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  warp = np.concatenate(np.random.permutation(splits)).ravel()\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22050, 32)\n",
      "(22050, 32)\n",
      "(22050, 32)\n",
      "(22050, 32)\n",
      "Classifying\n",
      "Classifying done\n",
      "FOLD: 3\n",
      "Train IDS: [0, 1, 2, 4, 5, 6, 7, 8, 9]\n",
      "Test IDS: [3]\n",
      "Val IDS: []\n",
      "Loading dataset from cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/texs/Documentos/Repositories/mts_feature_learning/source/augmentation.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  warp = np.concatenate(np.random.permutation(splits)).ravel()\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22050, 32)\n",
      "(22050, 32)\n",
      "(22050, 32)\n",
      "(22050, 32)\n",
      "Classifying\n",
      "Classifying done\n",
      "FOLD: 4\n",
      "Train IDS: [0, 1, 2, 3, 5, 6, 7, 8, 9]\n",
      "Test IDS: [4]\n",
      "Val IDS: []\n",
      "Loading dataset from cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/texs/Documentos/Repositories/mts_feature_learning/source/augmentation.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  warp = np.concatenate(np.random.permutation(splits)).ravel()\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22050, 32)\n",
      "(22050, 32)\n",
      "(22050, 32)\n",
      "(22050, 32)\n",
      "Classifying\n",
      "Classifying done\n",
      "FOLD: 5\n",
      "Train IDS: [0, 1, 2, 3, 4, 6, 7, 8, 9]\n",
      "Test IDS: [5]\n",
      "Val IDS: []\n",
      "Loading dataset from cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/texs/Documentos/Repositories/mts_feature_learning/source/augmentation.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  warp = np.concatenate(np.random.permutation(splits)).ravel()\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23310, 32)\n",
      "(23310, 32)\n",
      "(23310, 32)\n",
      "(23310, 32)\n",
      "Classifying\n",
      "Classifying done\n",
      "FOLD: 6\n",
      "Train IDS: [0, 1, 2, 3, 4, 5, 7, 8, 9]\n",
      "Test IDS: [6]\n",
      "Val IDS: []\n",
      "Loading dataset from cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/texs/Documentos/Repositories/mts_feature_learning/source/augmentation.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  warp = np.concatenate(np.random.permutation(splits)).ravel()\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23310, 32)\n",
      "(23310, 32)\n",
      "(23310, 32)\n",
      "(23310, 32)\n",
      "Classifying\n",
      "Classifying done\n",
      "FOLD: 7\n",
      "Train IDS: [0, 1, 2, 3, 4, 5, 6, 8, 9]\n",
      "Test IDS: [7]\n",
      "Val IDS: []\n",
      "Loading dataset from cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/texs/Documentos/Repositories/mts_feature_learning/source/augmentation.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  warp = np.concatenate(np.random.permutation(splits)).ravel()\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23310, 32)\n",
      "(23310, 32)\n",
      "(23310, 32)\n",
      "(23310, 32)\n",
      "Classifying\n",
      "Classifying done\n",
      "FOLD: 8\n",
      "Train IDS: [0, 1, 2, 3, 4, 5, 6, 7, 9]\n",
      "Test IDS: [8]\n",
      "Val IDS: []\n",
      "Loading dataset from cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/texs/Documentos/Repositories/mts_feature_learning/source/augmentation.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  warp = np.concatenate(np.random.permutation(splits)).ravel()\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23310, 32)\n",
      "(23310, 32)\n",
      "(23310, 32)\n",
      "(23310, 32)\n",
      "Classifying\n",
      "Classifying done\n",
      "FOLD: 9\n",
      "Train IDS: [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
      "Test IDS: [9]\n",
      "Val IDS: []\n",
      "Loading dataset from cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/texs/Documentos/Repositories/mts_feature_learning/source/augmentation.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  warp = np.concatenate(np.random.permutation(splits)).ravel()\n",
      "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23310, 32)\n",
      "(23310, 32)\n",
      "(23310, 32)\n",
      "(23310, 32)\n",
      "Classifying\n",
      "Classifying done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# from source.torch_utils import getContrastiveFeatures\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "firstTimeSave = True\n",
    "storage = MTSStorage('har_augmentations')\n",
    "# storage.delete()\n",
    "storage.load()\n",
    "\n",
    "\n",
    "components_map = {}\n",
    "\n",
    "for k in range(KFOLDS):\n",
    "    print('FOLD: {}'.format(k))\n",
    "    # ------------------------ Reading the dataset ------------------------\n",
    "    X_train, y_train, I_train, X_test, y_test, I_test = load_data(k)\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    train_ys = []\n",
    "    \n",
    "    # All test embeddings including original in ALL_AUGMENTATIONS order\n",
    "    test_embeddings = []\n",
    "    # Can either be the original or the mixture of original and augmented\n",
    "    train_embeddings = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    mts_train = TSerie(X = X_train, y = y_train, I = I_train,  classLabels=activities_map)\n",
    "    mts_test = TSerie(X = X_test, y = y_test, I = I_test,  classLabels=activities_map)\n",
    "    \n",
    "    minl, maxl = mts_train.minMaxNormalization()\n",
    "    mts_test.minMaxNormalization(minl=minl, maxl=maxl)\n",
    "    \n",
    "    if AUG_RATIO == 0:\n",
    "        X_train_all = mts_train.X\n",
    "        y_train_all = mts_train.y\n",
    "        I_train_all = mts_train.I\n",
    "    else:\n",
    "        X_rand_aug, y_rand_aug, I_rand_aug = random_augmentation(mts_train.X, mts_train.y, mts_train.I, n_times=AUG_RATIO)\n",
    "        X_train_all = np.concatenate([X_rand_aug, mts_train.X], axis=0)\n",
    "        y_train_all = np.concatenate([y_rand_aug, mts_train.y], axis=0)\n",
    "        I_train_all = np.concatenate([I_rand_aug, mts_train.I], axis=0)\n",
    "    \n",
    "    # print('ALL shape')\n",
    "    # print(X_train_all.shape)\n",
    "    # print('ALL y shape')\n",
    "    # print(y_train_all.shape)\n",
    "    \n",
    "    # Training reducer\n",
    "    if NORM == 1:\n",
    "        mts_train.center()\n",
    "        mts_test.center()\n",
    "    elif NORM == 2:\n",
    "        mts_train.znorm()\n",
    "        mts_test.znorm()\n",
    "    \n",
    "    mts_train.X_o = mts_train.X\n",
    "    mts_test.X_o = mts_test.X\n",
    "    \n",
    "    # Saving augmentations\n",
    "    test_tranformations = [\n",
    "        augment(mts_test.X, mts_test.y, mts_test.I,  repeats_per_augmentation = 1, augmentations = [ALL_AUGMENTATIONS[i]], include_original = False)\n",
    "        for i in range(len(ALL_AUGMENTATIONS))\n",
    "    ]\n",
    "    mts_train_t = TSerie(X = X_train_all, y = y_train_all, I = I_train_all, classLabels=activities_map)\n",
    "    if NORM == 1:\n",
    "        mts_train_t.center()\n",
    "    elif NORM == 2:\n",
    "        mts_train_t.znorm()\n",
    "    mts_train_t.X_o = mts_train_t.X\n",
    "    \n",
    "    test_mts = {}\n",
    "    for i in range(len(ALL_AUGMENTATIONS)):\n",
    "        tranformation_name = ALL_AUGMENTATIONS[i]\n",
    "        test_tranformations[i] = test_tranformations[i][0].copy()\n",
    "        \n",
    "        mts_test_t = TSerie(X = test_tranformations[i], y = mts_test.y, I = mts_test.I, classLabels=activities_map)\n",
    "        if NORM == 1:\n",
    "            mts_test_t.center()\n",
    "        elif NORM == 2:\n",
    "            mts_test_t.znorm()\n",
    "        \n",
    "        mts_test_t.X_o = mts_test_t.X\n",
    "        test_mts[tranformation_name] = mts_test_t\n",
    "    \n",
    "    \n",
    "    for t in range(len(N_DIMENSIONS)):\n",
    "        dimensions = N_DIMENSIONS[t]    \n",
    "        mts_train.X = filter_dimensions(mts_train.X_o, har_dimensions, dimensions)\n",
    "        mts_test.X = filter_dimensions(mts_test.X_o, har_dimensions, dimensions)\n",
    "        \n",
    "        \n",
    "        mts_train.folding_features_v2()\n",
    "        mts_test.folding_features_v2()\n",
    "\n",
    "        #  TRAIN PROJECTOR\n",
    "        n_neighbors = 15\n",
    "        reducer = FeatureExtractor(N_COMPONENTS, n_neighbors)        \n",
    "        embeddings_train = reducer.fit_transform(mts_train.features, y=mts_train.y)\n",
    "        \n",
    "        \n",
    "        mts_train_t.X = filter_dimensions(mts_train_t.X_o, har_dimensions, dimensions)\n",
    "        mts_train_t.folding_features_v2()\n",
    "        mts_train_t.features = reducer.transform(mts_train_t.features)\n",
    "        train_map = mts_train_t.features\n",
    "        \n",
    "        \n",
    "        test_map = {}\n",
    "        for i in range(len(ALL_AUGMENTATIONS)):\n",
    "            tranformation_name = ALL_AUGMENTATIONS[i]\n",
    "            mts_test_t = test_mts[tranformation_name]\n",
    "            mts_test_t.X = filter_dimensions(mts_test_t.X_o, har_dimensions, dimensions)\n",
    "            mts_test_t.folding_features_v2()\n",
    "            mts_test_t.features = reducer.transform(mts_test_t.features)\n",
    "            test_map[tranformation_name] = mts_test_t.features\n",
    "        \n",
    "        train_embeddings.append(train_map)\n",
    "        test_embeddings.append(test_map)\n",
    "        \n",
    "    \n",
    "    names_comb = []\n",
    "    embeddings_comb = []\n",
    "    for i, combo in enumerate(powerset(list(range(len(N_DIMS_NAMES)))), 1):\n",
    "        indexes = list(combo)\n",
    "        name = ''\n",
    "        train_embedding = []\n",
    "        # train_y = {}\n",
    "        test_embedding = {}\n",
    "        if len(indexes) == 0:\n",
    "            continue\n",
    "        for ind in indexes:\n",
    "            name = name + ' ' + N_DIMS_NAMES[ind]\n",
    "            if len(test_embedding) == 0:\n",
    "                for aug in ALL_AUGMENTATIONS:\n",
    "                    test_embedding[aug] = test_embeddings[ind][aug]\n",
    "            else:\n",
    "                for aug in ALL_AUGMENTATIONS:\n",
    "                    test_embedding[aug] = np.concatenate([test_embedding[aug], test_embeddings[ind][aug]], axis=1)        \n",
    "\n",
    "            print(train_embeddings[ind].shape)\n",
    "            if len(train_embedding) == 0:\n",
    "                train_embedding = train_embeddings[ind]\n",
    "            else:\n",
    "                train_embedding = np.concatenate([train_embedding, train_embeddings[ind]], axis=1)\n",
    "        \n",
    "        names_comb.append(name)\n",
    "        embeddings_comb.append((train_embedding, test_embedding))\n",
    "    \n",
    "    print('Classifying')\n",
    "    for j in range(len(names_comb)):\n",
    "        name = names_comb[j]\n",
    "        # clf = AdaBoostClassifier()\n",
    "        # clf = XGBClassifier(tree_method='gpu_hist', predictor='gpu_predictor')\n",
    "        clf = XGBRegressor(tree_method='gpu_hist', predictor='gpu_predictor')\n",
    "        # clf = svm.SVC()\n",
    "        \n",
    "        train_feat = []\n",
    "        train_y = y_train_all\n",
    "        \n",
    "        train_feat, test_feat_map = embeddings_comb[j]\n",
    "        \n",
    "        clf.fit(train_feat, train_y)\n",
    "        \n",
    "        pred_train = clf.predict(train_feat)\n",
    "        f1_tr = metrics.mean_absolute_error(train_y, pred_train)\n",
    "        \n",
    "        f1_scores = [f1_tr]\n",
    "        \n",
    "        \n",
    "        all_pred = []\n",
    "        \n",
    "        first_done = False\n",
    "        for aug in ALL_AUGMENTATIONS:\n",
    "            test_feat = test_feat_map[aug]\n",
    "            pred_test = clf.predict(test_feat)\n",
    "            f1_te = metrics.mean_absolute_error(y_test, pred_test)\n",
    "            f1_scores.append(f1_te)\n",
    "            \n",
    "            if first_done:\n",
    "                if len(all_pred) == 0:\n",
    "                    all_pred = np.expand_dims(pred_test, axis=1)\n",
    "                    \n",
    "                else:\n",
    "                    all_pred = np.concatenate([all_pred, np.expand_dims(pred_test, axis=1)], axis = 1)\n",
    "                \n",
    "            first_done = True\n",
    "        \n",
    "        vote_out = np.zeros(all_pred.shape[0])\n",
    "        # for l in range(all_pred.shape[0]):\n",
    "        #     counts = np.bincount(all_pred[l])\n",
    "        #     vote_out[l] = np.argmax(counts)\n",
    "        f1_vot = metrics.mean_absolute_error(y_test, vote_out)\n",
    "        f1_scores.append(f1_vot)\n",
    "        \n",
    "        if name not in components_map:\n",
    "            components_map[name] = [f1_scores]\n",
    "        else:\n",
    "            components_map[name] = components_map[name] + [f1_scores]\n",
    "    print('Classifying done')\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Acc & 0.29±0.01 & 0.68±0.16 & 0.68±0.16 & 0.76±0.16 & 0.69±0.16 & 0.68±0.15 & 0.68±0.15 & 0.70±0.15 & 3.24±0.38\n",
      " Gyro & 0.30±0.02 & 0.74±0.18 & 0.75±0.20 & 0.79±0.22 & 0.76±0.19 & 0.75±0.18 & 0.75±0.19 & 0.99±0.21 & 3.24±0.38\n",
      " Acc Gyro & 0.20±0.01 & 0.69±0.18 & 0.68±0.18 & 0.73±0.19 & 0.69±0.17 & 0.69±0.18 & 0.68±0.18 & 0.72±0.18 & 3.24±0.38\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv\n",
    "from source.utils import plotMatResult\n",
    "\n",
    "row_names = []\n",
    "column_names = ['Train', 'Test', 'Rotation(Te)', 'Permutation(Te)', 'TimeW(Te)', 'MagnitudeW(Te)', 'Scaling(Te)', 'Jitter(Te)', 'Voting']\n",
    "mat_data = []\n",
    "\n",
    "# path = os.path.join(RESULTS_PATH, 'Umap_mult_{}.csv'.format('_'.join(AUGMENTATIONS)))\n",
    "path = os.path.join(RESULTS_PATH, 'Umap_mult.csv')\n",
    "with open(path, 'w', newline='') as csvfile:\n",
    "    row = ['Sensors', 'f1 train', 'f1 test']\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quoting=csv.QUOTE_MINIMAL)\n",
    "    spamwriter.writerow(row)\n",
    "    for name in names_comb:\n",
    "        row_names.append(name)\n",
    "        row = [name]\n",
    "        f1_mean_tr = np.array([ f1[0] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_tr = np.array([ f1[0] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te = np.array([ f1[1] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te = np.array([ f1[1] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_rot = np.array([ f1[2] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_rot = np.array([ f1[2] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_per = np.array([ f1[3] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_per = np.array([ f1[3] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_tim = np.array([ f1[4] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_tim = np.array([ f1[4] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_mag = np.array([ f1[5] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_mag = np.array([ f1[5] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_sca = np.array([ f1[6] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_sca = np.array([ f1[6] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_jit = np.array([ f1[7] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_jit = np.array([ f1[7] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_vot = np.array([ f1[8] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_vot = np.array([ f1[8] for f1 in components_map[name]]).std()\n",
    "        \n",
    "        row = [\n",
    "            name, \n",
    "            '{:.2f}±{:.2f}'.format(f1_mean_tr, f1_stds_tr), \n",
    "            '{:.2f}±{:.2f}'.format(f1_mean_te, f1_stds_te), \n",
    "            '{:.2f}±{:.2f}'.format(f1_mean_te_rot, f1_stds_te_rot), \n",
    "            '{:.2f}±{:.2f}'.format(f1_mean_te_per, f1_stds_te_per), \n",
    "            '{:.2f}±{:.2f}'.format(f1_mean_te_tim, f1_stds_te_tim), \n",
    "            '{:.2f}±{:.2f}'.format(f1_mean_te_mag, f1_stds_te_mag), \n",
    "            '{:.2f}±{:.2f}'.format(f1_mean_te_sca, f1_stds_te_sca), \n",
    "            '{:.2f}±{:.2f}'.format(f1_mean_te_jit, f1_stds_te_jit), \n",
    "            '{:.2f}±{:.2f}'.format(f1_mean_te_vot, f1_stds_te_vot), \n",
    "        ]\n",
    "        print(' & '.join(row))\n",
    "        mat_data.append(row[1:])\n",
    "        spamwriter.writerow(row)\n",
    "\n",
    "if NORM == 1:\n",
    "    footer = 'Data centered along x-axis'\n",
    "if NORM == 2:\n",
    "    footer = 'Z score normalization'\n",
    "else:\n",
    "    footer = ''\n",
    "plotMatResult('Voting results ratio 1-{}'.format(AUG_RATIO), footer, row_names, column_names, mat_data, plot_fig=True, save_fig=True, file_name='met_ratio_1_{}_{}.png'.format(AUG_RATIO, footer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = y_test <= 1.5\n",
    "# mask = np.logical_and( y_test > 1.5 , y_test <= 3 )\n",
    "# mask = np.logical_and( y_test > 3 , y_test <= 6 )\n",
    "# mask =  y_test > 6\n",
    "# print(y_test[mask].shape)\n",
    "# metrics.mean_absolute_error(y_test[mask], pred_test[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('rapidsml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f51f79755ef8f79693022c2238daee50231b90d0a3e393b6e63d90a9477d06f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
