{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from source.read_HAR_dataset import read_har_dataset, har_dimensions, har_activities, har_activities_map, har_ind_IDS\n",
    "from source.utils import  filter_dimensions\n",
    "from source.tserie import TSerie\n",
    "from source.utils import classify_dataset\n",
    "from itertools import chain, combinations\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "from source.utils import idsStd\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import umap\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from source.augmentation import  * \n",
    "# from cuml.datasets import make_blobs\n",
    "# from cuml.neighbors import NearestNeighbors\n",
    "# from cuml.manifold import UMAP\n",
    "# from cuml.cluster import DBSCAN\n",
    "\n",
    "\n",
    "sys.path.insert(0, '/home/texs/Documentos/Repositories/mts_viz')\n",
    "from server.source.storage import MTSStorage\n",
    "\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)  # allows duplicate elements\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "NORM = 0 # 0: No normalization, 1: centering 2: z_score_norm\n",
    "DATASET = 'HAR-UML20'\n",
    "KFOLDS = 1\n",
    "N_TESTS = 2\n",
    "# EPOCHS = 15\n",
    "EPOCHS = 10\n",
    "FEATURE_SIZE = 512\n",
    "ENCODING_SIZE = 8\n",
    "METRIC  = 'braycurtis'\n",
    "RESULTS_PATH = 'outputs/augmentation/'\n",
    "# AUGMENTATIONS = ['rotation', 'permutation', 'time_warp', 'magnitude_warp', 'scaling', 'jitter']\n",
    "AUGMENTATIONS = ['none']\n",
    "# AUGMENTATIONS = ['none', 'rotation', 'permutation', 'time_warp', 'magnitude_warp', 'scaling', 'jitter']\n",
    "# AUGMENTATIONS = ['none']\n",
    "# AUGMENTATIONS = ['scaling']\n",
    "ALL_AUGMENTATIONS = ['none', 'rotation', 'permutation', 'time_warp', 'magnitude_warp', 'scaling', 'jitter']\n",
    "# ALL_AUGMENTATIONS = ['none']\n",
    "# AUGMENTATIONS = ['magnitude_warp']\n",
    "REPEATS_PER_AUGMENTATION = 1\n",
    "INCLUDE_ORIGINAL = False\n",
    "# N_DIMS_NAMES = ['Acc', 'Gyro', 'Mag']\n",
    "N_DIMS_NAMES = ['Acc', 'Gyro']\n",
    "# N_DIMS_NAMES = ['Acc']\n",
    "N_DIMENSIONS = [\n",
    "    [\n",
    "        'Accelerometer-X',\t\n",
    "        'Accelerometer-Y',\t\n",
    "        'Accelerometer-Z',\n",
    "    ],\n",
    "    [\n",
    "        'Gyrometer-X',\n",
    "        'Gyrometer-Y',\n",
    "        'Gyrometer-Z',\n",
    "    ],\n",
    "    # [\n",
    "    #     'Magnetometer-X',\n",
    "    #     'Magnetometer-Y',\n",
    "    #     'Magnetometer-Z'\n",
    "    # ]\n",
    "]\n",
    "\n",
    "firstTimeSave = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    def __init__(self, epochs = 100, batch_size = 32, loss_metric = 'SimCLR', encoding_size = 8, mode = 'subsequences'):\n",
    "        self.model = None\n",
    "        self.device = None\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_metric = loss_metric\n",
    "        self.encoding_size = encoding_size\n",
    "        self.mode = mode\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        embeddings, self.model, self.device =  getContrastiveFeatures(X.transpose([0, 2, 1]), y,\n",
    "                epochs = self.epochs, \n",
    "                loss_metric=self.loss_metric, \n",
    "                feat_size=FEATURE_SIZE, \n",
    "                encoding_size=ENCODING_SIZE,\n",
    "                mode=self.mode,\n",
    "        )\n",
    "        print(X.shape)\n",
    "        return embeddings\n",
    "    \n",
    "    def transform(self, X):\n",
    "        print(X.shape)\n",
    "        return self.model.encode(X.transpose([0, 2, 1]), self.device)\n",
    "\n",
    "\n",
    "def augmentData(X, y, augmentation, repeat = 3):\n",
    "    X_out = []\n",
    "    y_out = []\n",
    "    for i in range(repeat):\n",
    "        if augmentation == 'rotation':\n",
    "            augmented = rotation(X, angle_range=[-np.pi/4, np.pi/4])\n",
    "            # augmented = rotation(X, angle_range=[-np.pi/64, np.pi/64])\n",
    "        elif augmentation == 'permutation':\n",
    "            augmented = permutation(X)\n",
    "        elif augmentation == 'time_warp':\n",
    "            augmented = time_warp(X, sigma=0.03)\n",
    "        elif augmentation == 'magnitude_warp':\n",
    "            augmented = magnitude_warp(X, sigma=0.04, knot=4)\n",
    "        elif augmentation == 'scaling':\n",
    "            augmented = scaling(X, sigma=0.05)\n",
    "        elif augmentation == 'jitter':\n",
    "            augmented = jitter(X, sigma=0.01)\n",
    "        else:\n",
    "            augmented = X.copy()\n",
    "        if len(X_out) == 0:\n",
    "            X_out = augmented\n",
    "            y_out = y.copy()\n",
    "        else:\n",
    "            X_out = np.concatenate((X_out, augmented), axis=0)\n",
    "            y_out = np.concatenate((y_out, y), axis=0)\n",
    "    return X_out, y_out\n",
    "\n",
    "def augment(X, y, augmentations, repeats_per_augmentation=1, include_original=False):\n",
    "    X_aug = []\n",
    "    y_aug = []\n",
    "    if include_original:\n",
    "        X_aug = X.copy()\n",
    "        y_aug = y.copy()\n",
    "    for augmentation in augmentations:\n",
    "        curr_X_aug, curr_y_aug = augmentData(X, y, augmentation, repeat=repeats_per_augmentation)\n",
    "        if len(X_aug) == 0:\n",
    "            X_aug = curr_X_aug\n",
    "            y_aug = curr_y_aug\n",
    "        else:\n",
    "            X_aug = np.concatenate((X_aug, curr_X_aug), axis=0)\n",
    "            y_aug = np.concatenate((y_aug, curr_y_aug), axis=0)\n",
    "    return X_aug, y_aug\n",
    "        \n",
    "def minoritySampling(X, y):\n",
    "    rus = RandomUnderSampler(sampling_strategy='not minority', random_state=1)\n",
    "    N, T, D = X.shape\n",
    "    X_temp = X.reshape([N, T * D])\n",
    "    X_temp, y = rus.fit_resample(X_temp, y)\n",
    "    X = X_temp.reshape([X_temp.shape[0], T, D])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "activities_map = {\n",
    "    0: \"Sedentary\",\n",
    "    1: \"Walking\",\n",
    "    2: \"Running\",\n",
    "    3: \"Downstairs\",\n",
    "    4: \"Upstairs\"\n",
    "}\n",
    "all_dimensions = har_dimensions\n",
    "\n",
    "def load_data(k):\n",
    "    all_ids = har_ind_IDS\n",
    "    test_ids = all_ids[k: k + N_TESTS]\n",
    "    train_ids = all_ids[:k] + all_ids[k + N_TESTS:]        \n",
    "    \n",
    "    data = read_har_dataset('./datasets/HAR-UML20/', train_ids=train_ids, test_ids=test_ids, val_ids=[], cache=True)\n",
    "    ids_train, X_train, y_train, I_train, train_kcal_MET = data['train']\n",
    "    # ids_val, X_val, y_val, I_val, val_kcal_MET = data['val']\n",
    "    ids_test, X_test, y_test, I_test, test_kcal_MET = data['test']\n",
    "    \n",
    "    \n",
    "\n",
    "    all_dimensions = har_dimensions\n",
    "    activities_map = har_activities_map\n",
    "    \n",
    "    y_train[y_train==0] = 0\n",
    "    y_train[y_train==1] = 0\n",
    "    y_train[y_train==2] = 0\n",
    "    y_test[y_test==0] = 0\n",
    "    y_test[y_test==1] = 0\n",
    "    y_test[y_test==2] = 0\n",
    "\n",
    "    for i in range(3, len(har_activities)):\n",
    "        y_train[y_train==i] = i - 2\n",
    "        y_test[y_test==i] = i - 2\n",
    "    \n",
    "    ind_std_train = idsStd(train_ids , X_train, I_train)\n",
    "    ind_std_test = idsStd(test_ids, X_test, I_test)\n",
    "    \n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    unique, counts = np.unique(y_test, return_counts=True)\n",
    "    \n",
    "    I_train = np.expand_dims(I_train, axis=1)\n",
    "    I_test = np.expand_dims(I_test, axis=1)\n",
    "    ltrain = np.arange(len(y_train))\n",
    "    ltest = np.arange(len(y_test))\n",
    "    \n",
    "    X_train, zlabels_train = minoritySampling(X_train, ltrain)\n",
    "    X_test, zlabels_test = minoritySampling(X_test, ltest)\n",
    "    \n",
    "    y_train = y_train[ltrain]\n",
    "    I_train = I_train[ltrain]\n",
    "    y_test = y_test[ltest]\n",
    "    I_test = I_test[ltest]\n",
    "    \n",
    "    return X_train, y_train, I_train, X_test, y_test, I_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 0\n",
      "Train IDS: [2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Test IDS: [0, 1]\n",
      "Val IDS: []\n",
      "Loading dataset from cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/texs/Documentos/Repositories/mts_feature_learning/source/augmentation.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  warp = np.concatenate(np.random.permutation(splits)).ravel()\n",
      "<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEN\n",
      "7\n",
      "Subsequence length: 180\n",
      "Epoch[1] Train loss    avg: 3.935458023536024\n",
      "Epoch[10] Train loss    avg: 3.4930665831880523\n",
      "(6300, 200, 3)\n",
      "(2100, 200, 3)\n",
      "(2100, 200, 3)\n",
      "(2100, 200, 3)\n",
      "(2100, 200, 3)\n",
      "(2100, 200, 3)\n",
      "(2100, 200, 3)\n",
      "(2100, 200, 3)\n",
      "LEN\n",
      "7\n",
      "Subsequence length: 180\n",
      "Epoch[1] Train loss    avg: 3.881516998794478\n",
      "Epoch[10] Train loss    avg: 3.4603446788594203\n",
      "(6300, 200, 3)\n",
      "(2100, 200, 3)\n",
      "(2100, 200, 3)\n",
      "(2100, 200, 3)\n",
      "(2100, 200, 3)\n",
      "(2100, 200, 3)\n",
      "(2100, 200, 3)\n",
      "(2100, 200, 3)\n",
      "Classifying\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/sklearn/utils/validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying done\n"
     ]
    }
   ],
   "source": [
    "from source.torch_utils import getContrastiveFeatures\n",
    "import torch\n",
    "\n",
    "storage = MTSStorage('har_augmentations')\n",
    "storage.delete()\n",
    "storage.load()\n",
    "\n",
    "\n",
    "components_map = {}\n",
    "\n",
    "for k in range(KFOLDS):\n",
    "    \n",
    "    print('FOLD: {}'.format(k))\n",
    "    # ------------------------ Reading the dataset ------------------------\n",
    "    X_train, y_train, I_train, X_test, y_test, I_test = load_data(k+3)\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    # --------------------------------AugmentData ------------------------------------\n",
    "    all_train_mts = TSerie(X = X_train, y = y_train, I = I_train, classLabels = activities_map)\n",
    "    all_test_mts = TSerie(X = X_test, y = y_test, I = I_test, classLabels = activities_map)\n",
    "    # --------------------------------------------------------------------------------\n",
    "    \n",
    "    # minl, maxl = all_train_mts.minMaxNormalization()\n",
    "    # all_test_mts.minMaxNormalization(minl=minl, maxl=maxl)\n",
    "    \n",
    "    \n",
    "    X_train, y_train = augment(all_train_mts.X, y_train, repeats_per_augmentation = REPEATS_PER_AUGMENTATION, augmentations = AUGMENTATIONS, include_original = INCLUDE_ORIGINAL)\n",
    "    # all_train_mts.X\n",
    "    X_test = all_test_mts.X\n",
    "    \n",
    "    additional = 1 if INCLUDE_ORIGINAL else 0\n",
    "    # X_train = np.repeat(X_train, REPEATS_PER_AUGMENTATION * len(AUGMENTATIONS) + additional, axis=0)\n",
    "    # y_train = np.repeat(y_train, REPEATS_PER_AUGMENTATION * len(AUGMENTATIONS) + additional, axis=0)\n",
    "    I_train = np.repeat(I_train, REPEATS_PER_AUGMENTATION * len(AUGMENTATIONS) + additional, axis=0)\n",
    "    # X_train = np.concatenate([X_train, X_train], axis=0)\n",
    "    # y_train = np.concatenate([y_train, y_train], axis=0)\n",
    "    # I_train = np.concatenate([I_train, I_train], axis=0)\n",
    "    \n",
    "    \n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    train_embeddings = []\n",
    "    test_embeddings = []\n",
    "    for t in range(len(N_DIMENSIONS)):    \n",
    "        dimensions = N_DIMENSIONS[t]\n",
    "        X_train_f = filter_dimensions(X_train, all_dimensions, dimensions)\n",
    "        X_test_f = filter_dimensions(X_test, all_dimensions, dimensions)\n",
    "        \n",
    "        mts_train = TSerie(X = X_train_f, y = y_train, I = I_train, dimensions = dimensions, classLabels=activities_map)\n",
    "        mts_test = TSerie(X = X_test_f, y = y_test, I = I_test, dimensions = dimensions, classLabels=activities_map)\n",
    "        \n",
    "        if NORM == 1:\n",
    "            mts_train.center()\n",
    "            mts_test.center()\n",
    "        elif NORM == 2:\n",
    "            mts_train.znorm()\n",
    "            mts_test.znorm()\n",
    "        \n",
    "        test_tranformations = [\n",
    "            augment(X_test_f.copy(), y_test, repeats_per_augmentation = 1, augmentations = [ALL_AUGMENTATIONS[i]], include_original = False)\n",
    "            for i in range(len(ALL_AUGMENTATIONS))\n",
    "        ]\n",
    "        \n",
    "        print('LEN')\n",
    "        print(len(test_tranformations))\n",
    "        \n",
    "        for i in range(len(test_tranformations)):\n",
    "            test_tranformations[i] = test_tranformations[i][0]\n",
    "            # test_tranformations[i] = test_tranformations[i].transpose([0, 2, 1])\n",
    "                \n",
    "        \n",
    "        reducer = FeatureExtractor(epochs = EPOCHS, loss_metric='SupConLoss')        \n",
    "        # reducer = FeatureExtractor(epochs = EPOCHS, loss_metric='SimCLR')        \n",
    "        \n",
    "        embeddings_train = reducer.fit_transform(mts_train.X, mts_train.y)\n",
    "        test_embes = []\n",
    "        for test_tr in test_tranformations:\n",
    "            mts_aug = TSerie(X = test_tr, y = y_test, I = I_test, dimensions = dimensions, classLabels=activities_map)\n",
    "            if NORM == 1:\n",
    "                mts_aug.center()\n",
    "            elif NORM == 2:\n",
    "                mts_aug.znorm()\n",
    "            \n",
    "            embed = reducer.transform(mts_aug.X)\n",
    "            test_embes.append(embed)\n",
    "        \n",
    "        \n",
    "        test_map={}\n",
    "        for i in range(len(ALL_AUGMENTATIONS)):\n",
    "             test_map[ALL_AUGMENTATIONS[i]] = test_embes[i]\n",
    "        \n",
    "        # ['none', 'rotation', 'permutation', 'time_warp', 'magnitude_warp', 'scaling', 'jitter']\n",
    "        train_embeddings.append(embeddings_train)\n",
    "        test_embeddings.append(test_map)\n",
    "        \n",
    "        reducer = None\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        mts_train.features = embeddings_train\n",
    "\n",
    "        # reducer = umap.UMAP(n_components=2, metric=METRIC, n_neighbors=n_neighbors)\n",
    "        # coords_train = reducer.fit_transform(mts_train.features, y=mts_train.y)\n",
    "        \n",
    "    \n",
    "    names_comb = []\n",
    "    embeddings_comb = []\n",
    "    for i, combo in enumerate(powerset(list(range(len(N_DIMS_NAMES)))), 1):\n",
    "        indexes = list(combo)\n",
    "        name = ''\n",
    "        train_embedding = []\n",
    "        test_embedding = {}\n",
    "        if len(indexes) == 0:\n",
    "            continue\n",
    "        for ind in indexes:\n",
    "            name = name + ' ' + N_DIMS_NAMES[ind]\n",
    "            if len(train_embedding) == 0:\n",
    "                train_embedding = train_embeddings[ind]\n",
    "                for aug in ALL_AUGMENTATIONS:\n",
    "                    test_embedding[aug] = test_embeddings[ind][aug]\n",
    "            else:\n",
    "                train_embedding = np.concatenate([train_embedding, train_embeddings[ind]], axis=1)\n",
    "                for aug in ALL_AUGMENTATIONS:\n",
    "                    test_embedding[aug] = np.concatenate([test_embedding[aug], test_embeddings[ind][aug]], axis=1)        \n",
    "        names_comb.append(name)\n",
    "        embeddings_comb.append((train_embedding, test_embedding))\n",
    "    \n",
    "    print('Classifying')\n",
    "    for j in range(len(names_comb)):\n",
    "        name = names_comb[j]\n",
    "        # clf = AdaBoostClassifier()\n",
    "        # clf = XGBClassifier()\n",
    "        # clf = XGBClassifier(tree_method='gpu_hist', predictor='gpu_predictor')\n",
    "        clf = svm.SVC()\n",
    "        train_feat, test_feat_map = embeddings_comb[j]\n",
    "        clf.fit(train_feat, mts_train.y)\n",
    "        \n",
    "        pred_train = clf.predict(train_feat)\n",
    "        f1_tr = metrics.f1_score(mts_train.y, pred_train, average='weighted')\n",
    "        \n",
    "        f1_scores = [f1_tr]\n",
    "        \n",
    "        for aug in ALL_AUGMENTATIONS:\n",
    "            test_feat = test_feat_map[aug]\n",
    "            pred_test = clf.predict(test_feat)\n",
    "            f1_te = metrics.f1_score(y_test, pred_test, average='weighted')\n",
    "            f1_scores.append(f1_te)\n",
    "        \n",
    "        if name not in components_map:\n",
    "            components_map[name] = [f1_scores]\n",
    "        else:\n",
    "            components_map[name] = components_map[name] + [f1_scores]\n",
    "    print('Classifying done')\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Acc', '0.998 (0.000)', '0.637 (0.000)', '0.668 (0.000)', '0.581 (0.000)', '0.612 (0.000)', '0.614 (0.000)', '0.680 (0.000)', '0.607 (0.000)']\n",
      "[' Gyro', '0.999 (0.000)', '0.678 (0.000)', '0.635 (0.000)', '0.621 (0.000)', '0.614 (0.000)', '0.664 (0.000)', '0.668 (0.000)', '0.675 (0.000)']\n",
      "[' Acc Gyro', '0.999 (0.000)', '0.680 (0.000)', '0.696 (0.000)', '0.606 (0.000)', '0.637 (0.000)', '0.670 (0.000)', '0.708 (0.000)', '0.646 (0.000)']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "path = os.path.join(RESULTS_PATH, 'har_kfold_{}.csv'.format('_'.join(AUGMENTATIONS)))\n",
    "with open(path, 'w', newline='') as csvfile:\n",
    "    row = ['Sensors', 'f1 train', 'f1 test']\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quoting=csv.QUOTE_MINIMAL)\n",
    "    spamwriter.writerow(row)\n",
    "    for name in names_comb:\n",
    "        row = [name]\n",
    "        f1_mean_tr = np.array([ f1[0] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_tr = np.array([ f1[0] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te = np.array([ f1[1] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te = np.array([ f1[1] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_rot = np.array([ f1[2] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_rot = np.array([ f1[2] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_per = np.array([ f1[3] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_per = np.array([ f1[3] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_tim = np.array([ f1[4] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_tim = np.array([ f1[4] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_mag = np.array([ f1[5] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_mag = np.array([ f1[5] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_sca = np.array([ f1[6] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_sca = np.array([ f1[6] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_jit = np.array([ f1[7] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_jit = np.array([ f1[7] for f1 in components_map[name]]).std()\n",
    "        \n",
    "        row = [\n",
    "            name, \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_tr, f1_stds_tr), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te, f1_stds_te), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te_rot, f1_stds_te_rot), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te_per, f1_stds_te_per), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te_tim, f1_stds_te_tim), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te_mag, f1_stds_te_mag), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te_sca, f1_stds_te_sca), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te_jit, f1_stds_te_jit), \n",
    "        ]\n",
    "        print(row)\n",
    "        spamwriter.writerow(row)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('contrastive')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af38bbb6e377e6d688e9d49e963edf808fac73a4ecd279c84061bb0d9783d83c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
