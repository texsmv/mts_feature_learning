{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from source.read_HAR_dataset import read_har_dataset, har_dimensions, har_activities, har_activities_map, har_ind_IDS\n",
    "from source.utils import  filter_dimensions\n",
    "from source.tserie import TSerie\n",
    "from source.utils import classify_dataset\n",
    "from itertools import chain, combinations\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "from source.utils import idsStd\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import umap\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from source.augmentation import  * \n",
    "# from cuml.datasets import make_blobs\n",
    "from cuml.neighbors import NearestNeighbors\n",
    "from cuml.manifold import UMAP\n",
    "from cuml.cluster import DBSCAN\n",
    "\n",
    "\n",
    "sys.path.insert(0, '/home/texs/Documentos/Repositories/mts_viz')\n",
    "from server.source.storage import MTSStorage\n",
    "\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)  # allows duplicate elements\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "# Z_SCORE_NORM = True\n",
    "NORM = 0 # 0: No normalization, 1: centering 2: z_score_norm\n",
    "DATASET = 'HAR-UML20'\n",
    "KFOLDS = 1\n",
    "N_TESTS = 2\n",
    "N_COMPONENTS = 32\n",
    "METRIC  = 'braycurtis'\n",
    "RESULTS_PATH = 'outputs/augmentation/'\n",
    "# AUGMENTATIONS = ['rotation', 'permutation', 'time_warp', 'magnitude_warp', 'scaling', 'jitter']\n",
    "# AUGMENTATIONS = ['none']\n",
    "AUGMENTATIONS = ['scaling']\n",
    "# ALL_AUGMENTATIONS = ['none', 'rotation', 'permutation', 'time_warp', 'magnitude_warp', 'scaling', 'jitter']\n",
    "# ALL_AUGMENTATIONS = ['none', 'rotation', 'rotation', 'rotation', 'rotation', 'rotation', 'rotation']\n",
    "ALL_AUGMENTATIONS = ['none'] * 7\n",
    "# AUGMENTATIONS = ['magnitude_warp']\n",
    "REPEATS_PER_AUGMENTATION = 1\n",
    "INCLUDE_ORIGINAL = True\n",
    "# N_DIMS_NAMES = ['Acc', 'Gyro', 'Mag']\n",
    "N_DIMS_NAMES = ['Acc', 'Gyro']\n",
    "# N_DIMS_NAMES = ['Acc']\n",
    "N_DIMENSIONS = [\n",
    "    [\n",
    "        'Accelerometer-X',\t\n",
    "        'Accelerometer-Y',\t\n",
    "        'Accelerometer-Z',\n",
    "    ],\n",
    "    [\n",
    "        'Gyrometer-X',\n",
    "        'Gyrometer-Y',\n",
    "        'Gyrometer-Z',\n",
    "    ],\n",
    "    # [\n",
    "    #     'Magnetometer-X',\n",
    "    #     'Magnetometer-Y',\n",
    "    #     'Magnetometer-Z'\n",
    "    # ]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    def __init__(self, n_components, n_neighbors):\n",
    "        self.reducer = UMAP(n_components=n_components, n_neighbors=n_neighbors, n_epochs=2000)\n",
    "        self.nearNeigh = NearestNeighbors(n_neighbors=n_neighbors, metric=METRIC)\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.nearNeigh.fit(X)\n",
    "        knn_graph = self.nearNeigh.kneighbors_graph(X, mode=\"distance\")\n",
    "        embeddings =  self.reducer.fit_transform(X, y=y, knn_graph=knn_graph.tocsr(), convert_dtype=True)\n",
    "        return embeddings\n",
    "    \n",
    "    def transform(self, X):\n",
    "        knn_graph = self.nearNeigh.kneighbors_graph(X, mode=\"distance\")\n",
    "        embeddings =  self.reducer.transform(X, knn_graph=knn_graph.tocsr(), convert_dtype=True)\n",
    "        return embeddings\n",
    "\n",
    "def augmentData(X, y, augmentation, repeat = 3):\n",
    "    X_out = []\n",
    "    y_out = []\n",
    "    for i in range(repeat):\n",
    "        if augmentation == 'rotation':\n",
    "            augmented = rotation(X, angle_range=[-np.pi/4, np.pi/4])\n",
    "            # augmented = rotation(X, angle_range=[-np.pi/64, np.pi/64])\n",
    "        elif augmentation == 'permutation':\n",
    "            augmented = permutation(X)\n",
    "        elif augmentation == 'time_warp':\n",
    "            augmented = time_warp(X, sigma=0.03)\n",
    "        elif augmentation == 'magnitude_warp':\n",
    "            augmented = magnitude_warp(X, sigma=0.04, knot=4)\n",
    "        elif augmentation == 'scaling':\n",
    "            augmented = scaling(X, sigma=0.05)\n",
    "        elif augmentation == 'jitter':\n",
    "            augmented = jitter(X, sigma=0.01)\n",
    "        # elif augmentation == 'magnitude_pert':\n",
    "        #     augmented = magnitude_pert(X, prange=[0, 1])\n",
    "        else:\n",
    "            augmented = X\n",
    "        if len(X_out) == 0:\n",
    "            X_out = augmented\n",
    "            y_out = y\n",
    "        else:\n",
    "            X_out = np.concatenate((X_out, augmented), axis=0)\n",
    "            y_out = np.concatenate((y_out, y), axis=0)\n",
    "    return X_out, y_out\n",
    "\n",
    "def augment(X, y, augmentations, repeats_per_augmentation=1, include_original=False):\n",
    "    X_aug = []\n",
    "    y_aug = []\n",
    "    if include_original:\n",
    "        X_aug = X\n",
    "        y_aug = y\n",
    "    for augmentation in augmentations:\n",
    "        curr_X_aug, curr_y_aug = augmentData(X, y, augmentation, repeat=repeats_per_augmentation)\n",
    "        if len(X_aug) == 0:\n",
    "            X_aug = curr_X_aug\n",
    "            y_aug = curr_y_aug\n",
    "        else:\n",
    "            X_aug = np.concatenate((X_aug, curr_X_aug), axis=0)\n",
    "            y_aug = np.concatenate((y_aug, curr_y_aug), axis=0)\n",
    "    return X_aug, y_aug\n",
    "\n",
    "def minoritySampling(X, y):\n",
    "    rus = RandomUnderSampler(sampling_strategy='not minority', random_state=1)\n",
    "    N, T, D = X.shape\n",
    "    X_temp = X.reshape([N, T * D])\n",
    "    X_temp, y = rus.fit_resample(X_temp, y)\n",
    "    X = X_temp.reshape([X_temp.shape[0], T, D])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "activities_map = {\n",
    "    0: \"Sedentary\",\n",
    "    1: \"Walking\",\n",
    "    2: \"Running\",\n",
    "    3: \"Downstairs\",\n",
    "    4: \"Upstairs\"\n",
    "}\n",
    "def load_data(k):\n",
    "    all_ids = har_ind_IDS\n",
    "    test_ids = all_ids[k: k + N_TESTS]\n",
    "    train_ids = all_ids[:k] + all_ids[k + N_TESTS:]        \n",
    "    \n",
    "    data = read_har_dataset('./datasets/HAR-UML20/', train_ids=train_ids, test_ids=test_ids, val_ids=[], cache=True)\n",
    "    ids_train, X_train, y_train, I_train, train_kcal_MET = data['train']\n",
    "    # ids_val, X_val, y_val, I_val, val_kcal_MET = data['val']\n",
    "    ids_test, X_test, y_test, I_test, test_kcal_MET = data['test']\n",
    "    \n",
    "    \n",
    "\n",
    "    all_dimensions = har_dimensions\n",
    "    activities_map = har_activities_map\n",
    "    \n",
    "    y_train[y_train==0] = 0\n",
    "    y_train[y_train==1] = 0\n",
    "    y_train[y_train==2] = 0\n",
    "    y_test[y_test==0] = 0\n",
    "    y_test[y_test==1] = 0\n",
    "    y_test[y_test==2] = 0\n",
    "\n",
    "    for i in range(3, len(har_activities)):\n",
    "        y_train[y_train==i] = i - 2\n",
    "        y_test[y_test==i] = i - 2\n",
    "    \n",
    "    ind_std_train = idsStd(train_ids , X_train, I_train)\n",
    "    ind_std_test = idsStd(test_ids, X_test, I_test)\n",
    "    \n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    unique, counts = np.unique(y_test, return_counts=True)\n",
    "    \n",
    "    I_train = np.expand_dims(I_train, axis=1)\n",
    "    I_test = np.expand_dims(I_test, axis=1)\n",
    "    ltrain = np.arange(len(y_train))\n",
    "    ltest = np.arange(len(y_test))\n",
    "    \n",
    "    X_train, zlabels_train = minoritySampling(X_train, ltrain)\n",
    "    X_test, zlabels_test = minoritySampling(X_test, ltest)\n",
    "    \n",
    "    y_train = y_train[ltrain]\n",
    "    I_train = I_train[ltrain]\n",
    "    y_test = y_test[ltest]\n",
    "    I_test = I_test[ltest]\n",
    "    \n",
    "    return X_train, y_train, I_train, X_test, y_test, I_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 0\n",
      "Train IDS: [2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Test IDS: [0, 1]\n",
      "Val IDS: []\n",
      "Loading dataset from cache...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_19972/934762083.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mNORM\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mmts_train_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mznorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mmts_train_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfolding_features_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m             \u001b[0mmts_train_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreducer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmts_train_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mtrain_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mALL_AUGMENTATIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmts_train_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documentos/Repositories/mts_feature_learning/source/tserie.py\u001b[0m in \u001b[0;36mfolding_features_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Features shape: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from source.torch_utils import getContrastiveFeatures\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "firstTimeSave = True\n",
    "storage = MTSStorage('har_augmentations')\n",
    "# storage.delete()\n",
    "storage.load()\n",
    "\n",
    "\n",
    "components_map = {}\n",
    "\n",
    "for k in range(KFOLDS):\n",
    "    print('FOLD: {}'.format(k))\n",
    "    # ------------------------ Reading the dataset ------------------------\n",
    "    X_train, y_train, I_train, X_test, y_test, I_test = load_data(k)\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    train_embeddings = []\n",
    "    train_ys = []\n",
    "    test_embeddings = []\n",
    "    \n",
    "    augment_idx = []\n",
    "    all_y = []\n",
    "    cont = 0\n",
    "    for aug in ALL_AUGMENTATIONS:\n",
    "        if aug == 'none':\n",
    "            idx_trans = np.random.choice(np.arange(len(y_train)), len(y_train), replace=False)\n",
    "        else:\n",
    "            n = len(y_train)\n",
    "            nsize = len(y_train) // (len(ALL_AUGMENTATIONS) - 1)\n",
    "            begin = cont * nsize\n",
    "            end = (cont + 1) * nsize\n",
    "            idx_trans = np.arange(len(y_train))[begin: end]\n",
    "            # idx_trans = np.random.choice(np.arange(len(y_train_o)), , replace=False)\n",
    "            cont = cont + 1\n",
    "        # idx_trans = np.random.choice(np.arange(len(y_train_o)), len(y_train_o) // (len(ALL_AUGMENTATIONS)), replace=False)\n",
    "        augment_idx.append(idx_trans)\n",
    "        all_y.append(y_train[idx_trans])\n",
    "        \n",
    "    \n",
    "    for t in range(len(N_DIMENSIONS)):\n",
    "        dimensions = N_DIMENSIONS[t]    \n",
    "        X_train_f = filter_dimensions(X_train, har_dimensions, dimensions)\n",
    "        X_test_f = filter_dimensions(X_test, har_dimensions, dimensions)\n",
    "        \n",
    "        mts_train = TSerie(X = X_train_f, y = y_train, I = I_train, dimensions = dimensions, classLabels=activities_map)\n",
    "        mts_test = TSerie(X = X_test_f, y = y_test, I = I_test, dimensions = dimensions, classLabels=activities_map)\n",
    "        \n",
    "        minl, maxl = mts_train.minMaxNormalization()\n",
    "        mts_test.minMaxNormalization(minl=minl, maxl=maxl)\n",
    "        \n",
    "        \n",
    "        # Saving augmentations\n",
    "        test_tranformations = [\n",
    "            augment(mts_test.X, mts_test.y, repeats_per_augmentation = 1, augmentations = [ALL_AUGMENTATIONS[i]], include_original = False)\n",
    "            for i in range(len(ALL_AUGMENTATIONS))\n",
    "        ]\n",
    "        train_tranformations = [\n",
    "            augment(mts_train.X, mts_train.y, repeats_per_augmentation = 1, augmentations = [ALL_AUGMENTATIONS[i]], include_original = False)\n",
    "            for i in range(len(ALL_AUGMENTATIONS))\n",
    "        ]\n",
    "        \n",
    "        # Training reducer\n",
    "        if NORM == 1:\n",
    "            mts_train.center()\n",
    "            mts_test.center()\n",
    "        elif NORM == 2:\n",
    "            mts_train.znorm()\n",
    "            mts_test.znorm()\n",
    "        \n",
    "        mts_train.folding_features_v2()\n",
    "        mts_test.folding_features_v2()\n",
    "        n_neighbors = 15\n",
    "        reducer = FeatureExtractor(N_COMPONENTS, n_neighbors)        \n",
    "        embeddings_train = reducer.fit_transform(mts_train.features, y=mts_train.y)\n",
    "        embeddings_test = reducer.transform(mts_test.features)\n",
    "        \n",
    "\n",
    "        # Transforming augmentations        \n",
    "        test_map = {}\n",
    "        train_map = {}\n",
    "        for i in range(len(ALL_AUGMENTATIONS)):\n",
    "            tranformation_name = ALL_AUGMENTATIONS[i]\n",
    "            \n",
    "            # Testing\n",
    "            test_tranformations[i] = test_tranformations[i][0].copy()\n",
    "            train_tranformations[i] = train_tranformations[i][0].copy()\n",
    "            \n",
    "            mts_test_t = TSerie(X = test_tranformations[i], y = mts_test.y, I = mts_test.I, dimensions = dimensions, classLabels=activities_map)\n",
    "            if NORM == 1:\n",
    "                mts_test_t.center()\n",
    "            elif NORM == 2:\n",
    "                mts_test_t.znorm()\n",
    "            mts_test_t.folding_features_v2()\n",
    "            mts_test_t.features = reducer.transform(mts_test_t.features)\n",
    "            test_map[ALL_AUGMENTATIONS[i]] = mts_test_t.features\n",
    "\n",
    "            # Training\n",
    "            idx = augment_idx[i]\n",
    "            mts_train_t = TSerie(X = train_tranformations[i][idx], y = y_train[idx], I = I_train[idx], dimensions = dimensions, classLabels=activities_map)\n",
    "            if NORM == 1:\n",
    "                mts_train_t.center()\n",
    "            elif NORM == 2:\n",
    "                mts_train_t.znorm()\n",
    "            mts_train_t.folding_features_v2()\n",
    "            mts_train_t.features = reducer.transform(mts_train_t.features)\n",
    "            train_map[ALL_AUGMENTATIONS[i]] = mts_train_t.features\n",
    "        \n",
    "        train_embeddings.append(train_map)\n",
    "        test_embeddings.append(test_map)\n",
    "        \n",
    "        # embeddings_train, embeddings_test = getContrastiveFeatures(mts_train.X.transpose([0, 2, 1]), mts_train.y, epochs =20, loss_metric='SupConLoss', X_test=mts_test.X.transpose([0, 2, 1]))\n",
    "            # if firstTimeSave:\n",
    "                # print(np.random.random([test_tranformations[i].shape[0], 2]).shape)\n",
    "                # storage.add_mts(\n",
    "                #     'test {}_{}'.format(ALL_AUGMENTATIONS[i], N_DIMS_NAMES[t]),\n",
    "                #     mts_test_t.X, \n",
    "                #     dimensions,\n",
    "                #     coords={\n",
    "                #         'umap': mts_test_t.features\n",
    "                #     }, \n",
    "                #     labels={\n",
    "                #         'activities': mts_test.y, \n",
    "                #         # 'participants': mts_test.I\n",
    "                #     }, \n",
    "                #     labelsNames={'activities': activities_map },\n",
    "                #     sampling = True,\n",
    "                #     n_samples = 400\n",
    "                # )\n",
    "                # storage.add_mts(\n",
    "                #     'train {}_{}'.format(ALL_AUGMENTATIONS[i], N_DIMS_NAMES[t]),\n",
    "                #     mts_train_t.X, \n",
    "                #     dimensions,\n",
    "                #     coords={\n",
    "                #         'umap': mts_train_t.features\n",
    "                #     }, \n",
    "                #     labels={\n",
    "                #         'activities': mts_train_t.y, \n",
    "                #         # 'participants': mts_test.I\n",
    "                #     }, \n",
    "                #     labelsNames={'activities': activities_map },\n",
    "                #     sampling = True,\n",
    "                #     n_samples = 400\n",
    "                # )\n",
    "                # storage.save()\n",
    "            \n",
    "    firstTimeSave = False\n",
    "    \n",
    "    names_comb = []\n",
    "    embeddings_comb = []\n",
    "    for i, combo in enumerate(powerset(list(range(len(N_DIMS_NAMES)))), 1):\n",
    "        indexes = list(combo)\n",
    "        name = ''\n",
    "        train_embedding = {}\n",
    "        # train_y = {}\n",
    "        test_embedding = {}\n",
    "        if len(indexes) == 0:\n",
    "            continue\n",
    "        for ind in indexes:\n",
    "            name = name + ' ' + N_DIMS_NAMES[ind]\n",
    "            if len(test_embedding) == 0:\n",
    "                for aug in ALL_AUGMENTATIONS:\n",
    "                    test_embedding[aug] = test_embeddings[ind][aug]\n",
    "            else:\n",
    "                for aug in ALL_AUGMENTATIONS:\n",
    "                    test_embedding[aug] = np.concatenate([test_embedding[aug], test_embeddings[ind][aug]], axis=1)        \n",
    "\n",
    "            if len(train_embedding) == 0:\n",
    "                for aug in ALL_AUGMENTATIONS:\n",
    "                    train_embedding[aug] = train_embeddings[ind][aug]\n",
    "            else:\n",
    "                for aug in ALL_AUGMENTATIONS:\n",
    "                    train_embedding[aug] = np.concatenate([train_embedding[aug], train_embeddings[ind][aug]], axis=1)\n",
    "        \n",
    "        names_comb.append(name)\n",
    "        embeddings_comb.append((train_embedding, test_embedding))\n",
    "    \n",
    "    print('Classifying')\n",
    "    for j in range(len(names_comb)):\n",
    "        name = names_comb[j]\n",
    "        # clf = AdaBoostClassifier()\n",
    "        clf = XGBClassifier(tree_method='gpu_hist', predictor='gpu_predictor')\n",
    "        # clf = svm.SVC()\n",
    "        \n",
    "        train_feat = []\n",
    "        train_y = []\n",
    "        \n",
    "        train_feat_map, test_feat_map = embeddings_comb[j]\n",
    "        for a in range(len(ALL_AUGMENTATIONS)):\n",
    "            aug = ALL_AUGMENTATIONS[a]\n",
    "            if(len(train_feat)) == 0:\n",
    "                train_feat = train_feat_map[aug]\n",
    "                train_y = all_y[a]\n",
    "            else:\n",
    "                train_feat = np.concatenate([train_feat, train_feat_map[aug]], axis = 0) \n",
    "                train_y = np.concatenate([train_y, all_y[a]], axis = 0) \n",
    "        # train_y = \n",
    "        clf.fit(train_feat, train_y)\n",
    "        \n",
    "        pred_train = clf.predict(train_feat)\n",
    "        f1_tr = metrics.f1_score(train_y, pred_train, average='weighted')\n",
    "        \n",
    "        f1_scores = [f1_tr]\n",
    "        \n",
    "        for aug in ALL_AUGMENTATIONS:\n",
    "            test_feat = test_feat_map[aug]\n",
    "            pred_test = clf.predict(test_feat)\n",
    "            f1_te = metrics.f1_score(y_test, pred_test, average='weighted')\n",
    "            f1_scores.append(f1_te)\n",
    "        \n",
    "        if name not in components_map:\n",
    "            components_map[name] = [f1_scores]\n",
    "        else:\n",
    "            components_map[name] = components_map[name] + [f1_scores]\n",
    "    print('Classifying done')\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_embeddings[0]['rotation'][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Acc', '0.307 (0.003)', '0.288 (0.012)', '0.288 (0.012)', '0.288 (0.012)', '0.288 (0.012)', '0.288 (0.012)', '0.288 (0.012)', '0.288 (0.012)']\n",
      "[' Gyro', '0.325 (0.003)', '0.305 (0.021)', '0.305 (0.021)', '0.305 (0.021)', '0.305 (0.021)', '0.305 (0.021)', '0.305 (0.021)', '0.305 (0.021)']\n",
      "[' Acc Gyro', '0.331 (0.003)', '0.327 (0.027)', '0.327 (0.027)', '0.327 (0.027)', '0.327 (0.027)', '0.327 (0.027)', '0.327 (0.027)', '0.327 (0.027)']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv\n",
    "from source.utils import plotMatResult\n",
    "\n",
    "row_names = []\n",
    "column_names = ['Train', 'Test', 'Rotation(Te)', 'Permutation(Te)', 'TimeW(Te)', 'MagnitudeW(Te)', 'Scaling(Te)', 'Jitter(Te)']\n",
    "mat_data = []\n",
    "\n",
    "path = os.path.join(RESULTS_PATH, 'Umap_kfold_{}.csv'.format('_'.join(AUGMENTATIONS)))\n",
    "with open(path, 'w', newline='') as csvfile:\n",
    "    row = ['Sensors', 'f1 train', 'f1 test']\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quoting=csv.QUOTE_MINIMAL)\n",
    "    spamwriter.writerow(row)\n",
    "    for name in names_comb:\n",
    "        row_names.append(name)\n",
    "        row = [name]\n",
    "        f1_mean_tr = np.array([ f1[0] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_tr = np.array([ f1[0] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te = np.array([ f1[1] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te = np.array([ f1[1] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_rot = np.array([ f1[2] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_rot = np.array([ f1[2] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_per = np.array([ f1[3] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_per = np.array([ f1[3] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_tim = np.array([ f1[4] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_tim = np.array([ f1[4] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_mag = np.array([ f1[5] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_mag = np.array([ f1[5] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_sca = np.array([ f1[6] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_sca = np.array([ f1[6] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_jit = np.array([ f1[7] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_jit = np.array([ f1[7] for f1 in components_map[name]]).std()\n",
    "        \n",
    "        row = [\n",
    "            name, \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_tr, f1_stds_tr), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te, f1_stds_te), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te_rot, f1_stds_te_rot), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te_per, f1_stds_te_per), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te_tim, f1_stds_te_tim), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te_mag, f1_stds_te_mag), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te_sca, f1_stds_te_sca), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te_jit, f1_stds_te_jit), \n",
    "        ]\n",
    "        mat_data.append(row[1:])\n",
    "        print(row)\n",
    "        spamwriter.writerow(row)\n",
    "\n",
    "\n",
    "plotMatResult('results', 'footer', row_names, column_names, mat_data, plot_fig=True, save_fig=True, file_name='umap_none.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Acc Gyro',\n",
       " '0.951 (0.000)',\n",
       " '0.600 (0.000)',\n",
       " '0.582 (0.000)',\n",
       " '0.570 (0.000)',\n",
       " '0.523 (0.000)',\n",
       " '0.479 (0.000)',\n",
       " '0.591 (0.000)',\n",
       " '0.594 (0.000)']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[' Acc', '0.755 (0.000)', '0.420 (0.000)', '0.435 (0.000)', '0.423 (0.000)', '0.360 (0.000)', '0.365 (0.000)', '0.430 (0.000)', '0.423 (0.000)']\n",
    "[' Gyro', '0.769 (0.000)', '0.458 (0.000)', '0.461 (0.000)', '0.435 (0.000)', '0.446 (0.000)', '0.416 (0.000)', '0.453 (0.000)', '0.459 (0.000)']\n",
    "[' Acc Gyro', '0.951 (0.000)', '0.600 (0.000)', '0.582 (0.000)', '0.570 (0.000)', '0.523 (0.000)', '0.479 (0.000)', '0.591 (0.000)', '0.594 (0.000)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Rotation 0.999 (0.001)', '0.955 (0.027)')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Rotation 0.999 (0.001)', '0.955 (0.027)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Original Acc', '0.998 (0.001)', '0.925 (0.045)')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Original Acc', '0.998 (0.001)', '0.925 (0.045)'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('rapidsml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4f51f79755ef8f79693022c2238daee50231b90d0a3e393b6e63d90a9477d06f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
