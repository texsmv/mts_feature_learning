{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from source.read_HAR_dataset import read_har_dataset, har_dimensions, har_activities, har_activities_map, har_ind_IDS\n",
    "from source.utils import  filter_dimensions\n",
    "from source.tserie import TSerie\n",
    "from source.utils import classify_dataset\n",
    "from itertools import chain, combinations\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "from source.utils import idsStd\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import umap\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from source.augmentation import  * \n",
    "# from cuml.datasets import make_blobs\n",
    "# from cuml.neighbors import NearestNeighbors\n",
    "# from cuml.manifold import UMAP\n",
    "# from cuml.cluster import DBSCAN\n",
    "\n",
    "\n",
    "sys.path.insert(0, '/home/texs/Documentos/Repositories/mts_viz')\n",
    "from server.source.storage import MTSStorage\n",
    "\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)  # allows duplicate elements\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "N_COMPONENTS=64\n",
    "Z_SCORE_NORM = True\n",
    "DATASET = 'HAR-UML20'\n",
    "KFOLDS = 1\n",
    "N_TESTS = 2\n",
    "METRIC  = 'braycurtis'\n",
    "RESULTS_PATH = 'outputs/augmentation/'\n",
    "AUGMENTATIONS = ['none', 'rotation', 'permutation', 'time_warp', 'magnitude_warp', 'scaling', 'jitter']\n",
    "# AUGMENTATIONS = ['none']\n",
    "# AUGMENTATIONS = ['scaling']\n",
    "ALL_AUGMENTATIONS = ['none', 'rotation', 'permutation', 'time_warp', 'magnitude_warp', 'scaling', 'jitter']\n",
    "# AUGMENTATIONS = ['magnitude_warp']\n",
    "REPEATS_PER_AUGMENTATION = 1\n",
    "INCLUDE_ORIGINAL = True\n",
    "# N_DIMS_NAMES = ['Acc', 'Gyro', 'Mag']\n",
    "N_DIMS_NAMES = ['Acc', 'Gyro']\n",
    "# N_DIMS_NAMES = ['Acc']\n",
    "N_DIMENSIONS = [\n",
    "    [\n",
    "        'Accelerometer-X',\t\n",
    "        'Accelerometer-Y',\t\n",
    "        'Accelerometer-Z',\n",
    "    ],\n",
    "    [\n",
    "        'Gyrometer-X',\n",
    "        'Gyrometer-Y',\n",
    "        'Gyrometer-Z',\n",
    "    ],\n",
    "    # [\n",
    "    #     'Magnetometer-X',\n",
    "    #     'Magnetometer-Y',\n",
    "    #     'Magnetometer-Z'\n",
    "    # ]\n",
    "]\n",
    "\n",
    "firstTimeSave = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def center(X_train, X_test, train_ind, test_ind, dimensions ):\n",
    "    N_tr, T, D = X_train.shape\n",
    "    N_te, T, D = X_test.shape\n",
    "    X_train_sh = np.zeros(X_train.shape)\n",
    "    X_test_sh = np.zeros(X_test.shape)\n",
    "    for i in range(N_tr):\n",
    "        \n",
    "        for k in range(D):\n",
    "            if k in dimensions:\n",
    "                mag = np.mean(X_train[i, :, k], axis = 0)\n",
    "                indice = np.where(train_ind ==  I_train[i])[0][0]\n",
    "                std = ind_std_train[indice][k] * 6\n",
    "                X_train_sh[i, :, k] = (X_train[i, :, k] - mag)\n",
    "                # X_train_sh[i, :, k] = np.concatenate([[0], fft(X_train[i, :, k])[1:]])\n",
    "            else:\n",
    "                X_train_sh[i, :, k] = X_train[i, :, k]\n",
    "\n",
    "    for i in range(N_te):\n",
    "        for k in range(D):\n",
    "            if k in dimensions:\n",
    "                mag = np.mean(X_test[i, :, k], axis = 0)\n",
    "                indice = np.where(test_ind ==  I_test[i])[0][0]\n",
    "                std = ind_std_test[indice][k] * 6\n",
    "                X_test_sh[i, :, k] = (X_test[i, :, k] - mag)\n",
    "                # X_test_sh[i, :, k] = fft(X_test[i, :, k])[0:]\n",
    "            else:\n",
    "                X_test_sh[i, :, k] = X_test[i, :, k]\n",
    "    return X_train_sh, X_test_sh\n",
    "\n",
    "def znorm(X_train, X_test, train_ind, test_ind, ind_std_train, ind_std_test, dimensions):\n",
    "    N_tr, T, D = X_train.shape\n",
    "    N_te, T, D = X_test.shape\n",
    "    X_train_sh = np.zeros(X_train.shape)\n",
    "    X_test_sh = np.zeros(X_test.shape)\n",
    "    for i in range(N_tr):\n",
    "        \n",
    "        for k in range(D):\n",
    "            if k in dimensions:\n",
    "                mag = np.mean(X_train[i, :, k], axis = 0)\n",
    "                indice = np.where(train_ind ==  I_train[i])[0][0]\n",
    "                std = ind_std_train[indice][k] * 6\n",
    "                X_train_sh[i, :, k] = (X_train[i, :, k] - mag) / std\n",
    "            else:\n",
    "                X_train_sh[i, :, k] = X_train[i, :, k]\n",
    "\n",
    "    for i in range(N_te):\n",
    "        for k in range(D):\n",
    "            if k in dimensions:\n",
    "                mag = np.mean(X_test[i, :, k], axis = 0)\n",
    "                indice = np.where(test_ind ==  I_test[i])[0][0]\n",
    "                std = ind_std_test[indice][k] * 6\n",
    "                X_test_sh[i, :, k] = (X_test[i, :, k] - mag) / std\n",
    "            else:\n",
    "                X_test_sh[i, :, k] = X_test[i, :, k]\n",
    "    return X_train_sh, X_test_sh\n",
    "\n",
    "def augmentData(X, y, augmentation, repeat = 3):\n",
    "    X_out = []\n",
    "    y_out = []\n",
    "    for i in range(repeat):\n",
    "        if augmentation == 'rotation':\n",
    "            augmented = rotation(X, angle_range=[-np.pi/4, np.pi/4])\n",
    "        elif augmentation == 'permutation':\n",
    "            augmented = permutation(X)\n",
    "        elif augmentation == 'time_warp':\n",
    "            augmented = time_warp(X, sigma=0.1)\n",
    "        elif augmentation == 'magnitude_warp':\n",
    "            augmented = magnitude_warp(X, sigma=0.2, knot=4)\n",
    "        elif augmentation == 'scaling':\n",
    "            augmented = scaling(X, sigma=0.1)\n",
    "        elif augmentation == 'jitter':\n",
    "            augmented = jitter(X, sigma=0.02)\n",
    "        else:\n",
    "            augmented = X.copy()\n",
    "        if len(X_out) == 0:\n",
    "            X_out = augmented\n",
    "            y_out = y.copy()\n",
    "        else:\n",
    "            X_out = np.concatenate((X_out, augmented), axis=0)\n",
    "            y_out = np.concatenate((y_out, y), axis=0)\n",
    "    return X_out, y_out\n",
    "\n",
    "def augment(X, y, augmentations, repeats_per_augmentation=1, include_original=False):\n",
    "    X_aug = []\n",
    "    y_aug = []\n",
    "    if include_original:\n",
    "        X_aug = X.copy()\n",
    "        y_aug = y.copy()\n",
    "    for augmentation in augmentations:\n",
    "        curr_X_aug, curr_y_aug = augmentData(X, y, augmentation, repeat=repeats_per_augmentation)\n",
    "        if len(X_aug) == 0:\n",
    "            X_aug = curr_X_aug\n",
    "            y_aug = curr_y_aug\n",
    "        else:\n",
    "            X_aug = np.concatenate((X_aug, curr_X_aug), axis=0)\n",
    "            y_aug = np.concatenate((y_aug, curr_y_aug), axis=0)\n",
    "    return X_aug, y_aug\n",
    "        \n",
    "def minoritySampling(X, y):\n",
    "    rus = RandomUnderSampler(sampling_strategy='not minority', random_state=1)\n",
    "    N, T, D = X.shape\n",
    "    X_temp = X.reshape([N, T * D])\n",
    "    X_temp, y = rus.fit_resample(X_temp, y)\n",
    "    X = X_temp.reshape([X_temp.shape[0], T, D])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train IDS: [2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Test IDS: [0, 1]\n",
      "Val IDS: []\n",
      "Loading dataset from cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/texs/Documentos/Repositories/mts_feature_learning/source/augmentation.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  warp = np.concatenate(np.random.permutation(splits)).ravel()\n",
      "<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mts shape: N: 1500 -  T: 200 - D: 3\n",
      "mts shape: N: 1500 -  T: 200 - D: 3\n",
      "mts shape: N: 1500 -  T: 200 - D: 3\n",
      "mts shape: N: 1500 -  T: 200 - D: 3\n",
      "mts shape: N: 1500 -  T: 200 - D: 3\n",
      "mts shape: N: 1500 -  T: 200 - D: 3\n",
      "mts shape: N: 1500 -  T: 200 - D: 3\n",
      "Subsequence length: 180\n",
      "Epoch[1] Train loss    avg: 3.6633144005669487\n",
      "mts shape: N: 1500 -  T: 200 - D: 3\n",
      "mts shape: N: 1500 -  T: 200 - D: 3\n",
      "mts shape: N: 1500 -  T: 200 - D: 3\n",
      "mts shape: N: 1500 -  T: 200 - D: 3\n",
      "mts shape: N: 1500 -  T: 200 - D: 3\n",
      "mts shape: N: 1500 -  T: 200 - D: 3\n",
      "mts shape: N: 1500 -  T: 200 - D: 3\n",
      "Subsequence length: 180\n",
      "Epoch[1] Train loss    avg: 3.758205520417955\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[10:44:34] ../src/tree/updater_gpu_hist.cu:712: Exception in gpu_hist: [10:44:34] ../src/c_api/../data/../common/device_helpers.cuh:428: Memory allocation error on worker 0: std::bad_alloc: cudaErrorMemoryAllocation: out of memory\n- Free memory: 125370368\n- Requested memory: 201359360\n\nStack trace:\n  [bt] (0) /home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x38f939) [0x7f0dd2f8f939]\n  [bt] (1) /home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x393d4b) [0x7f0dd2f93d4b]\n  [bt] (2) /home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x402cba) [0x7f0dd3002cba]\n  [bt] (3) /home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x403278) [0x7f0dd3003278]\n  [bt] (4) /home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x3f536c) [0x7f0dd2ff536c]\n  [bt] (5) /home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x3a234f) [0x7f0dd2fa234f]\n  [bt] (6) /home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x3a5395) [0x7f0dd2fa5395]\n  [bt] (7) /home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x416809) [0x7f0dd3016809]\n  [bt] (8) /home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x416ece) [0x7f0dd3016ece]\n\n\n\nStack trace:\n  [bt] (0) /home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x637999) [0x7f0dd3237999]\n  [bt] (1) /home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x658cc5) [0x7f0dd3258cc5]\n  [bt] (2) /home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x1d2e43) [0x7f0dd2dd2e43]\n  [bt] (3) /home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x1d3c80) [0x7f0dd2dd3c80]\n  [bt] (4) /home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x20fd12) [0x7f0dd2e0fd12]\n  [bt] (5) /home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7f0dd2ca9688]\n  [bt] (6) /home/texs/anaconda3/envs/contrastive/lib/python3.10/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0e229049dd]\n  [bt] (7) /home/texs/anaconda3/envs/contrastive/lib/python3.10/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0e22904067]\n  [bt] (8) /home/texs/anaconda3/envs/contrastive/lib/python3.10/lib-dynload/_ctypes.cpython-310-x86_64-linux-gnu.so(+0x13a25) [0x7f0e2291da25]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/texs/Documentos/Repositories/mts_feature_learning/contrastive_kfold_aug.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/texs/Documentos/Repositories/mts_feature_learning/contrastive_kfold_aug.ipynb#ch0000002?line=232'>233</a>\u001b[0m clf \u001b[39m=\u001b[39m XGBClassifier(tree_method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgpu_hist\u001b[39m\u001b[39m'\u001b[39m, predictor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgpu_predictor\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/texs/Documentos/Repositories/mts_feature_learning/contrastive_kfold_aug.ipynb#ch0000002?line=233'>234</a>\u001b[0m train_feat, test_feat_map \u001b[39m=\u001b[39m embeddings_comb[j]\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/texs/Documentos/Repositories/mts_feature_learning/contrastive_kfold_aug.ipynb#ch0000002?line=234'>235</a>\u001b[0m clf\u001b[39m.\u001b[39;49mfit(train_feat, mts_train\u001b[39m.\u001b[39;49my)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/texs/Documentos/Repositories/mts_feature_learning/contrastive_kfold_aug.ipynb#ch0000002?line=236'>237</a>\u001b[0m pred_train \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mpredict(train_feat)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/texs/Documentos/Repositories/mts_feature_learning/contrastive_kfold_aug.ipynb#ch0000002?line=237'>238</a>\u001b[0m f1_tr \u001b[39m=\u001b[39m metrics\u001b[39m.\u001b[39mf1_score(mts_train\u001b[39m.\u001b[39my, pred_train, average\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mweighted\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/core.py:532\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    531\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 532\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/sklearn.py:1400\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1379\u001b[0m model, metric, params, early_stopping_rounds, callbacks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_configure_fit(\n\u001b[1;32m   1380\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1381\u001b[0m )\n\u001b[1;32m   1382\u001b[0m train_dmatrix, evals \u001b[39m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[1;32m   1383\u001b[0m     missing\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing,\n\u001b[1;32m   1384\u001b[0m     X\u001b[39m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1397\u001b[0m     enable_categorical\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menable_categorical,\n\u001b[1;32m   1398\u001b[0m )\n\u001b[0;32m-> 1400\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m   1401\u001b[0m     params,\n\u001b[1;32m   1402\u001b[0m     train_dmatrix,\n\u001b[1;32m   1403\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_num_boosting_rounds(),\n\u001b[1;32m   1404\u001b[0m     evals\u001b[39m=\u001b[39;49mevals,\n\u001b[1;32m   1405\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39;49mearly_stopping_rounds,\n\u001b[1;32m   1406\u001b[0m     evals_result\u001b[39m=\u001b[39;49mevals_result,\n\u001b[1;32m   1407\u001b[0m     obj\u001b[39m=\u001b[39;49mobj,\n\u001b[1;32m   1408\u001b[0m     custom_metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[1;32m   1409\u001b[0m     verbose_eval\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   1410\u001b[0m     xgb_model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1411\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m   1412\u001b[0m )\n\u001b[1;32m   1414\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective):\n\u001b[1;32m   1415\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobjective \u001b[39m=\u001b[39m params[\u001b[39m\"\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/core.py:532\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    531\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 532\u001b[0m \u001b[39mreturn\u001b[39;00m f(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    180\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m--> 181\u001b[0m bst\u001b[39m.\u001b[39;49mupdate(dtrain, i, obj)\n\u001b[1;32m    182\u001b[0m \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[1;32m    183\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/core.py:1733\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1730\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_features(dtrain)\n\u001b[1;32m   1732\u001b[0m \u001b[39mif\u001b[39;00m fobj \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1733\u001b[0m     _check_call(_LIB\u001b[39m.\u001b[39;49mXGBoosterUpdateOneIter(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m   1734\u001b[0m                                             ctypes\u001b[39m.\u001b[39;49mc_int(iteration),\n\u001b[1;32m   1735\u001b[0m                                             dtrain\u001b[39m.\u001b[39;49mhandle))\n\u001b[1;32m   1736\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1737\u001b[0m     pred \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(dtrain, output_margin\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/core.py:203\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \n\u001b[1;32m    194\u001b[0m \u001b[39mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39m    return value from API calls\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 203\u001b[0m     \u001b[39mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[39m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [10:44:34] ../src/tree/updater_gpu_hist.cu:712: Exception in gpu_hist: [10:44:34] ../src/c_api/../data/../common/device_helpers.cuh:428: Memory allocation error on worker 0: std::bad_alloc: cudaErrorMemoryAllocation: out of memory\n- Free memory: 125370368\n- Requested memory: 201359360\n\nStack trace:\n  [bt] (0) /home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x38f939) [0x7f0dd2f8f939]\n  [bt] (1) /home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x393d4b) [0x7f0dd2f93d4b]\n  [bt] (2) /home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x402cba) [0x7f0dd3002cba]\n  [bt] (3) /home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x403278) [0x7f0dd3003278]\n  [bt] (4) /home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x3f536c) [0x7f0dd2ff536c]\n  [bt] (5) /home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x3a234f) [0x7f0dd2fa234f]\n  [bt] (6) /home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x3a5395) [0x7f0dd2fa5395]\n  [bt] (7) /home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x416809) [0x7f0dd3016809]\n  [bt] (8) /home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x416ece) [0x7f0dd3016ece]\n\n\n\nStack trace:\n  [bt] (0) /home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x637999) [0x7f0dd3237999]\n  [bt] (1) /home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x658cc5) [0x7f0dd3258cc5]\n  [bt] (2) /home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x1d2e43) [0x7f0dd2dd2e43]\n  [bt] (3) /home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x1d3c80) [0x7f0dd2dd3c80]\n  [bt] (4) /home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(+0x20fd12) [0x7f0dd2e0fd12]\n  [bt] (5) /home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7f0dd2ca9688]\n  [bt] (6) /home/texs/anaconda3/envs/contrastive/lib/python3.10/lib-dynload/../../libffi.so.7(+0x69dd) [0x7f0e229049dd]\n  [bt] (7) /home/texs/anaconda3/envs/contrastive/lib/python3.10/lib-dynload/../../libffi.so.7(+0x6067) [0x7f0e22904067]\n  [bt] (8) /home/texs/anaconda3/envs/contrastive/lib/python3.10/lib-dynload/_ctypes.cpython-310-x86_64-linux-gnu.so(+0x13a25) [0x7f0e2291da25]\n\n"
     ]
    }
   ],
   "source": [
    "from source.torch_utils import getContrastiveFeatures\n",
    "import torch\n",
    "\n",
    "storage = MTSStorage('har_augmentations')\n",
    "storage.delete()\n",
    "storage.load()\n",
    "\n",
    "\n",
    "components_map = {}\n",
    "\n",
    "for k in range(KFOLDS):\n",
    "    # ------------------------ Reading the dataset ------------------------\n",
    "    if DATASET == 'HAR-UML20':\n",
    "        all_ids = har_ind_IDS\n",
    "        test_ids = all_ids[k: k + N_TESTS]\n",
    "        train_ids = all_ids[:k] + all_ids[k + N_TESTS:]        \n",
    "        \n",
    "        data = read_har_dataset('./datasets/HAR-UML20/', train_ids=train_ids, test_ids=test_ids, val_ids=[], cache=True)\n",
    "        ids_train, X_train, y_train, I_train, train_kcal_MET = data['train']\n",
    "        # ids_val, X_val, y_val, I_val, val_kcal_MET = data['val']\n",
    "        ids_test, X_test, y_test, I_test, test_kcal_MET = data['test']\n",
    "        \n",
    "        har_activities_map = {\n",
    "            0: \"Sedentary\",\n",
    "            1: \"Walking\",\n",
    "            2: \"Running\",\n",
    "            3: \"Downstairs\",\n",
    "            4: \"Upstairs\"\n",
    "        }\n",
    "\n",
    "        \n",
    "        all_dimensions = har_dimensions\n",
    "        activities_map = har_activities_map\n",
    "        \n",
    "        y_train[y_train==0] = 0\n",
    "        y_train[y_train==1] = 0\n",
    "        y_train[y_train==2] = 0\n",
    "        y_test[y_test==0] = 0\n",
    "        y_test[y_test==1] = 0\n",
    "        y_test[y_test==2] = 0\n",
    "\n",
    "        for i in range(3, len(har_activities)):\n",
    "            y_train[y_train==i] = i - 2\n",
    "            y_test[y_test==i] = i - 2\n",
    "        \n",
    "        ind_std_train = idsStd(train_ids , X_train, I_train)\n",
    "        ind_std_test = idsStd(test_ids, X_test, I_test)\n",
    "        \n",
    "        # unique, counts = np.unique(y_train, return_counts=True)\n",
    "        # unique, counts = np.unique(y_test, return_counts=True)\n",
    "        X_train, y_train = minoritySampling(X_train, y_train)\n",
    "        X_test, y_test = minoritySampling(X_test, y_test)\n",
    "        X_test_o = X_test.copy()\n",
    "        y_test_o = y_test.copy()\n",
    "        \n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    # --------------------------------AugmentData ---------------------------------------------------\n",
    "    if Z_SCORE_NORM:\n",
    "        # X_train, X_test = center(X_train, X_test, train_ids, test_ids, dimensions = [0, 1, 2])\n",
    "        X_train, X_test = znorm(X_train, X_test, train_ids, test_ids, ind_std_train, ind_std_test, dimensions = [0, 1, 2, 3, 4, 5])\n",
    "        X_test_o = X_test.copy()\n",
    "    \n",
    "    X_train, y_train = augment(X_train, y_train, repeats_per_augmentation = REPEATS_PER_AUGMENTATION, augmentations = AUGMENTATIONS, include_original = INCLUDE_ORIGINAL)\n",
    "    \n",
    "    additional = 1 if INCLUDE_ORIGINAL else 0\n",
    "    # X_train = np.repeat(X_train, REPEATS_PER_AUGMENTATION * len(AUGMENTATIONS) + additional, axis=0)\n",
    "    # y_train = np.repeat(y_train, REPEATS_PER_AUGMENTATION * len(AUGMENTATIONS) + additional, axis=0)\n",
    "    I_train = np.repeat(I_train, REPEATS_PER_AUGMENTATION * len(AUGMENTATIONS) + additional, axis=0)\n",
    "    # X_train = np.concatenate([X_train, X_train], axis=0)\n",
    "    # y_train = np.concatenate([y_train, y_train], axis=0)\n",
    "    # I_train = np.concatenate([I_train, I_train], axis=0)\n",
    "    \n",
    "    \n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    train_embeddings = []\n",
    "    test_embeddings = []\n",
    "    for t in range(len(N_DIMENSIONS)):    \n",
    "        dimensions = N_DIMENSIONS[t]\n",
    "        X_train_f = filter_dimensions(X_train, all_dimensions, dimensions)\n",
    "        X_test_f = filter_dimensions(X_test, all_dimensions, dimensions)\n",
    "        X_test_f_o = filter_dimensions(X_test_o, all_dimensions, dimensions)\n",
    "        \n",
    "        mts_train = TSerie(X = X_train_f, y = y_train, I = I_train, dimensions = dimensions, classLabels=activities_map)\n",
    "        mts_test = TSerie(X = X_test_f, y = y_test, I = I_test, dimensions = dimensions, classLabels=activities_map)\n",
    "        \n",
    "        mts_train.folding_features_v2()\n",
    "        mts_test.folding_features_v2()\n",
    "        \n",
    "        n_neighbors = 15\n",
    "        \n",
    "        # reducer = umap.UMAP(n_components=N_COMPONENTS, metric=METRIC, n_neighbors=n_neighbors)\n",
    "        # embeddings_train = reducer.fit_transform(mts_train.features, y=mts_train.y)\n",
    "        # embeddings_test = reducer.transform(mts_test.features)\n",
    "        \n",
    "        test_tranformations = [\n",
    "            augment(X_test_f_o.copy(), y_test, repeats_per_augmentation = 1, augmentations = [ALL_AUGMENTATIONS[i]], include_original = False)\n",
    "            for i in range(len(ALL_AUGMENTATIONS))\n",
    "        ]\n",
    "        \n",
    "        for i in range(len(test_tranformations)):\n",
    "            # X_trans, y_trans =  augment(X_test_f_o.copy(), y_test, repeats_per_augmentation = 1, augmentations = [ALL_AUGMENTATIONS[i]], include_original = False)\n",
    "            test_tranformations[i] = test_tranformations[i][0].copy()\n",
    "            test_tranformations[i] = test_tranformations[i].transpose([0, 2, 1])\n",
    "            if firstTimeSave:\n",
    "                # print(np.random.random([test_tranformations[i].shape[0], 2]).shape)\n",
    "                storage.add_mts(\n",
    "                    'test {}_{}'.format(ALL_AUGMENTATIONS[i], N_DIMS_NAMES[t]),\n",
    "                    test_tranformations[i].transpose([0, 2, 1]), \n",
    "                    dimensions,\n",
    "                    coords={\n",
    "                        'umap': np.random.random([test_tranformations[i].shape[0], 2])\n",
    "                    }, \n",
    "                    labels={\n",
    "                        'activities': mts_test.y, \n",
    "                        # 'participants': mts_test.I\n",
    "                    }, \n",
    "                    labelsNames={'activities': activities_map },\n",
    "                    sampling = True,\n",
    "                    n_samples = 400\n",
    "                )\n",
    "                storage.save()\n",
    "        \n",
    "                \n",
    "        \n",
    "        embeddings_train, test_embes  = getContrastiveFeatures(mts_train.X.transpose([0, 2, 1]), mts_train.y, epochs =4, loss_metric='SupConLoss', X_test=test_tranformations)\n",
    "        \n",
    "        test_map={}\n",
    "        for i in range(len(ALL_AUGMENTATIONS)):\n",
    "             test_map[ALL_AUGMENTATIONS[i]] = test_embes[i]\n",
    "        \n",
    "        # ['none', 'rotation', 'permutation', 'time_warp', 'magnitude_warp', 'scaling', 'jitter']\n",
    "        train_embeddings.append(embeddings_train)\n",
    "        test_embeddings.append(test_map)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        mts_train.features = embeddings_train\n",
    "        # mts_test.features = embeddings_test\n",
    "        \n",
    "        # reducer = UMAP(n_components=N_COMPONENTS, n_neighbors=n_neighbors)\n",
    "        # nn = NearestNeighbors(n_neighbors=n_neighbors, metric=METRIC)\n",
    "        # nn.fit(mts_train.features)\n",
    "        # knn_graph = nn.kneighbors_graph(mts_train.features, mode=\"distance\")\n",
    "        # embeddings_train = reducer.fit_transform(mts_train.features, y=mts_train.y, knn_graph=knn_graph.tocsr(), convert_dtype=True)\n",
    "        \n",
    "        # knn_graph2 = nn.kneighbors_graph(mts_test.features, mode=\"distance\")\n",
    "        # embeddings_test = reducer.transform(mts_test.features, knn_graph=knn_graph2.tocsr())\n",
    "        \n",
    "        # train_embeddings.append(embeddings_train)\n",
    "        # test_embeddings.append(embeddings_test)\n",
    "        # reducer = umap.UMAP(n_components=2, metric=METRIC, n_neighbors=n_neighbors)\n",
    "        # coords_train = reducer.fit_transform(mts_train.features, y=mts_train.y)\n",
    "        # coords_test = reducer.transform(mts_test.features)\n",
    "        \n",
    "        \n",
    "\n",
    "        # reducer = UMAP(n_components=2, n_neighbors=n_neighbors)\n",
    "        # nn = NearestNeighbors(n_neighbors=n_neighbors, metric=METRIC)\n",
    "        # nn.fit(mts_train.features)\n",
    "        # knn_graph = nn.kneighbors_graph(mts_train.features, mode=\"distance\")\n",
    "        # coords_train = reducer.fit_transform(mts_train.features, y=mts_train.y, knn_graph=knn_graph.tocsr(), convert_dtype=False)\n",
    "        \n",
    "        # knn_graph2 = nn.kneighbors_graph(mts_test.features, mode=\"distance\")\n",
    "        # coords_test = reducer.transform(mts_test.features,  knn_graph=knn_graph2.tocsr())\n",
    "        \n",
    "\n",
    "        # indMapTrain = {int(ind): 'sub_' + str(ind) for ind in np.unique(mts_train.I)}\n",
    "        # indMapTest = {int(ind): 'sub_' + str(ind) for ind in np.unique(mts_test.I)}\n",
    "        \n",
    "        # storage.add_mts(\n",
    "        #     'train_{}_{}'.format(' '.join(AUGMENTATIONS), ' '.join(dimensions)), \n",
    "        #     mts_train.X,\n",
    "        #     dimensions,\n",
    "        #     coords={'umap': coords_train}, \n",
    "        #     labels={\n",
    "        #         'activities': mts_train.y, \n",
    "        #         # 'participants': mts_train.I\n",
    "        #     }, \n",
    "        #     labelsNames={'activities': activities_map, 'participants': indMapTrain},\n",
    "        #     sampling = True,\n",
    "        #     n_samples = 400\n",
    "        # )\n",
    "                \n",
    "        # storage.add_mts(\n",
    "        #     'test_{}_{}'.format(' '.join(AUGMENTATIONS), ' '.join(dimensions)), \n",
    "        #     mts_test.X, \n",
    "        #     dimensions,\n",
    "        #     coords={'umap': coords_test}, \n",
    "        #     labels={\n",
    "        #         'activities': mts_test.y, \n",
    "        #         # 'participants': mts_test.I\n",
    "        #     }, \n",
    "        #     labelsNames={'activities': activities_map, 'participants': indMapTest},\n",
    "        #     sampling = True,\n",
    "        #     n_samples = 400\n",
    "        # )\n",
    "        \n",
    "        storage.save()\n",
    "    firstTimeSave = False\n",
    "    \n",
    "    names_comb = []\n",
    "    embeddings_comb = []\n",
    "    for i, combo in enumerate(powerset(list(range(len(N_DIMS_NAMES)))), 1):\n",
    "        indexes = list(combo)\n",
    "        name = ''\n",
    "        train_embedding = []\n",
    "        test_embedding = {}\n",
    "        if len(indexes) == 0:\n",
    "            continue\n",
    "        for ind in indexes:\n",
    "            name = name + ' ' + N_DIMS_NAMES[ind]\n",
    "            if len(train_embedding) == 0:\n",
    "                train_embedding = train_embeddings[ind]\n",
    "                for aug in ALL_AUGMENTATIONS:\n",
    "                    test_embedding[aug] = test_embeddings[ind][aug]\n",
    "            else:\n",
    "                train_embedding = np.concatenate([train_embedding, train_embeddings[ind]], axis=1)\n",
    "                for aug in ALL_AUGMENTATIONS:\n",
    "                    test_embedding[aug] = np.concatenate([test_embedding[aug], test_embeddings[ind][aug]], axis=1)        \n",
    "        names_comb.append(name)\n",
    "        embeddings_comb.append((train_embedding, test_embedding))\n",
    "    \n",
    "    for j in range(len(names_comb)):\n",
    "        print(j)\n",
    "        name = names_comb[j]\n",
    "        # clf = AdaBoostClassifier()\n",
    "        # clf = XGBClassifier()\n",
    "        # clf = svm.SVC()\n",
    "        clf = XGBClassifier(tree_method='gpu_hist', predictor='gpu_predictor')\n",
    "        train_feat, test_feat_map = embeddings_comb[j]\n",
    "        clf.fit(train_feat, mts_train.y)\n",
    "        \n",
    "        pred_train = clf.predict(train_feat)\n",
    "        f1_tr = metrics.f1_score(mts_train.y, pred_train, average='weighted')\n",
    "        \n",
    "        f1_scores = [f1_tr]\n",
    "        \n",
    "        for aug in ALL_AUGMENTATIONS:\n",
    "            test_feat = test_feat_map[aug]\n",
    "            pred_test = clf.predict(test_feat)\n",
    "            f1_te = metrics.f1_score(y_test_o, pred_test, average='weighted')\n",
    "            f1_scores.append(f1_te)\n",
    "        \n",
    "        if name not in components_map:\n",
    "            components_map[name] = [f1_scores]\n",
    "        else:\n",
    "            components_map[name] = components_map[name] + [f1_scores]\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Acc', '1.000 (0.000)', '0.828 (0.000)', '0.801 (0.000)', '0.710 (0.000)', '0.486 (0.000)', '0.807 (0.000)', '0.816 (0.000)', '0.742 (0.000)']\n",
      "[' Gyro', '1.000 (0.000)', '0.748 (0.000)', '0.738 (0.000)', '0.673 (0.000)', '0.484 (0.000)', '0.746 (0.000)', '0.755 (0.000)', '0.603 (0.000)']\n",
      "[' Acc Gyro', '1.000 (0.000)', '0.844 (0.000)', '0.821 (0.000)', '0.754 (0.000)', '0.518 (0.000)', '0.833 (0.000)', '0.836 (0.000)', '0.794 (0.000)']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "path = os.path.join(RESULTS_PATH, 'har_kfold_{}.csv'.format('_'.join(AUGMENTATIONS)))\n",
    "with open(path, 'w', newline='') as csvfile:\n",
    "    row = ['Sensors', 'f1 train', 'f1 test']\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quoting=csv.QUOTE_MINIMAL)\n",
    "    spamwriter.writerow(row)\n",
    "    for name in names_comb:\n",
    "        row = [name]\n",
    "        f1_mean_tr = np.array([ f1[0] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_tr = np.array([ f1[0] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te = np.array([ f1[1] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te = np.array([ f1[1] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_rot = np.array([ f1[2] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_rot = np.array([ f1[2] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_per = np.array([ f1[3] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_per = np.array([ f1[3] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_tim = np.array([ f1[4] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_tim = np.array([ f1[4] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_mag = np.array([ f1[5] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_mag = np.array([ f1[5] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_sca = np.array([ f1[6] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_sca = np.array([ f1[6] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_jit = np.array([ f1[7] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_jit = np.array([ f1[7] for f1 in components_map[name]]).std()\n",
    "        \n",
    "        row = [\n",
    "            name, \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_tr, f1_stds_tr), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te, f1_stds_te), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te_rot, f1_stds_te_rot), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te_per, f1_stds_te_per), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te_tim, f1_stds_te_tim), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te_mag, f1_stds_te_mag), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te_sca, f1_stds_te_sca), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te_jit, f1_stds_te_jit), \n",
    "        ]\n",
    "        print(row)\n",
    "        spamwriter.writerow(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Centered  0.966 (0.017)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Centered  0.966 (0.017)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Rotation 0.999 (0.001)', '0.955 (0.027)')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Rotation 0.999 (0.001)', '0.955 (0.027)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Original Acc', '0.998 (0.001)', '0.925 (0.045)')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Original Acc', '0.998 (0.001)', '0.925 (0.045)'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('contrastive')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af38bbb6e377e6d688e9d49e963edf808fac73a4ecd279c84061bb0d9783d83c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
