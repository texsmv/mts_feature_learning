{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/texs/anaconda3/envs/contrastive/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from source.read_HAR_dataset import read_har_dataset, har_dimensions, har_activities, har_activities_map, har_ind_IDS\n",
    "from source.utils import  filter_dimensions\n",
    "from source.tserie import TSerie\n",
    "from source.utils import classify_dataset\n",
    "from itertools import chain, combinations\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "from source.utils import idsStd\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import umap\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from source.augmentation import  * \n",
    "# from cuml.datasets import make_blobs\n",
    "# from cuml.neighbors import NearestNeighbors\n",
    "# from cuml.manifold import UMAP\n",
    "# from cuml.cluster import DBSCAN\n",
    "\n",
    "\n",
    "sys.path.insert(0, '/home/texs/Documentos/Repositories/mts_viz')\n",
    "from server.source.storage import MTSStorage\n",
    "\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)  # allows duplicate elements\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "N_COMPONENTS=32\n",
    "Z_SCORE_NORM = False\n",
    "DATASET = 'HAR-UML20'\n",
    "KFOLDS = 1\n",
    "N_TESTS = 2\n",
    "METRIC  = 'braycurtis'\n",
    "RESULTS_PATH = 'outputs/augmentation/'\n",
    "# AUGMENTATIONS = ['rotation', 'permutation', 'time_warp', 'magnitude_warp', 'scaling', 'jitter']\n",
    "# AUGMENTATIONS = ['none']\n",
    "AUGMENTATIONS = ['scaling']\n",
    "ALL_AUGMENTATIONS = ['none', 'rotation', 'permutation', 'time_warp', 'magnitude_warp', 'scaling', 'jitter']\n",
    "# ALL_AUGMENTATIONS = ['none', 'rotation', 'rotation', 'rotation', 'rotation', 'rotation', 'rotation']\n",
    "# ALL_AUGMENTATIONS = ['none', 'rotation'] \n",
    "# AUGMENTATIONS = ['magnitude_warp']\n",
    "REPEATS_PER_AUGMENTATION = 1\n",
    "INCLUDE_ORIGINAL = True\n",
    "# N_DIMS_NAMES = ['Acc', 'Gyro', 'Mag']\n",
    "N_DIMS_NAMES = ['Acc', 'Gyro']\n",
    "# N_DIMS_NAMES = ['Acc']\n",
    "N_DIMENSIONS = [\n",
    "    [\n",
    "        'Accelerometer-X',\t\n",
    "        'Accelerometer-Y',\t\n",
    "        'Accelerometer-Z',\n",
    "    ],\n",
    "    [\n",
    "        'Gyrometer-X',\n",
    "        'Gyrometer-Y',\n",
    "        'Gyrometer-Z',\n",
    "    ],\n",
    "    # [\n",
    "    #     'Magnetometer-X',\n",
    "    #     'Magnetometer-Y',\n",
    "    #     'Magnetometer-Z'\n",
    "    # ]\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor:\n",
    "    def __init__(self, epochs = 100, batch_size = 32, loss_metric = 'SimCLR', encoding_size = 8, mode = 'subsequences'):\n",
    "        self.model = None\n",
    "        self.device = None\n",
    "        self.epochs = epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_metric = loss_metric\n",
    "        self.encoding_size = encoding_size\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        self.nearNeigh.fit(X)\n",
    "        knn_graph = self.nearNeigh.kneighbors_graph(X, mode=\"distance\")\n",
    "        embeddings =  self.reducer.fit_transform(X, y=y, knn_graph=knn_graph.tocsr(), convert_dtype=True)\n",
    "        return embeddings\n",
    "    \n",
    "    def transform(self, X):\n",
    "        knn_graph = self.nearNeigh.kneighbors_graph(X, mode=\"distance\")\n",
    "        embeddings =  self.reducer.transform(X, knn_graph=knn_graph.tocsr(), convert_dtype=True)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "def center(X_train, X_test, train_ind, test_ind, dimensions ):\n",
    "    N_tr, T, D = X_train.shape\n",
    "    N_te, T, D = X_test.shape\n",
    "    X_train_sh = np.zeros(X_train.shape)\n",
    "    X_test_sh = np.zeros(X_test.shape)\n",
    "    for i in range(N_tr):\n",
    "        \n",
    "        for k in range(D):\n",
    "            if k in dimensions:\n",
    "                mag = np.mean(X_train[i, :, k], axis = 0)\n",
    "                indice = np.where(train_ind ==  I_train[i])[0][0]\n",
    "                std = ind_std_train[indice][k] * 6\n",
    "                X_train_sh[i, :, k] = (X_train[i, :, k] - mag)\n",
    "                # X_train_sh[i, :, k] = np.concatenate([[0], fft(X_train[i, :, k])[1:]])\n",
    "            else:\n",
    "                X_train_sh[i, :, k] = X_train[i, :, k]\n",
    "\n",
    "    for i in range(N_te):\n",
    "        for k in range(D):\n",
    "            if k in dimensions:\n",
    "                mag = np.mean(X_test[i, :, k], axis = 0)\n",
    "                indice = np.where(test_ind ==  I_test[i])[0][0]\n",
    "                std = ind_std_test[indice][k] * 6\n",
    "                X_test_sh[i, :, k] = (X_test[i, :, k] - mag)\n",
    "                # X_test_sh[i, :, k] = fft(X_test[i, :, k])[0:]\n",
    "            else:\n",
    "                X_test_sh[i, :, k] = X_test[i, :, k]\n",
    "    return X_train_sh, X_test_sh\n",
    "\n",
    "def znorm(X_train, X_test, train_ind, test_ind, ind_std_train, ind_std_test, dimensions):\n",
    "    N_tr, T, D = X_train.shape\n",
    "    N_te, T, D = X_test.shape\n",
    "    X_train_sh = np.zeros(X_train.shape)\n",
    "    X_test_sh = np.zeros(X_test.shape)\n",
    "    for i in range(N_tr):\n",
    "        \n",
    "        for k in range(D):\n",
    "            if k in dimensions:\n",
    "                mag = np.mean(X_train[i, :, k], axis = 0)\n",
    "                indice = np.where(train_ind ==  I_train[i])[0][0]\n",
    "                std = ind_std_train[indice][k] * 6\n",
    "                X_train_sh[i, :, k] = (X_train[i, :, k] - mag) / std\n",
    "            else:\n",
    "                X_train_sh[i, :, k] = X_train[i, :, k]\n",
    "\n",
    "    for i in range(N_te):\n",
    "        for k in range(D):\n",
    "            if k in dimensions:\n",
    "                mag = np.mean(X_test[i, :, k], axis = 0)\n",
    "                indice = np.where(test_ind ==  I_test[i])[0][0]\n",
    "                std = ind_std_test[indice][k] * 6\n",
    "                X_test_sh[i, :, k] = (X_test[i, :, k] - mag) / std\n",
    "            else:\n",
    "                X_test_sh[i, :, k] = X_test[i, :, k]\n",
    "    return X_train_sh, X_test_sh\n",
    "\n",
    "def augmentData(X, y, augmentation, repeat = 3):\n",
    "    X_out = []\n",
    "    y_out = []\n",
    "    for i in range(repeat):\n",
    "        if augmentation == 'rotation':\n",
    "            augmented = rotation(X, angle_range=[-np.pi/8, np.pi/8])\n",
    "            # augmented = rotation(X, angle_range=[-np.pi/64, np.pi/64])\n",
    "        elif augmentation == 'permutation':\n",
    "            augmented = permutation(X)\n",
    "        elif augmentation == 'time_warp':\n",
    "            augmented = time_warp(X, sigma=0.03)\n",
    "        elif augmentation == 'magnitude_warp':\n",
    "            augmented = magnitude_warp(X, sigma=0.04, knot=4)\n",
    "        elif augmentation == 'scaling':\n",
    "            augmented = scaling(X, sigma=0.05)\n",
    "        elif augmentation == 'jitter':\n",
    "            augmented = jitter(X, sigma=0.01)\n",
    "        else:\n",
    "            augmented = X\n",
    "        if len(X_out) == 0:\n",
    "            X_out = augmented\n",
    "            y_out = y\n",
    "        else:\n",
    "            X_out = np.concatenate((X_out, augmented), axis=0)\n",
    "            y_out = np.concatenate((y_out, y), axis=0)\n",
    "    return X_out, y_out\n",
    "\n",
    "def augment(X, y, augmentations, repeats_per_augmentation=1, include_original=False):\n",
    "    X_aug = []\n",
    "    y_aug = []\n",
    "    if include_original:\n",
    "        X_aug = X\n",
    "        y_aug = y\n",
    "    for augmentation in augmentations:\n",
    "        curr_X_aug, curr_y_aug = augmentData(X, y, augmentation, repeat=repeats_per_augmentation)\n",
    "        if len(X_aug) == 0:\n",
    "            X_aug = curr_X_aug\n",
    "            y_aug = curr_y_aug\n",
    "        else:\n",
    "            X_aug = np.concatenate((X_aug, curr_X_aug), axis=0)\n",
    "            y_aug = np.concatenate((y_aug, curr_y_aug), axis=0)\n",
    "    return X_aug, y_aug\n",
    "\n",
    "def minoritySampling(X, y):\n",
    "    rus = RandomUnderSampler(sampling_strategy='not minority', random_state=1)\n",
    "    N, T, D = X.shape\n",
    "    X_temp = X.reshape([N, T * D])\n",
    "    X_temp, y = rus.fit_resample(X_temp, y)\n",
    "    X = X_temp.reshape([X_temp.shape[0], T, D])\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train IDS: [2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Test IDS: [0, 1]\n",
      "Val IDS: []\n",
      "Loading dataset from cache...\n",
      "Subsequence length: 180\n",
      "Using contrastive metric!!!!!!!!!!!!\n",
      "Epoch[1] Train loss    avg: 4.077798395292134\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/texs/Documentos/Repositories/mts_feature_learning/umap_kfold_aug_v3.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/texs/Documentos/Repositories/mts_feature_learning/umap_kfold_aug_v3.ipynb#W2sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m n_neighbors \u001b[39m=\u001b[39m \u001b[39m15\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/texs/Documentos/Repositories/mts_feature_learning/umap_kfold_aug_v3.ipynb#W2sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m \u001b[39m# reducer = umap.UMAP(n_components=N_COMPONENTS, metric=METRIC, n_neighbors=n_neighbors)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/texs/Documentos/Repositories/mts_feature_learning/umap_kfold_aug_v3.ipynb#W2sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m \u001b[39m# embeddings_train = reducer.fit_transform(mts_train.features, y=mts_train.y)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/texs/Documentos/Repositories/mts_feature_learning/umap_kfold_aug_v3.ipynb#W2sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m \u001b[39m# embeddings_test = reducer.transform(mts_test.features)\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/home/texs/Documentos/Repositories/mts_feature_learning/umap_kfold_aug_v3.ipynb#W2sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m embeddings_train, model, device \u001b[39m=\u001b[39m  getContrastiveFeatures(mts_train\u001b[39m.\u001b[39;49mX\u001b[39m.\u001b[39;49mtranspose([\u001b[39m0\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m1\u001b[39;49m]), mts_train\u001b[39m.\u001b[39;49my, epochs \u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, loss_metric\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mSupConLoss\u001b[39;49m\u001b[39m'\u001b[39;49m,)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/texs/Documentos/Repositories/mts_feature_learning/umap_kfold_aug_v3.ipynb#W2sZmlsZQ%3D%3D?line=118'>119</a>\u001b[0m \u001b[39m# embeddings_train, embeddings_test = getContrastiveFeatures(mts_train.X.transpose([0, 2, 1]), mts_train.y, epochs =20, loss_metric='SupConLoss', X_test=mts_test.X.transpose([0, 2, 1]))\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/texs/Documentos/Repositories/mts_feature_learning/umap_kfold_aug_v3.ipynb#W2sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell:/home/texs/Documentos/Repositories/mts_feature_learning/umap_kfold_aug_v3.ipynb#W2sZmlsZQ%3D%3D?line=120'>121</a>\u001b[0m \u001b[39m# Create test augmentations \u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/texs/Documentos/Repositories/mts_feature_learning/umap_kfold_aug_v3.ipynb#W2sZmlsZQ%3D%3D?line=121'>122</a>\u001b[0m \u001b[39m# TODO sample\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/texs/Documentos/Repositories/mts_feature_learning/umap_kfold_aug_v3.ipynb#W2sZmlsZQ%3D%3D?line=122'>123</a>\u001b[0m test_tranformations \u001b[39m=\u001b[39m [\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/texs/Documentos/Repositories/mts_feature_learning/umap_kfold_aug_v3.ipynb#W2sZmlsZQ%3D%3D?line=123'>124</a>\u001b[0m     augment(mts_test\u001b[39m.\u001b[39mX, mts_test\u001b[39m.\u001b[39my, repeats_per_augmentation \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, augmentations \u001b[39m=\u001b[39m [ALL_AUGMENTATIONS[i]], include_original \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/texs/Documentos/Repositories/mts_feature_learning/umap_kfold_aug_v3.ipynb#W2sZmlsZQ%3D%3D?line=124'>125</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(ALL_AUGMENTATIONS))\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/texs/Documentos/Repositories/mts_feature_learning/umap_kfold_aug_v3.ipynb#W2sZmlsZQ%3D%3D?line=125'>126</a>\u001b[0m ]\n",
      "File \u001b[0;32m~/Documentos/Repositories/mts_feature_learning/source/torch_utils.py:78\u001b[0m, in \u001b[0;36mgetContrastiveFeatures\u001b[0;34m(X, y, epochs, batch_size, head, loss_metric, feat_size, encoding_size, mode, X_test, conv_filters, conv_kernels)\u001b[0m\n\u001b[1;32m     76\u001b[0m     loss \u001b[39m=\u001b[39m train_batch_triplet(model, data, optimizer, criterion, device, subsequence_length)\n\u001b[1;32m     77\u001b[0m \u001b[39melif\u001b[39;00m loss_metric \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mSupConLoss\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> 78\u001b[0m     loss \u001b[39m=\u001b[39m train_batch(model, data, optimizer, criterion, device, subsequence_length, supervised\u001b[39m=\u001b[39;49msupervised, mode\u001b[39m=\u001b[39;49mmode )\n\u001b[1;32m     79\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     80\u001b[0m     loss \u001b[39m=\u001b[39m train_batch(model, data, optimizer, criterion, device, subsequence_length, supervised\u001b[39m=\u001b[39msupervised, mode\u001b[39m=\u001b[39mmode)\n",
      "File \u001b[0;32m~/Documentos/Repositories/mts_feature_learning/source/models/contrastive/models.py:367\u001b[0m, in \u001b[0;36mtrain_batch\u001b[0;34m(model, data, optimizer, criterion, device, win_len, supervised, mode)\u001b[0m\n\u001b[1;32m    365\u001b[0m viewsCodes \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([codes1, codes2, codes3, codes4], \u001b[39m1\u001b[39m)\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m supervised:\n\u001b[0;32m--> 367\u001b[0m     loss \u001b[39m=\u001b[39m criterion(viewsCodes, lA)\n\u001b[1;32m    368\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m     loss \u001b[39m=\u001b[39m criterion(viewsCodes)\n",
      "File \u001b[0;32m~/anaconda3/envs/contrastive/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documentos/Repositories/mts_feature_learning/source/models/contrastive/losses.py:93\u001b[0m, in \u001b[0;36mSupConLoss.forward\u001b[0;34m(self, features, labels, mask)\u001b[0m\n\u001b[1;32m     90\u001b[0m     mask \u001b[39m=\u001b[39m mask\u001b[39m.\u001b[39mfloat()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     92\u001b[0m contrast_count \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m---> 93\u001b[0m contrast_feature \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(torch\u001b[39m.\u001b[39;49munbind(features, dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m), dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrast_mode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mone\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m     95\u001b[0m     anchor_feature \u001b[39m=\u001b[39m features[:, \u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from source.torch_utils import getContrastiveFeatures\n",
    "\n",
    "\n",
    "firstTimeSave = True\n",
    "storage = MTSStorage('har_augmentations')\n",
    "storage.delete()\n",
    "storage.load()\n",
    "\n",
    "\n",
    "components_map = {}\n",
    "\n",
    "for k in range(KFOLDS):\n",
    "    # ------------------------ Reading the dataset ------------------------\n",
    "    if DATASET == 'HAR-UML20':\n",
    "        all_ids = har_ind_IDS\n",
    "        test_ids = all_ids[k: k + N_TESTS]\n",
    "        train_ids = all_ids[:k] + all_ids[k + N_TESTS:]        \n",
    "        \n",
    "        data = read_har_dataset('./datasets/HAR-UML20/', train_ids=train_ids, test_ids=test_ids, val_ids=[], cache=True)\n",
    "        ids_train, X_train, y_train, I_train, train_kcal_MET = data['train']\n",
    "        # ids_val, X_val, y_val, I_val, val_kcal_MET = data['val']\n",
    "        ids_test, X_test, y_test, I_test, test_kcal_MET = data['test']\n",
    "        \n",
    "        har_activities_map = {\n",
    "            0: \"Sedentary\",\n",
    "            1: \"Walking\",\n",
    "            2: \"Running\",\n",
    "            3: \"Downstairs\",\n",
    "            4: \"Upstairs\"\n",
    "        }\n",
    "\n",
    "        all_dimensions = har_dimensions\n",
    "        activities_map = har_activities_map\n",
    "        \n",
    "        y_train[y_train==0] = 0\n",
    "        y_train[y_train==1] = 0\n",
    "        y_train[y_train==2] = 0\n",
    "        y_test[y_test==0] = 0\n",
    "        y_test[y_test==1] = 0\n",
    "        y_test[y_test==2] = 0\n",
    "\n",
    "        for i in range(3, len(har_activities)):\n",
    "            y_train[y_train==i] = i - 2\n",
    "            y_test[y_test==i] = i - 2\n",
    "        \n",
    "        ind_std_train = idsStd(train_ids , X_train, I_train)\n",
    "        ind_std_test = idsStd(test_ids, X_test, I_test)\n",
    "        \n",
    "        # unique, counts = np.unique(y_train, return_counts=True)\n",
    "        # unique, counts = np.unique(y_test, return_counts=True)\n",
    "        X_train, y_train = minoritySampling(X_train, y_train)\n",
    "        X_test, y_test = minoritySampling(X_test, y_test)\n",
    "        # X_train_o = X_train.copy()\n",
    "        # y_train_o = y_train.copy()\n",
    "        # X_test_o = X_test.copy()\n",
    "        # y_test_o = y_test.copy()\n",
    "        \n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    # --------------------------------AugmentData ---------------------------------------------------\n",
    "    if Z_SCORE_NORM:\n",
    "        # X_train, X_test = center(X_train, X_test, train_ids, test_ids, dimensions = [0, 1, 2])\n",
    "        X_train, X_test = znorm(X_train, X_test, train_ids, test_ids, ind_std_train, ind_std_test, dimensions = [0, 1, 2, 3, 4, 5])\n",
    "        # X_test_o = X_test.copy()\n",
    "    \n",
    "    # -----------------------------------------------------------------------------------------------\n",
    "\n",
    "    train_embeddings = []\n",
    "    train_ys = []\n",
    "    test_embeddings = []\n",
    "    dimension_model = []\n",
    "    \n",
    "    # idx_trans = np.random.choice(np.arange(len(y_train_o)), len(y_train_o) // (len(ALL_AUGMENTATIONS) - 1), replace=False)\n",
    "    # Y_\n",
    "    \n",
    "    augment_idx = []\n",
    "    all_y = []\n",
    "    \n",
    "    cont = 0\n",
    "    for aug in ALL_AUGMENTATIONS:\n",
    "        if aug == 'none':\n",
    "            idx_trans = np.random.choice(np.arange(len(y_train)), len(y_train), replace=False)\n",
    "        else:\n",
    "            n = len(y_train)\n",
    "            nsize = len(y_train) // (len(ALL_AUGMENTATIONS) - 1)\n",
    "            begin = cont * nsize\n",
    "            end = (cont + 1) * nsize\n",
    "            idx_trans = np.arange(len(y_train))[begin: end]\n",
    "            # idx_trans = np.random.choice(np.arange(len(y_train_o)), , replace=False)\n",
    "            cont = cont + 1\n",
    "        # idx_trans = np.random.choice(np.arange(len(y_train_o)), len(y_train_o) // (len(ALL_AUGMENTATIONS)), replace=False)\n",
    "        augment_idx.append(idx_trans)\n",
    "        all_y.append(y_train[idx_trans])\n",
    "        \n",
    "    for t in range(len(N_DIMENSIONS)):\n",
    "        dimensions = N_DIMENSIONS[t]    \n",
    "        X_train_f = filter_dimensions(X_train, all_dimensions, dimensions)\n",
    "        X_test_f = filter_dimensions(X_test, all_dimensions, dimensions)\n",
    "        # X_test_f_o = filter_dimensions(X_test_o, all_dimensions, dimensions)\n",
    "        # X_train_f_o = filter_dimensions(X_train_o, all_dimensions, dimensions)\n",
    "        \n",
    "        mts_train = TSerie(X = X_train_f, y = y_train, I = I_train, dimensions = dimensions, classLabels=activities_map)\n",
    "        mts_test = TSerie(X = X_test_f, y = y_test, I = I_test, dimensions = dimensions, classLabels=activities_map)\n",
    "        \n",
    "        minl, maxl = mts_train.minMaxNormalization()\n",
    "        mts_test.minMaxNormalization(minl=minl, maxl=maxl)\n",
    "        \n",
    "        # mts_train.folding_features_v2()\n",
    "        # mts_test.folding_features_v2()\n",
    "        \n",
    "        n_neighbors = 15\n",
    "        \n",
    "        # reducer = umap.UMAP(n_components=N_COMPONENTS, metric=METRIC, n_neighbors=n_neighbors)\n",
    "        # embeddings_train = reducer.fit_transform(mts_train.features, y=mts_train.y)\n",
    "        # embeddings_test = reducer.transform(mts_test.features)\n",
    "        embeddings_train, model, device =  getContrastiveFeatures(mts_train.X.transpose([0, 2, 1]), mts_train.y, epochs =20, loss_metric='SupConLoss',)\n",
    "        \n",
    "        # embeddings_train, embeddings_test = getContrastiveFeatures(mts_train.X.transpose([0, 2, 1]), mts_train.y, epochs =20, loss_metric='SupConLoss', X_test=mts_test.X.transpose([0, 2, 1]))\n",
    "        \n",
    "        # Create test augmentations \n",
    "        # TODO sample\n",
    "        test_tranformations = [\n",
    "            augment(mts_test.X, mts_test.y, repeats_per_augmentation = 1, augmentations = [ALL_AUGMENTATIONS[i]], include_original = False)\n",
    "            for i in range(len(ALL_AUGMENTATIONS))\n",
    "        ]\n",
    "        \n",
    "        train_tranformations = [\n",
    "            augment(mts_train.X, mts_train.y, repeats_per_augmentation = 1, augmentations = [ALL_AUGMENTATIONS[i]], include_original = False)\n",
    "            for i in range(len(ALL_AUGMENTATIONS))\n",
    "        ]\n",
    "        \n",
    "        test_map = {}\n",
    "        train_map = {}\n",
    "        \n",
    "        for i in range(len(ALL_AUGMENTATIONS)):\n",
    "            tranformation_name = ALL_AUGMENTATIONS[i]\n",
    "            # X_trans, y_trans =  augment(X_test_f_o.copy(), y_test, repeats_per_augmentation = 1, augmentations = [ALL_AUGMENTATIONS[i]], include_original = False)\n",
    "            test_tranformations[i] = test_tranformations[i][0].copy()\n",
    "            train_tranformations[i] = train_tranformations[i][0].copy()\n",
    "            \n",
    "            mts_test_t = TSerie(X = test_tranformations[i], dimensions = dimensions, classLabels=activities_map)\n",
    "            # mts_test_t.folding_features_v2()\n",
    "            \n",
    "            # n_samples = None\n",
    "            # if tranformation_name == 'none':\n",
    "            #     idx = np.random.choice(np.arange(len(y_train_o)), len(y_train_o), replace=False)\n",
    "            # else:\n",
    "            #     idx_trans = np.random.choice(np.arange(len(y_train_o)), len(y_train_o) // (len(ALL_AUGMENTATIONS) - 1), replace=False)\n",
    "            #     idx = idx_trans\n",
    "\n",
    "            idx = augment_idx[i]\n",
    "            \n",
    "            mts_train_t = TSerie(X = train_tranformations[i][idx], y = y_train[idx], dimensions = dimensions, classLabels=activities_map)\n",
    "            # mts_train_t.folding_features_v2()\n",
    "            \n",
    "            test_map[ALL_AUGMENTATIONS[i]] = model.encode(mts_test_t.X.transpose([0, 2, 1]), device)\n",
    "            train_map[ALL_AUGMENTATIONS[i]] = model.encode(mts_train_t.X.transpose([0, 2, 1]), device)\n",
    "            # train_map_y[ALL_AUGMENTATIONS[i]] = mts_train_t.y\n",
    "            \n",
    "            if firstTimeSave:\n",
    "                # print(np.random.random([test_tranformations[i].shape[0], 2]).shape)\n",
    "                storage.add_mts(\n",
    "                    'test {}_{}'.format(ALL_AUGMENTATIONS[i], N_DIMS_NAMES[t]),\n",
    "                    mts_test_t.X, \n",
    "                    dimensions,\n",
    "                    coords={\n",
    "                        'umap': np.random.random([test_tranformations[i].shape[0], 2])\n",
    "                    }, \n",
    "                    # labels={\n",
    "                    #     'activities': mts_test.y, \n",
    "                    #     # 'participants': mts_test.I\n",
    "                    # }, \n",
    "                    # labelsNames={'activities': activities_map },\n",
    "                    sampling = True,\n",
    "                    n_samples = 400\n",
    "                )\n",
    "                storage.save()\n",
    "            \n",
    "        \n",
    "        # dimension_model.append(reducer)\n",
    "        train_embeddings.append(train_map)\n",
    "        # train_ys.append(train_map_y)\n",
    "        test_embeddings.append(test_map)\n",
    "    firstTimeSave = False\n",
    "    \n",
    "    names_comb = []\n",
    "    embeddings_comb = []\n",
    "    for i, combo in enumerate(powerset(list(range(len(N_DIMS_NAMES)))), 1):\n",
    "        indexes = list(combo)\n",
    "        name = ''\n",
    "        train_embedding = {}\n",
    "        # train_y = {}\n",
    "        test_embedding = {}\n",
    "        if len(indexes) == 0:\n",
    "            continue\n",
    "        for ind in indexes:\n",
    "            name = name + ' ' + N_DIMS_NAMES[ind]\n",
    "            if len(test_embedding) == 0:\n",
    "                for aug in ALL_AUGMENTATIONS:\n",
    "                    test_embedding[aug] = test_embeddings[ind][aug]\n",
    "            else:\n",
    "                for aug in ALL_AUGMENTATIONS:\n",
    "                    test_embedding[aug] = np.concatenate([test_embedding[aug], test_embeddings[ind][aug]], axis=1)        \n",
    "\n",
    "            if len(train_embedding) == 0:\n",
    "                for aug in ALL_AUGMENTATIONS:\n",
    "                    train_embedding[aug] = train_embeddings[ind][aug]\n",
    "                    # train_y[aug] = train_ys[ind][aug]\n",
    "            else:\n",
    "                for aug in ALL_AUGMENTATIONS:\n",
    "                    train_embedding[aug] = np.concatenate([train_embedding[aug], train_embeddings[ind][aug]], axis=1)\n",
    "                    # train_y[aug] = np.concatenate([train_y[aug], train_ys[ind][aug]], axis=0)\n",
    "            \n",
    "            # if len(test_embedding) == 0:\n",
    "            # for aug in ALL_AUGMENTATIONS:\n",
    "            #     if len(train_embedding) == 0:\n",
    "            #         train_embedding = train_embeddings[ind][aug]\n",
    "            #         # train_y = train_ys[ind][aug]\n",
    "            #     else:\n",
    "            #         train_embedding = np.concatenate([train_embedding, train_embeddings[ind][aug]], axis=1)\n",
    "                    # train_y = np.concatenate([train_y, train_ys[ind][aug]], axis=0)\n",
    "            # else:\n",
    "            #     for aug in ALL_AUGMENTATIONS:\n",
    "            #         train_embedding = np.concatenate([train_embedding, train_embeddings[ind][aug]], axis=1)\n",
    "            #         train_y = np.concatenate([train_y, train_ys[ind][aug]], axis=1)\n",
    "            #         test_embedding[aug] = np.concatenate([test_embedding[aug], test_embeddings[ind][aug]], axis=1)        \n",
    "        # print(train_ys[ind]['none'])\n",
    "        # ind = indexes[1]\n",
    "        # print(train_ys[ind]['none'].shape)\n",
    "        # print(train_ys[ind]['rotation'].shape)\n",
    "        # train_y = np.concatenate([train_ys[ind]['none'], train_ys[ind]['rotation']], axis=0)\n",
    "        \n",
    "        names_comb.append(name)\n",
    "        embeddings_comb.append((train_embedding, test_embedding))\n",
    "    \n",
    "    for j in range(len(names_comb)):\n",
    "        name = names_comb[j]\n",
    "        # clf = AdaBoostClassifier()\n",
    "        clf = XGBClassifier()\n",
    "        # clf = svm.SVC()\n",
    "        \n",
    "        train_feat = []\n",
    "        train_y = []\n",
    "        \n",
    "        train_feat_map, test_feat_map = embeddings_comb[j]\n",
    "        \n",
    "        for a in range(len(ALL_AUGMENTATIONS)):\n",
    "            aug = ALL_AUGMENTATIONS[a]\n",
    "            if(len(train_feat)) == 0:\n",
    "                train_feat = train_feat_map[aug]\n",
    "                train_y = all_y[a]\n",
    "            else:\n",
    "                train_feat = np.concatenate([train_feat, train_feat_map[aug]], axis = 0) \n",
    "                train_y = np.concatenate([train_y, all_y[a]], axis = 0) \n",
    "            # if a == 5:\n",
    "            #     break\n",
    "        # train_y = \n",
    "        clf.fit(train_feat, train_y)\n",
    "        \n",
    "        pred_train = clf.predict(train_feat)\n",
    "        f1_tr = metrics.f1_score(train_y, pred_train, average='weighted')\n",
    "        \n",
    "        f1_scores = [f1_tr]\n",
    "        \n",
    "        for aug in ALL_AUGMENTATIONS:\n",
    "            test_feat = test_feat_map[aug]\n",
    "            pred_test = clf.predict(test_feat)\n",
    "            f1_te = metrics.f1_score(y_test, pred_test, average='weighted')\n",
    "            f1_scores.append(f1_te)\n",
    "        \n",
    "        if name not in components_map:\n",
    "            components_map[name] = [f1_scores]\n",
    "        else:\n",
    "            components_map[name] = components_map[name] + [f1_scores]\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_feat.shape\n",
    "\n",
    "# train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Acc', '1.000 (0.000)', '0.835 (0.000)', '0.479 (0.000)', '0.814 (0.000)', '0.819 (0.000)', '0.663 (0.000)', '0.676 (0.000)', '0.712 (0.000)']\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "path = os.path.join(RESULTS_PATH, 'Umap_kfold_{}.csv'.format('_'.join(AUGMENTATIONS)))\n",
    "with open(path, 'w', newline='') as csvfile:\n",
    "    row = ['Sensors', 'f1 train', 'f1 test']\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quoting=csv.QUOTE_MINIMAL)\n",
    "    spamwriter.writerow(row)\n",
    "    for name in names_comb:\n",
    "        row = [name]\n",
    "        f1_mean_tr = np.array([ f1[0] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_tr = np.array([ f1[0] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te = np.array([ f1[1] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te = np.array([ f1[1] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_rot = np.array([ f1[2] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_rot = np.array([ f1[2] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_per = np.array([ f1[3] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_per = np.array([ f1[3] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_tim = np.array([ f1[4] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_tim = np.array([ f1[4] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_mag = np.array([ f1[5] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_mag = np.array([ f1[5] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_sca = np.array([ f1[6] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_sca = np.array([ f1[6] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_jit = np.array([ f1[7] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_jit = np.array([ f1[7] for f1 in components_map[name]]).std()\n",
    "        \n",
    "        row = [\n",
    "            name, \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_tr, f1_stds_tr), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te, f1_stds_te), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te_rot, f1_stds_te_rot), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te_per, f1_stds_te_per), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te_tim, f1_stds_te_tim), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te_mag, f1_stds_te_mag), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te_sca, f1_stds_te_sca), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te_jit, f1_stds_te_jit), \n",
    "        ]\n",
    "        print(row)\n",
    "        spamwriter.writerow(row)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Centered  0.966 (0.017)'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Centered  0.966 (0.017)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Rotation 0.999 (0.001)', '0.955 (0.027)')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Rotation 0.999 (0.001)', '0.955 (0.027)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Original Acc', '0.998 (0.001)', '0.925 (0.045)')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Original Acc', '0.998 (0.001)', '0.925 (0.045)'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('contrastive')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af38bbb6e377e6d688e9d49e963edf808fac73a4ecd279c84061bb0d9783d83c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
