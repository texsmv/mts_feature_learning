{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from source.read_HAR_dataset import read_har_dataset, har_dimensions, har_activities, har_activities_map, har_ind_IDS\n",
    "from source.utils import  filter_dimensions\n",
    "from source.tserie import TSerie\n",
    "from source.utils import classify_dataset\n",
    "from itertools import chain, combinations\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import svm\n",
    "from source.utils import idsStd\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "import umap\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from source.augmentation import  * \n",
    "# from cuml.datasets import make_blobs\n",
    "# from cuml.neighbors import NearestNeighbors\n",
    "# from cuml.manifold import UMAP\n",
    "# from cuml.cluster import DBSCAN\n",
    "\n",
    "\n",
    "sys.path.insert(0, '/home/texs/Documentos/Repositories/mts_viz')\n",
    "from server.source.storage import MTSStorage\n",
    "\n",
    "def powerset(iterable):\n",
    "    \"powerset([1,2,3]) --> () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)\"\n",
    "    s = list(iterable)  # allows duplicate elements\n",
    "    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n",
    "\n",
    "NORM = 0 # 0: No normalization, 1: centering 2: z_score_norm\n",
    "N_COMPONENTS=32\n",
    "DATASET = 'HAR-UML20'\n",
    "KFOLDS = 9\n",
    "N_TESTS = 2\n",
    "EPOCHS=20\n",
    "RESULTS_PATH = 'outputs/augmentation/'\n",
    "# AUGMENTATIONS = ['rotation', 'permutation', 'time_warp', 'magnitude_warp', 'scaling', 'jitter']\n",
    "# AUGMENTATIONS = ['none']\n",
    "AUGMENTATIONS = ['scaling']\n",
    "ALL_AUGMENTATIONS = ['none', 'rotation', 'permutation', 'time_warp', 'magnitude_warp', 'scaling', 'jitter']\n",
    "# ALL_AUGMENTATIONS = ['none', 'rotation', 'rotation', 'rotation', 'rotation', 'rotation', 'rotation']\n",
    "# ALL_AUGMENTATIONS = ['none', 'rotation'] \n",
    "# AUGMENTATIONS = ['magnitude_warp']\n",
    "REPEATS_PER_AUGMENTATION = 1\n",
    "INCLUDE_ORIGINAL = True\n",
    "# N_DIMS_NAMES = ['Acc', 'Gyro', 'Mag']\n",
    "N_DIMS_NAMES = ['Acc', 'Gyro']\n",
    "# N_DIMS_NAMES = ['Acc']\n",
    "N_DIMENSIONS = [\n",
    "    [\n",
    "        'Accelerometer-X',\t\n",
    "        'Accelerometer-Y',\t\n",
    "        'Accelerometer-Z',\n",
    "    ],\n",
    "    [\n",
    "        'Gyrometer-X',\n",
    "        'Gyrometer-Y',\n",
    "        'Gyrometer-Z',\n",
    "    ],\n",
    "    # [\n",
    "    #     'Magnetometer-X',\n",
    "    #     'Magnetometer-Y',\n",
    "    #     'Magnetometer-Z'\n",
    "    # ]\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.torch_utils import getContrastiveFeatures\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, epochs = 100, batch_size = 32, loss_metric = 'SimCLR', encoding_size = 8, mode = 'subsequences'):\n",
    "        self.model = None\n",
    "        self.device = None\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.loss_metric = loss_metric\n",
    "        self.encoding_size = encoding_size\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        embeddings, self.model, self.device =  getContrastiveFeatures(X.transpose([0, 2, 1]), y, epochs = self.epochs, loss_metric=self.loss_metric, feat_size=256)\n",
    "        return embeddings\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return self.model.encode(X.transpose([0, 2, 1]), self.device)\n",
    "\n",
    "\n",
    "def augmentData(X, y, augmentation, repeat = 3):\n",
    "    X_out = []\n",
    "    y_out = []\n",
    "    for i in range(repeat):\n",
    "        if augmentation == 'rotation':\n",
    "            augmented = rotation(X, angle_range=[-np.pi/4, np.pi/4])\n",
    "            # augmented = rotation(X, angle_range=[-np.pi/64, np.pi/64])\n",
    "        elif augmentation == 'permutation':\n",
    "            augmented = permutation(X)\n",
    "        elif augmentation == 'time_warp':\n",
    "            augmented = time_warp(X, sigma=0.03)\n",
    "        elif augmentation == 'magnitude_warp':\n",
    "            augmented = magnitude_warp(X, sigma=0.04, knot=4)\n",
    "        elif augmentation == 'scaling':\n",
    "            augmented = scaling(X, sigma=0.05)\n",
    "        elif augmentation == 'jitter':\n",
    "            augmented = jitter(X, sigma=0.01)\n",
    "        # elif augmentation == 'magnitude_pert':\n",
    "        #     augmented = magnitude_pert(X, prange=[0, 1])\n",
    "        else:\n",
    "            augmented = X\n",
    "        if len(X_out) == 0:\n",
    "            X_out = augmented\n",
    "            y_out = y\n",
    "        else:\n",
    "            X_out = np.concatenate((X_out, augmented), axis=0)\n",
    "            y_out = np.concatenate((y_out, y), axis=0)\n",
    "    return X_out, y_out\n",
    "\n",
    "def augment(X, y, augmentations, repeats_per_augmentation=1, include_original=False):\n",
    "    X_aug = []\n",
    "    y_aug = []\n",
    "    if include_original:\n",
    "        X_aug = X\n",
    "        y_aug = y\n",
    "    for augmentation in augmentations:\n",
    "        curr_X_aug, curr_y_aug = augmentData(X, y, augmentation, repeat=repeats_per_augmentation)\n",
    "        if len(X_aug) == 0:\n",
    "            X_aug = curr_X_aug\n",
    "            y_aug = curr_y_aug\n",
    "        else:\n",
    "            X_aug = np.concatenate((X_aug, curr_X_aug), axis=0)\n",
    "            y_aug = np.concatenate((y_aug, curr_y_aug), axis=0)\n",
    "    return X_aug, y_aug\n",
    "\n",
    "def minoritySampling(X, y):\n",
    "    rus = RandomUnderSampler(sampling_strategy='not minority', random_state=1)\n",
    "    N, T, D = X.shape\n",
    "    X_temp = X.reshape([N, T * D])\n",
    "    X_temp, y = rus.fit_resample(X_temp, y)\n",
    "    X = X_temp.reshape([X_temp.shape[0], T, D])\n",
    "    return X, y\n",
    "\n",
    "\n",
    "activities_map = {\n",
    "    0: \"Sedentary\",\n",
    "    1: \"Walking\",\n",
    "    2: \"Running\",\n",
    "    3: \"Downstairs\",\n",
    "    4: \"Upstairs\"\n",
    "}\n",
    "def load_data(k):\n",
    "    all_ids = har_ind_IDS\n",
    "    test_ids = all_ids[k: k + N_TESTS]\n",
    "    train_ids = all_ids[:k] + all_ids[k + N_TESTS:]        \n",
    "    \n",
    "    data = read_har_dataset('./datasets/HAR-UML20/', train_ids=train_ids, test_ids=test_ids, val_ids=[], cache=True)\n",
    "    ids_train, X_train, y_train, I_train, train_kcal_MET = data['train']\n",
    "    # ids_val, X_val, y_val, I_val, val_kcal_MET = data['val']\n",
    "    ids_test, X_test, y_test, I_test, test_kcal_MET = data['test']\n",
    "    \n",
    "    \n",
    "\n",
    "    all_dimensions = har_dimensions\n",
    "    activities_map = har_activities_map\n",
    "    \n",
    "    y_train[y_train==0] = 0\n",
    "    y_train[y_train==1] = 0\n",
    "    y_train[y_train==2] = 0\n",
    "    y_test[y_test==0] = 0\n",
    "    y_test[y_test==1] = 0\n",
    "    y_test[y_test==2] = 0\n",
    "\n",
    "    for i in range(3, len(har_activities)):\n",
    "        y_train[y_train==i] = i - 2\n",
    "        y_test[y_test==i] = i - 2\n",
    "    \n",
    "    ind_std_train = idsStd(train_ids , X_train, I_train)\n",
    "    ind_std_test = idsStd(test_ids, X_test, I_test)\n",
    "    \n",
    "    unique, counts = np.unique(y_train, return_counts=True)\n",
    "    unique, counts = np.unique(y_test, return_counts=True)\n",
    "    \n",
    "    I_train = np.expand_dims(I_train, axis=1)\n",
    "    I_test = np.expand_dims(I_test, axis=1)\n",
    "    ltrain = np.arange(len(y_train))\n",
    "    ltest = np.arange(len(y_test))\n",
    "    \n",
    "    X_train, zlabels_train = minoritySampling(X_train, ltrain)\n",
    "    X_test, zlabels_test = minoritySampling(X_test, ltest)\n",
    "    \n",
    "    y_train = y_train[ltrain]\n",
    "    I_train = I_train[ltrain]\n",
    "    y_test = y_test[ltest]\n",
    "    I_test = I_test[ltest]\n",
    "    \n",
    "    return X_train, y_train, I_train, X_test, y_test, I_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD: 0\n",
      "Train IDS: [2, 3, 4, 5, 6, 7, 8, 9]\n",
      "Test IDS: [0, 1]\n",
      "Val IDS: []\n",
      "Loading dataset from cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/texs/Documentos/Repositories/mts_feature_learning/source/augmentation.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  warp = np.concatenate(np.random.permutation(splits)).ravel()\n",
      "<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsequence length: 180\n",
      "Epoch[1] Train loss    avg: 3.9375304776399873\n",
      "Epoch[10] Train loss    avg: 3.5196431421386407\n",
      "Epoch[20] Train loss    avg: 3.4747730826363346\n",
      "Subsequence length: 180\n",
      "Epoch[1] Train loss    avg: 3.942179946124856\n",
      "Epoch[10] Train loss    avg: 3.5559092306243585\n",
      "Epoch[20] Train loss    avg: 3.4858983494908675\n",
      "Classifying\n",
      "0\n",
      "1\n",
      "2\n",
      "Classifying done\n",
      "FOLD: 1\n",
      "Train IDS: [0, 3, 4, 5, 6, 7, 8, 9]\n",
      "Test IDS: [1, 2]\n",
      "Val IDS: []\n",
      "Loading dataset from cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/texs/Documentos/Repositories/mts_feature_learning/source/augmentation.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  warp = np.concatenate(np.random.permutation(splits)).ravel()\n",
      "<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsequence length: 180\n",
      "Epoch[1] Train loss    avg: 3.9278397753759084\n",
      "Epoch[10] Train loss    avg: 3.5245844131798916\n",
      "Epoch[20] Train loss    avg: 3.472109084201948\n",
      "Subsequence length: 180\n",
      "Epoch[1] Train loss    avg: 3.984039178354486\n",
      "Epoch[10] Train loss    avg: 3.511493127358141\n",
      "Epoch[20] Train loss    avg: 3.4421344783705505\n",
      "Classifying\n",
      "0\n",
      "1\n",
      "2\n",
      "Classifying done\n",
      "FOLD: 2\n",
      "Train IDS: [0, 1, 4, 5, 6, 7, 8, 9]\n",
      "Test IDS: [2, 3]\n",
      "Val IDS: []\n",
      "Loading dataset from cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/texs/Documentos/Repositories/mts_feature_learning/source/augmentation.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  warp = np.concatenate(np.random.permutation(splits)).ravel()\n",
      "<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsequence length: 180\n",
      "Epoch[1] Train loss    avg: 4.082364175525414\n",
      "Epoch[10] Train loss    avg: 3.6056976826662943\n",
      "Epoch[20] Train loss    avg: 3.5167539736946223\n",
      "Subsequence length: 180\n",
      "Epoch[1] Train loss    avg: 3.9598682580260456\n",
      "Epoch[10] Train loss    avg: 3.55624275038085\n",
      "Epoch[20] Train loss    avg: 3.488369392259472\n",
      "Classifying\n",
      "0\n",
      "1\n",
      "2\n",
      "Classifying done\n",
      "FOLD: 3\n",
      "Train IDS: [0, 1, 2, 5, 6, 7, 8, 9]\n",
      "Test IDS: [3, 4]\n",
      "Val IDS: []\n",
      "Loading dataset from cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/texs/Documentos/Repositories/mts_feature_learning/source/augmentation.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  warp = np.concatenate(np.random.permutation(splits)).ravel()\n",
      "<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsequence length: 180\n",
      "Epoch[1] Train loss    avg: 4.069606692657858\n",
      "Epoch[10] Train loss    avg: 3.5739562862415606\n",
      "Epoch[20] Train loss    avg: 3.4890275170960403\n",
      "Subsequence length: 180\n",
      "Epoch[1] Train loss    avg: 4.010267867654713\n",
      "Epoch[10] Train loss    avg: 3.545702280731976\n",
      "Epoch[20] Train loss    avg: 3.4910088347904575\n",
      "Classifying\n",
      "0\n",
      "1\n",
      "2\n",
      "Classifying done\n",
      "FOLD: 4\n",
      "Train IDS: [0, 1, 2, 3, 6, 7, 8, 9]\n",
      "Test IDS: [4, 5]\n",
      "Val IDS: []\n",
      "Loading dataset from cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/texs/Documentos/Repositories/mts_feature_learning/source/augmentation.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  warp = np.concatenate(np.random.permutation(splits)).ravel()\n",
      "<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsequence length: 180\n",
      "Epoch[1] Train loss    avg: 3.9627014954884845\n",
      "Epoch[10] Train loss    avg: 3.531150920050485\n",
      "Epoch[20] Train loss    avg: 3.479377393495469\n",
      "Subsequence length: 180\n",
      "Epoch[1] Train loss    avg: 3.910364194143386\n",
      "Epoch[10] Train loss    avg: 3.506061013539632\n",
      "Epoch[20] Train loss    avg: 3.4636940978822253\n",
      "Classifying\n",
      "0\n",
      "1\n",
      "2\n",
      "Classifying done\n",
      "FOLD: 5\n",
      "Train IDS: [0, 1, 2, 3, 4, 7, 8, 9]\n",
      "Test IDS: [5, 6]\n",
      "Val IDS: []\n",
      "Loading dataset from cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/texs/Documentos/Repositories/mts_feature_learning/source/augmentation.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  warp = np.concatenate(np.random.permutation(splits)).ravel()\n",
      "<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsequence length: 180\n",
      "Epoch[1] Train loss    avg: 3.91717515034335\n",
      "Epoch[10] Train loss    avg: 3.5125571904437884\n",
      "Epoch[20] Train loss    avg: 3.4618506814752306\n",
      "Subsequence length: 180\n",
      "Epoch[1] Train loss    avg: 3.899334669113159\n",
      "Epoch[10] Train loss    avg: 3.5002458606447493\n",
      "Epoch[20] Train loss    avg: 3.4704825713166167\n",
      "Classifying\n",
      "0\n",
      "1\n",
      "2\n",
      "Classifying done\n",
      "FOLD: 6\n",
      "Train IDS: [0, 1, 2, 3, 4, 5, 8, 9]\n",
      "Test IDS: [6, 7]\n",
      "Val IDS: []\n",
      "Loading dataset from cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/texs/Documentos/Repositories/mts_feature_learning/source/augmentation.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  warp = np.concatenate(np.random.permutation(splits)).ravel()\n",
      "<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsequence length: 180\n",
      "Epoch[1] Train loss    avg: 3.950752880424261\n",
      "Epoch[10] Train loss    avg: 3.5321379687104906\n",
      "Epoch[20] Train loss    avg: 3.4860682114958763\n",
      "Subsequence length: 180\n",
      "Epoch[1] Train loss    avg: 3.923805267150913\n",
      "Epoch[10] Train loss    avg: 3.5202897489070892\n",
      "Epoch[20] Train loss    avg: 3.4586690988923823\n",
      "Classifying\n",
      "0\n",
      "1\n",
      "2\n",
      "Classifying done\n",
      "FOLD: 7\n",
      "Train IDS: [0, 1, 2, 3, 4, 5, 6, 9]\n",
      "Test IDS: [7, 8]\n",
      "Val IDS: []\n",
      "Loading dataset from cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/texs/Documentos/Repositories/mts_feature_learning/source/augmentation.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  warp = np.concatenate(np.random.permutation(splits)).ravel()\n",
      "<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsequence length: 180\n",
      "Epoch[1] Train loss    avg: 4.021833741239139\n",
      "Epoch[10] Train loss    avg: 3.5738021145973886\n",
      "Epoch[20] Train loss    avg: 3.4924702623060773\n",
      "Subsequence length: 180\n",
      "Epoch[1] Train loss    avg: 3.9287535420485904\n",
      "Epoch[10] Train loss    avg: 3.4977594757718697\n",
      "Epoch[20] Train loss    avg: 3.4475810517157828\n",
      "Classifying\n",
      "0\n",
      "1\n",
      "2\n",
      "Classifying done\n",
      "FOLD: 8\n",
      "Train IDS: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "Test IDS: [8, 9]\n",
      "Val IDS: []\n",
      "Loading dataset from cache...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/texs/Documentos/Repositories/mts_feature_learning/source/augmentation.py:34: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  warp = np.concatenate(np.random.permutation(splits)).ravel()\n",
      "<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsequence length: 180\n",
      "Epoch[1] Train loss    avg: 3.941724163080965\n",
      "Epoch[10] Train loss    avg: 3.501087099313736\n",
      "Epoch[20] Train loss    avg: 3.4572870241744176\n",
      "Subsequence length: 180\n",
      "Epoch[1] Train loss    avg: 3.917535505124501\n",
      "Epoch[10] Train loss    avg: 3.496577240526676\n",
      "Epoch[20] Train loss    avg: 3.4615611199821745\n",
      "Classifying\n",
      "0\n",
      "1\n",
      "2\n",
      "Classifying done\n"
     ]
    }
   ],
   "source": [
    "# from source.torch_utils import getContrastiveFeatures\n",
    "from sklearn.utils import shuffle\n",
    "import torch\n",
    "\n",
    "firstTimeSave = True\n",
    "storage = MTSStorage('har_augmentations')\n",
    "# storage.delete()\n",
    "storage.load()\n",
    "\n",
    "\n",
    "components_map = {}\n",
    "\n",
    "for k in range(KFOLDS):\n",
    "    print('FOLD: {}'.format(k))\n",
    "    # ------------------------ Reading the dataset ------------------------\n",
    "    X_train, y_train, I_train, X_test, y_test, I_test = load_data(k)\n",
    "    # ---------------------------------------------------------------------\n",
    "\n",
    "    \n",
    "    train_embeddings = []\n",
    "    train_ys = []\n",
    "    test_embeddings = []\n",
    "    \n",
    "    augment_idx = []\n",
    "    all_y = []\n",
    "    cont = 0\n",
    "    for aug in ALL_AUGMENTATIONS:\n",
    "        if aug == 'none':\n",
    "            idx_trans = np.random.choice(np.arange(len(y_train)), len(y_train), replace=False)\n",
    "        else:\n",
    "            n = len(y_train)\n",
    "            nsize = len(y_train) // (len(ALL_AUGMENTATIONS) - 1)\n",
    "            begin = cont * nsize\n",
    "            end = (cont + 1) * nsize\n",
    "            idx_trans = np.arange(len(y_train))[begin: end]\n",
    "            # idx_trans = np.random.choice(np.arange(len(y_train_o)), , replace=False)\n",
    "            cont = cont + 1\n",
    "        # idx_trans = np.random.choice(np.arange(len(y_train_o)), len(y_train_o) // (len(ALL_AUGMENTATIONS)), replace=False)\n",
    "        augment_idx.append(idx_trans)\n",
    "        all_y.append(y_train[idx_trans])\n",
    "        \n",
    "    \n",
    "    for t in range(len(N_DIMENSIONS)):\n",
    "        dimensions = N_DIMENSIONS[t]    \n",
    "        X_train_f = filter_dimensions(X_train, har_dimensions, dimensions)\n",
    "        X_test_f = filter_dimensions(X_test, har_dimensions, dimensions)\n",
    "        \n",
    "        mts_train = TSerie(X = X_train_f, y = y_train, I = I_train, dimensions = dimensions, classLabels=activities_map)\n",
    "        mts_test = TSerie(X = X_test_f, y = y_test, I = I_test, dimensions = dimensions, classLabels=activities_map)\n",
    "        \n",
    "        minl, maxl = mts_train.minMaxNormalization()\n",
    "        mts_test.minMaxNormalization(minl=minl, maxl=maxl)\n",
    "        \n",
    "        \n",
    "        # Saving augmentations\n",
    "        test_tranformations = [\n",
    "            augment(mts_test.X, mts_test.y, repeats_per_augmentation = 1, augmentations = [ALL_AUGMENTATIONS[i]], include_original = False)\n",
    "            for i in range(len(ALL_AUGMENTATIONS))\n",
    "        ]\n",
    "        train_tranformations = [\n",
    "            augment(mts_train.X, mts_train.y, repeats_per_augmentation = 1, augmentations = [ALL_AUGMENTATIONS[i]], include_original = False)\n",
    "            for i in range(len(ALL_AUGMENTATIONS))\n",
    "        ]\n",
    "        \n",
    "        # Training reducer\n",
    "        if NORM == 1:\n",
    "            mts_train.center()\n",
    "            mts_test.center()\n",
    "        elif NORM == 2:\n",
    "            mts_train.znorm()\n",
    "            mts_test.znorm()\n",
    "        \n",
    "        n_neighbors = 15\n",
    "        reducer = FeatureExtractor(epochs = EPOCHS, loss_metric='SupConLoss')        \n",
    "        embeddings_train = reducer.fit_transform(mts_train.X, y=mts_train.y)\n",
    "        embeddings_test = reducer.transform(mts_test.X)\n",
    "        \n",
    "\n",
    "        # Transforming augmentations        \n",
    "        test_map = {}\n",
    "        train_map = {}\n",
    "        for i in range(len(ALL_AUGMENTATIONS)):\n",
    "            tranformation_name = ALL_AUGMENTATIONS[i]\n",
    "            \n",
    "            # Testing\n",
    "            test_tranformations[i] = test_tranformations[i][0].copy()\n",
    "            train_tranformations[i] = train_tranformations[i][0].copy()\n",
    "            \n",
    "            mts_test_t = TSerie(X = test_tranformations[i], y = mts_test.y, I = mts_test.I, dimensions = dimensions, classLabels=activities_map)\n",
    "            if NORM == 1:\n",
    "                mts_test_t.center()\n",
    "            elif NORM == 2:\n",
    "                mts_test_t.znorm()\n",
    "            mts_test_t.features = reducer.transform(mts_test_t.X)\n",
    "            test_map[ALL_AUGMENTATIONS[i]] = mts_test_t.features\n",
    "\n",
    "            # Training\n",
    "            idx = augment_idx[i]\n",
    "            mts_train_t = TSerie(X = train_tranformations[i][idx], y = y_train[idx], I = I_train[idx], dimensions = dimensions, classLabels=activities_map)\n",
    "            if NORM == 1:\n",
    "                mts_train_t.center()\n",
    "            elif NORM == 2:\n",
    "                mts_train_t.znorm()\n",
    "            mts_train_t.features = reducer.transform(mts_train_t.X)\n",
    "            train_map[ALL_AUGMENTATIONS[i]] = mts_train_t.features\n",
    "        \n",
    "        train_embeddings.append(train_map)\n",
    "        test_embeddings.append(test_map)\n",
    "        reducer = None\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        # embeddings_train, embeddings_test = getContrastiveFeatures(mts_train.X.transpose([0, 2, 1]), mts_train.y, epochs =20, loss_metric='SupConLoss', X_test=mts_test.X.transpose([0, 2, 1]))\n",
    "            # if firstTimeSave:\n",
    "                # print(np.random.random([test_tranformations[i].shape[0], 2]).shape)\n",
    "                # storage.add_mts(\n",
    "                #     'test {}_{}'.format(ALL_AUGMENTATIONS[i], N_DIMS_NAMES[t]),\n",
    "                #     mts_test_t.X, \n",
    "                #     dimensions,\n",
    "                #     coords={\n",
    "                #         'umap': mts_test_t.features\n",
    "                #     }, \n",
    "                #     labels={\n",
    "                #         'activities': mts_test.y, \n",
    "                #         # 'participants': mts_test.I\n",
    "                #     }, \n",
    "                #     labelsNames={'activities': activities_map },\n",
    "                #     sampling = True,\n",
    "                #     n_samples = 400\n",
    "                # )\n",
    "                # storage.add_mts(\n",
    "                #     'train {}_{}'.format(ALL_AUGMENTATIONS[i], N_DIMS_NAMES[t]),\n",
    "                #     mts_train_t.X, \n",
    "                #     dimensions,\n",
    "                #     coords={\n",
    "                #         'umap': mts_train_t.features\n",
    "                #     }, \n",
    "                #     labels={\n",
    "                #         'activities': mts_train_t.y, \n",
    "                #         # 'participants': mts_test.I\n",
    "                #     }, \n",
    "                #     labelsNames={'activities': activities_map },\n",
    "                #     sampling = True,\n",
    "                #     n_samples = 400\n",
    "                # )\n",
    "                # storage.save()\n",
    "            \n",
    "    firstTimeSave = False\n",
    "    \n",
    "    names_comb = []\n",
    "    embeddings_comb = []\n",
    "    for i, combo in enumerate(powerset(list(range(len(N_DIMS_NAMES)))), 1):\n",
    "        indexes = list(combo)\n",
    "        name = ''\n",
    "        train_embedding = {}\n",
    "        # train_y = {}\n",
    "        test_embedding = {}\n",
    "        if len(indexes) == 0:\n",
    "            continue\n",
    "        for ind in indexes:\n",
    "            name = name + ' ' + N_DIMS_NAMES[ind]\n",
    "            if len(test_embedding) == 0:\n",
    "                for aug in ALL_AUGMENTATIONS:\n",
    "                    test_embedding[aug] = test_embeddings[ind][aug]\n",
    "            else:\n",
    "                for aug in ALL_AUGMENTATIONS:\n",
    "                    test_embedding[aug] = np.concatenate([test_embedding[aug], test_embeddings[ind][aug]], axis=1)        \n",
    "\n",
    "            if len(train_embedding) == 0:\n",
    "                for aug in ALL_AUGMENTATIONS:\n",
    "                    train_embedding[aug] = train_embeddings[ind][aug]\n",
    "            else:\n",
    "                for aug in ALL_AUGMENTATIONS:\n",
    "                    train_embedding[aug] = np.concatenate([train_embedding[aug], train_embeddings[ind][aug]], axis=1)\n",
    "        \n",
    "        names_comb.append(name)\n",
    "        embeddings_comb.append((train_embedding, test_embedding))\n",
    "    \n",
    "    print('Classifying')\n",
    "    for j in range(len(names_comb)):\n",
    "        print(j)\n",
    "        name = names_comb[j]\n",
    "        # clf = AdaBoostClassifier()\n",
    "        clf = XGBClassifier(tree_method='gpu_hist', predictor='gpu_predictor')\n",
    "        # clf = XGBClassifier()\n",
    "        # clf = svm.SVC()\n",
    "        \n",
    "        train_feat = []\n",
    "        train_y = []\n",
    "        \n",
    "        train_feat_map, test_feat_map = embeddings_comb[j]\n",
    "        for a in range(len(ALL_AUGMENTATIONS)):\n",
    "            aug = ALL_AUGMENTATIONS[a]\n",
    "            if(len(train_feat)) == 0:\n",
    "                train_feat = train_feat_map[aug]\n",
    "                train_y = all_y[a]\n",
    "            else:\n",
    "                train_feat = np.concatenate([train_feat, train_feat_map[aug]], axis = 0) \n",
    "                train_y = np.concatenate([train_y, all_y[a]], axis = 0) \n",
    "        # train_y = \n",
    "        clf.fit(train_feat, train_y)\n",
    "        \n",
    "        pred_train = clf.predict(train_feat)\n",
    "        f1_tr = metrics.f1_score(train_y, pred_train, average='weighted')\n",
    "        \n",
    "        f1_scores = [f1_tr]\n",
    "        \n",
    "        for aug in ALL_AUGMENTATIONS:\n",
    "            test_feat = test_feat_map[aug]\n",
    "            pred_test = clf.predict(test_feat)\n",
    "            f1_te = metrics.f1_score(y_test, pred_test, average='weighted')\n",
    "            f1_scores.append(f1_te)\n",
    "        \n",
    "        if name not in components_map:\n",
    "            components_map[name] = [f1_scores]\n",
    "        else:\n",
    "            components_map[name] = components_map[name] + [f1_scores]\n",
    "    print('Classifying done')\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_feat.shape\n",
    "\n",
    "# train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Acc', '1.000 (0.000)', '0.569 (0.056)', '0.508 (0.052)', '0.511 (0.047)', '0.529 (0.049)', '0.464 (0.050)', '0.699 (0.037)', '0.373 (0.058)']\n",
      "[' Gyro', '1.000 (0.000)', '0.609 (0.057)', '0.445 (0.053)', '0.560 (0.055)', '0.583 (0.053)', '0.470 (0.032)', '0.595 (0.026)', '0.330 (0.032)']\n",
      "[' Acc Gyro', '1.000 (0.000)', '0.642 (0.050)', '0.509 (0.051)', '0.586 (0.039)', '0.597 (0.044)', '0.494 (0.053)', '0.702 (0.036)', '0.390 (0.058)']\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv\n",
    "from source.utils import plotMatResult\n",
    "\n",
    "row_names = []\n",
    "column_names = ['Train', 'Test', 'Rotation(Te)', 'Permutation(Te)', 'TimeW(Te)', 'MagnitudeW(Te)', 'Scaling(Te)', 'Jitter(Te)']\n",
    "mat_data = []\n",
    "\n",
    "path = os.path.join(RESULTS_PATH, 'Umap_kfold_{}.csv'.format('_'.join(AUGMENTATIONS)))\n",
    "with open(path, 'w', newline='') as csvfile:\n",
    "    row = ['Sensors', 'f1 train', 'f1 test']\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quoting=csv.QUOTE_MINIMAL)\n",
    "    spamwriter.writerow(row)\n",
    "    for name in names_comb:\n",
    "        row_names.append(name)\n",
    "        row = [name]\n",
    "        f1_mean_tr = np.array([ f1[0] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_tr = np.array([ f1[0] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te = np.array([ f1[1] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te = np.array([ f1[1] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_rot = np.array([ f1[2] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_rot = np.array([ f1[2] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_per = np.array([ f1[3] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_per = np.array([ f1[3] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_tim = np.array([ f1[4] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_tim = np.array([ f1[4] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_mag = np.array([ f1[5] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_mag = np.array([ f1[5] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_sca = np.array([ f1[6] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_sca = np.array([ f1[6] for f1 in components_map[name]]).std()\n",
    "        f1_mean_te_jit = np.array([ f1[7] for f1 in components_map[name]]).mean()\n",
    "        f1_stds_te_jit = np.array([ f1[7] for f1 in components_map[name]]).std()\n",
    "        \n",
    "        row = [\n",
    "            name, \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_tr, f1_stds_tr), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te, f1_stds_te), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te_rot, f1_stds_te_rot), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te_per, f1_stds_te_per), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te_tim, f1_stds_te_tim), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te_mag, f1_stds_te_mag), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te_sca, f1_stds_te_sca), \n",
    "            '{:.3f} ({:.3f})'.format(f1_mean_te_jit, f1_stds_te_jit), \n",
    "        ]\n",
    "        mat_data.append(row[1:])\n",
    "        print(row)\n",
    "        spamwriter.writerow(row)\n",
    "\n",
    "\n",
    "plotMatResult('results', 'footer', row_names, column_names, mat_data, plot_fig=True, save_fig=True, file_name='contrastive_norm.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Centered  0.966 (0.017)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Centered  0.966 (0.017)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Rotation 0.999 (0.001)', '0.955 (0.027)')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Rotation 0.999 (0.001)', '0.955 (0.027)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Original Acc', '0.998 (0.001)', '0.925 (0.045)')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'Original Acc', '0.998 (0.001)', '0.925 (0.045)'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('contrastive')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af38bbb6e377e6d688e9d49e963edf808fac73a4ecd279c84061bb0d9783d83c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
